\documentclass[12pt, a4paper]{report}

\usepackage{listings}
\usepackage{xcolor}
\usepackage{ragged2e}
\usepackage{xurl}

\usepackage[a4paper, top=2cm, bottom=2cm, left=3cm, right=3cm]{geometry}

\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}

\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{cleveref}
\usepackage{csquotes}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}

\usepackage[backend=biber, style=numeric]{biblatex} 
\addbibresource{references.bib}
% plain — default.
% alphabetic — буквенные метки [Gol96].
% ieee — стиль IEEE (требует пакет biblatex-ieee).

\usepackage[nonumberlist, acronym]{glossaries}

\usepackage{tcolorbox}
\tcbuselibrary{listings, breakable}

\newtcblisting{mytcblisting}[2][]{%
  listing only,
  breakable,
  colframe=black,
  colback=white,
  boxsep=5pt,
  title={#2},
  listing options={
    basicstyle=\ttfamily\small,
    breaklines=true,
    breakatwhitespace=true,
    postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
    columns=fullflexible,
    keepspaces=true,
    language=C,
    numbers=left,
    numbersep=10pt,
    stepnumber=1,
    firstnumber=1,
    numberstyle=\tiny\color{gray}
  },
  #1
}

\bibliographystyle{plain}

\hypersetup{
    colorlinks=true,
    linkcolor=black,
    citecolor=black,
    filecolor=black,
    urlcolor=black
}

\newtheoremstyle{note}
    {3pt}
    {3pt}
    {}
    {1.25cm}
    {\bfseries}
    {.}
    { }
    {}

% Несколько полезных блоков для выделения важных частей текста. Пример:
% \begin{theorem}
%   Если $a$ и $b$ — положительные числа, то $a + b \geq 2\sqrt{ab}$.
% \end{theorem}
\theoremstyle{note}
\newtheorem*{note}{Замечание}
\newtheorem{theorem}{Теорема}
\newtheorem{lemma}{Лемма}
\newtheorem{corollary}{Следствие}
\newcommand{\R}[2]{\mathbb{R}^{#1 \times #2}}
\newcommand{\Rn}{\mathbb{R}^n}

\begin{document}

\tableofcontents

% \usepackage{glossaries}
% \newglossaryentry{svd}{name=SVD, description={Сингулярное разложение матрицы}}
% \printglossaries

\newpage
\section{Глоссарий}

\hspace{0.45cm} \textbf{Спектр сингулярного разложения} (сингулярный спектр) - это набор всех сингулярных значений: \( \operatorname{diag}(\Sigma) = \{ \sigma_1, \sigma_2, \dots, \sigma_{\min(m, n)} \} \).

\textbf{Нахождение полного SVD} - нахождение по первоначальной матрице правого и левого сингулярных векторов и диагональной матрицы сингулярных значений. 

\textbf{Итеративный алгоритм} - это такой алгоритм, условие схождения которого таково, что нельзя однозначно сказать, сколько итераций будет произведено в конкретном случае. Далее в тексте при использовании слова. 

\chapter{Идеи Парфенова/Дроздова}

\section{Уточнение поворотов Гивенса в implicit zero-shift QR}

\subsection{Описание алгоритма}

\subsubsection{Приведение к бидиагональной матрице}

На первом этапе изначальная матрица $A$ приводится к бидиагональной матрице $B$. Производя это приведение мы уменьшаем количество ненулевых элементов в матрице и сохраняем заложенную в нее информацию.

% TODO поискать цитаты
% Теорема Экарта-Янга: Бидиагонализация — этап в SVD, а Хаусхолдер/Гивенс оптимальны для унитарных преобразований.
% Блочные алгоритмы (Demmel et al.) Источник: Demmel, J., et al. "Communication-optimal parallel and sequential QR and LU factorizations" (2008). Оптимизация для кэш-памяти: комбинация Хаусхолдера с блочной обработкой. Ключевой вывод: "Blocked Householder remains the method of choice for dense matrices."
% Рандомизированные методы (Halko et al.) Источник: Halko, N., et al. "Finding structure with randomness" (2011). Для огромных матриц используют рандомизированную бидиагонализацию, но с опорой на те же базовые преобразования.
% Про то, что Гивенс и Хаусхолдер действительно самые крутые. Проверить: Golub & Van Loan, "Matrix Computations" (2013, 4th ed.) Глава 5.4: "Householder Bidiagonalization" Описан стандартный алгоритм приведения матрицы к бидиагональной форме с помощью преобразований Хаусхолдера. Глава 5.1.6: Использование поворотов Гивенса для точечного обнуления элементов. Цитата:  "The most stable and efficient way to compute the bidiagonal form is via Householder transformations." 
Обычно этот этап выполняется за счет алгоритмов, основанных на преобразованиях Хаусхолдера или поворотах Гивенса. В отдельных случаях могут применяться методы Ланцоша и рандомизированные алгоритмы. 

\subsubsection{Сведение бидиагональной матрицы к сингулярной}

Теперь, имея $B$ можно привести ее к диагональной с помощью последовательных поворотов Гивенса. Этот алгоритм можно описать рекуррентным соотношением \cite{Demmel1990}:

\begin{equation}
B_{(i+1)} = J_{2n-2} \cdots J_6 J_4 J_2 B_{(i)} J_1 J_3 J_5 \cdots J_{2n-3},
\end{equation}

где \( J_k \) - повороты Гивенса, которые зануляют внедиагональные элементы матрицы \( B \), приводя её к диагональной \( \Sigma \). 

% TODO найти цитату для уточнения точности машинного эпсилон
При этом ошибка, увеличивающаяся с каждым поворотом, остается незначительной и не превышает $p(n) \epsilon ||B_\ell||$ \cite{Demmel1990}, где  \( \epsilon \) — машинная точность (\( \epsilon \approx 10^{-16} \) для \texttt{double}), \( p(n) \) — умеренно растущая функция от размера матрицы \( n \) (например, \( n \), \( n^2 \), но не \( e^n \)). \( \|B_t\| \) — норма матрицы \( B_t \).
% Пруф того, что ошибка незначительна B. Parlett, The Symmetric Eigenvalue Problem, Prentice Hall, Englewood Cliffs, New Jersey

% TODO найти пруф, добавить цитату. Скорее всего это также в lawn, просто я проглядел
Нужно произвести \( n-1 \) поворотов с каждой стороны для бидиагональной матрицы \( B \). Через несколько итераций матрица \( B^{(i)} \) приближается к диагональной \( \widehat{\Sigma} \), содержащей сингулярные значения:
\begin{equation}
B = \left( \prod J_L \right) \widehat{\Sigma} \left( \prod J_R \right),
\end{equation}

% TODO объяснить сохранение ортогональности на сингулярных векторах
где \( \prod J_L \) и \( \prod J_R \) — накопленные произведения левых и правых поворотов Гивенса, из которых при их перемножении получаются сингулярные вектора \( U \) и \( V \). $\widehat{\Sigma}$ — матрица близких к истинным сингулярных значений.

В данном случае повороты Гивенса применяются итеративно, до того момента как алгоритм не сойдется. Далее описаны теоретические и применяемые на практике критерии схождения.

% TODO подружить эту часть с тем что выше (сделать акцент на тех критериях, которые используются в алгоритме и ужать это в 2 3 параграфа. Для вдохновения посмотреть код и поискать аналогии. Все остальное оставить в этом же subsubsubsection, но после основного/основных критериев, в новом параграфе). Также исправить все с активаного залога (рассмотрим) на пассивный (рассматриваются). Поменьше субъективного взгляда - нужно приводить факты (не наш критерий, а просто критерий и тп) 
\subsubsubsection{\textbf{Схождение вспомогательного алгоритма}}

В данном разделе рассмотрим критерии сходимости для нашего алгоритма, который является совмещением обычного shifted QR и implicit zero-shift QR, приведения бидиагольной матрицы $B$ к диагональной $\Sigma$. Пусть $s_1, s_2,...,s_n$ \--- диагональные элементы, а $e_1, e_2, ..., e_{n-1}$ \--- элементы на побочной диагонали нашей матрицы $B$. Также есть некоторый критерий допуска $tol$, который зависит от желаемой относительной точности сингулярных значений. Он должен быть меньше 1, но больше машинной точности $\epsilon$.

Наш критерий сходимости должен гарантировать, что обнуление $e_i$ сильно не повлияет на сингулярные значения. К примеру, код в LINPACK имеет два разных порога для обнуления элемента:
\begin{align}
\text{Если } (|e_i| + |e_{i-1}| + |s_i| = |e_i| + |e_{i-1}|), \text{ то обнуляем }s_i
\\\text{Если } (|s_i| + |s_{i-1}| + |e_{i-1}| = |s_i| + |s_{i-1}|), \text{ то обнуляем }e_{i-1}
\end{align}

Оба случая не подходят для нашего алгоритма. При случае $(1.1)$ нулевые сингулярные значения будут получаться там, где их не было до этого. Второй вариант $(1.2)$ также является неудовлетворительным. Покажем это на примере: возьмем $\eta$ настолько маленького значения, что в арифметике с плавающей точкой $1+\eta=1$. Тогда рассмотрим матрицу
\begin{center}
$B(x)=\begin{pmatrix}
    \eta^2&1&&\\
    &1&x\\
    &&1&1\\
    &&&\eta^2
\end{pmatrix}$
\end{center}

\noindentПри $x=\eta$ самое минимальное сингулярное значение $B(\eta)$ будет примерно равно $\eta^3$. Рассматриваемый критерий обнулит $\eta$, но $B(0)$ имеет минимальное сингулярное значение возле $\frac{\eta^2}{\sqrt{2}}$, что недопустимо отличается.

Пусть $\sigma$ обозначет нижнюю границу для наименьшего сингулярного значения, тогда самый простой допустимый вариант будет установка $e_i$ равными 0, если значение меньше, чем $tol\cdot\sigma$. Однако, при таком методе, наши числа на побочной диагонали будут зануляться очень долго. Гораздо лучшие критерии можно получить такими способами:
\newpageПусть $\lambda_j$ и $\mu_j$ вычисляются с помощью данных реккурентных соотношений:

\begin{minipage}{0.48\textwidth}
\begin{align*}
\mu_1& = |s_1| \\
\text{for }& j = 1 \text{ to } n-1 \text{ do} \\
&\mu_{j+1} = |s_{j+1}| \cdot \left( \frac{\mu_j}{\mu_j + |e_j|} \right)
\end{align*}
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
\begin{align*}
\lambda_n& = |s_n| \\
\text{for }& j = n-1 \text{ to } 1 \text{ step } -1 \text{ do} \\
&\lambda_j = |s_j| \cdot \left( \frac{\lambda_{j+1}}{\lambda_{j+1} + |e_j|} \right)
\end{align*}
\end{minipage}
\vspace{1em}

\noindent\textit{Критерий сходимости 1a}. Если $|\frac{e_j}{\mu_j}|\leq tol$, то обнуляем $e_j$.\vspace{1em}
\\\textit{Критерий сходимости 1b}. Если $|\frac{e_j}{\lambda_{j+1}}|\leq tol$, то обнуляем $e_j$.\vspace{1em}
\\\textit{Критерий сходимости 2a}. Если сингулярные вектора не требуеются и\linebreak $e^2_{n-1}\leq0.5\cdot tol\cdot [(\frac{\min\limits_{j<n}\mu_j}{\sqrt{n-1}})^2-|s_n|^2]$, то обнуляем $e_{n-1}$.\vspace{1em}
\\\textit{Критерий сходимости 2b}. Если сингулярные вектора не требуеются и\linebreak $e^2_1\leq0.5\cdot tol\cdot [(\frac{\min\limits_{j>1}\lambda_j}{\sqrt{n-1}})^2-|s_1|^2]$, то обнуляем $e_1$.\vspace{1em}

Эти критерии требуют больше вычислительных затрат, чем критерии, реализованные в библиотеке LINPACK, зато помогают избежать ситуаций, когда обнуление значения приводит к недопустимой относительной ошибке.

\subsubsection{Подстановка истинных сингулярных значений}

Вместо приближенной диагональной матрицы \( \widehat{\Sigma} \) подставим истинные сингулярные значения \( \Sigma \):

\begin{equation}
\tilde{B} = \left( \prod J_L \right) \Sigma \left( \prod J_R \right).
\end{equation}

Теперь, изменив левое выражение, мы получили матрицу $\tilde{B}$, отличную от $B$. Для решения задачи остается уточнить повороты, из которых получаются левые и правые сингулярные векторы, чтобы получить точное разложение.

\subsubsection{Уточнение поворотов Гивенса}

Для того, чтобы получить точные левые сингулярные векторы нужно решить следующую оптимизацинную задачу:
\( 
min\| \widehat{B} - B \| 
\)

Этого можно добиться путем изменения углов \( \theta \) поворотов Гивенса \( J_k \):

\begin{equation}
\tilde{B} = J(\theta_1) J(\theta_2) \cdots J(\theta_N) \Sigma J(\theta_{N+1}) J(\theta_{N+2}) \cdots J(\theta_{2N}).
\end{equation}

Для этого существует несколько идей, таких как градиентный спуск, покоординатный спуск и "подкрутки" каждого отдельного угла.

% https://github.com/Kobril/SVD-project/wiki/%D0%A3%D1%82%D0%BE%D1%87%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5-%D0%BF%D0%BE%D0%B2%D0%BE%D1%80%D0%BE%D1%82%D0%BE%D0%B2-%D0%93%D0%B8%D0%B2%D0%B5%D0%BD%D1%81%D0%B0-%D0%B2-implicit-zero%E2%80%90shift-QR-%D0%B8-%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0-QR_zero_proper.h
% Решение задачи оптимизации градиентным спуском не увенчалось большим успехом, так что было принято решение остановится на более простой идеей с небольшими изменениями значений синуса и косинуса в поворотах Гивенса.

\section{"Обратный" поворот Якоби}

\subsection{Принцип сведения $A$ к \Sigma}
В отличие от первой идеи, метод Якоби предполагает работу с изначальной матрицей $A$ при помощи поворотов Якоби (\ref{eq:2:1}), однако это то же самое, что повороты Гивенса. Есть два варианта работы алгоритма:\textit{ двусторонний метод Якоби, односторонний метод Якоби} \cite{Dongarra2018}.
\begin{equation} \label{eq:2:1}
    J(i,j,\theta) = 
    \begin{pmatrix}E&&&&\\
        &c&&s\\
        &&E&&\\
        &-s&&c\\
        &&&&E
    \end{pmatrix},\
    c = \cos(\theta), s =\sin(\theta) 
\end{equation}
\paragraph{Двусторонний метод Якоби}
Этот метод предполагает домножение изначальной матрицы $A$ слева на $J(i,j,\theta)$, справа на $K(i,j,\phi)$ таким образом, чтобы занулить недиагональные элементы, пока $\left\| A - diag(A) \right\|_F > tol$, $tol$ - заданный допуск. 
Отсюда следует рекуррентная последовательность, позволяющая получить сингулярные значения $\Sigma$
\begin{center}
    $A_{(k+1)} = J^T_{(k)}A_{(k)}K_{(k)}, \ A_{(0)}= A, \ A_{(k)}\longrightarrow\Sigma \text{ при } k\to\infty$
\end{center}

Левые повороты при перемножении дают левый сингулярный вектор, $U=J_{(0)}J_{(1)}...$, аналогично, правые повороты - правый сингулярный вектор $V=K_{(0)}K_{(1)}...$

Чтобы определить матрицы $J(i,j,\theta), K(i,j,\phi)$ требуется рассмотреть уравнение
\begin{equation}
    \hat{J}_{(k)}^T\hat{A}_{(k)}\hat{K}_{(k)} = \begin{pmatrix}
        c_J&s_J\\
        -s_J&c_J
    \end{pmatrix}^T
    \begin{pmatrix}
        a_{ii}&a_{ij}\\
        a_{ji}&a_{jj}
    \end{pmatrix}
    \begin{pmatrix}
        c_K&s_K\\
        -s_K&c_K
    \end{pmatrix} = \begin{pmatrix}
        d_{ii} &\\
        &d_{jj}
    \end{pmatrix} = A_{(k+1)},
\end{equation}
где $d_{ii}, d_{jj} -\text{сингулярные числа } \hat{A}_{(k)}$

Однако углы для $J, K$ находятся неоднозначно.
\paragraph{Односторонний метод Якоби}

% TODO Попробовать ужать часть с односторонним методом. Тут он, наверное, имеет только историческую ценность. И нужно объяснить, почему выбирается именно двусторонний метод. Чем он так лучше в нашем случае? + TODO описать критерии схождения этого этапа, вдохновляться первой задачей Миши
Идея метода заключается в том, чтобы использовать матрицу поворота Якоби к изначальной матрице $A$ только с правой стороны - $AJ$ для ортогонализации столбцов $A$, что неявно является двусторонним методом Якоби для матрицы $A^TA$.
Столбцы матрицы сходятся к $U\Sigma$, где левые сингулярные вектора перемножаются с сингулярными значениями.
\begin{center}
     $A_{(0)}=A,\ A_{(k+1)} = A_{(k)}J_{(k)},\ A_{(k)} \longrightarrow U\Sigma, \text{ при } k\to\infty$
 \end{center}
 Отсюда неявно следует, что $A^T_{(k)}A_{(k)} \to \Sigma^2$. Аналогично двустороннему методу Якоби $V = J_{(0)}J_{(1)}...$, получаем правый сингулярный вектор. Нахождение правого сингулярного вектора 
 \begin{center}
     $A_{(\infty)} = U\Sigma \Rightarrow U = A_{(\infty)}\Sigma^{-1}$
 \end{center}
 Заметим, что при малых сингулярных значений $\sigma_i<<1$ выражение $u_{ii} = \frac{a_{ii}}{\sigma_{ii}}$ может быть подсчитано с значительной потери точности, поэтому для исследования методов Якоби будет рассматриваться двусторонний метод.
% Для определения матрицы поворота требуется рассмотреть уравнение.
% \begin{equation}
%     \hat{J}^T_{(k)}\begin{pmatrix}
%         b_{ii}&b_{ij}\\
%         b_{ji}&b_{jj}
%     \end{pmatrix}\hat{J}_{(k)} = \begin{pmatrix}
%         d_{ii} &\\
%         & d_{jj}
%     \end{pmatrix},
% \end{equation}
% где $b_{ij} = a_i^Ta_j, a_i-\text{i-ый столбец }A_{(k)},\ d_{ii}, d_{jj} -\text{собственные числа } A^TA$.

\subsection{Обратный ход}

% Тут встает важный вопрос -- мы опять пытаемся "с нуля" находить вектора, или все же как то используем найденные до этого повороты? не забыть спросить Парфа. В этом и заключается главный вопрос. Исходный метод Якоби итеративно уменьшает норму внедиагональных элементов матрицы; возможно, обратные повороты стоит выбирать так, чтобы каждый поворот приближал текущую матрицу к исходной A по норме разницы некоторых элементов? В репе в ветке со второй идеей уже перенесен код, надо будет глянуть.

Ортогональные повороты \( J_i \) применяются слева к текущей матрице, изменяя её строки, а \( K_i \) применяются справа, изменяя её столбцы. Каждый шаг направлен на приближение текущей матрицы к исходной матрице \( A \).

На каждом шаге результат преобразования сохраняет ортогональность строк и столбцов матрицы, благодаря свойствам ортогональных поворотов. Это гарантирует, что итоговые матрицы \( U \) и \( V \) также будут ортогональными (или унитарными в комплексном случае).

\section{"Наивный" метод}

% https://github.com/Quaret/SVD-project/wiki/%D0%98%D1%81%D0%BF%D0%BE%D0%BB%D1%8C%D0%B7%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5-MR%5E3-%D0%B4%D0%BB%D1%8F-%D1%80%D0%B5%D1%88%D0%B5%D0%BD%D0%B8%D1%8F-BSVD
% https://github.com/MathPerv/SVD-project/wiki/%D0%9D%D0%B0%D0%BF%D1%80%D0%B0%D0%B2%D0%BB%D0%B5%D0%BD%D0%B8%D1%8F-%D0%B8%D1%81%D1%81%D0%BB%D0%B5%D0%B4%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D1%8F-SVD#%D0%B8%D0%B4%D0%B5%D1%8F-3-%D0%BD%D0%B0%D0%B8%D0%B2%D0%BD%D1%8B%D0%B9-%D0%BC%D0%B5%D1%82%D0%BE%D0%B4
% Олежа, прости, эту часть особо не правил, она все еще в том безобразном состоянии, в котором я защищал нирку... но уже 4 утра, я сдуваюсь... Но я верю в тебя!


Сведение задачи SVD к нахождению собственных разложений \( A A^T \) или \( A^T A \) является численно неустойчивым. При умножении матрицы на саму себя число обусловленности возводится в квадрат, что приводит к значительной потере точности.
Существует более устойчивая альтернатива, которая позволяет избежать прямого умножения матрицы на себя \cite{mr3_algo4triagonal_sym_eigen_and_bidiagSVD}. Рассмотрим составление новой матрицы следующего вида:

\begin{equation}
M = \begin{bmatrix} 0 & A^T \\ A & 0 \end{bmatrix}.
\end{equation}

Эту матрицу далее будем называть циклической. В случае бидиагональной матрицы \( B \) задача принимает еще более удобную форму:

\begin{equation}
M_B = \begin{bmatrix} 0 & B \\ B^T & 0 \end{bmatrix}.
\end{equation}

Соотношение между сингулярными числами/векторами и собственными числами/векторами для циклических матриц подробно рассмотрено в разделе 8.6.1 \cite{Golub2013}. 

\subsection{Приведение к форме Голуба-Кахана}

Используя бидиагональность \( B \), можно применить матрицу перестановки \( P \) к \( M_B \), чтобы привести её к \textbf{форме Голуба-Кахана} \( T_{GK} \):

\begin{equation}
T_{GK} = P \begin{bmatrix} 0 & B \\ B^T & 0 \end{bmatrix} P^T = 
\begin{bmatrix}
0 & \alpha_1 & 0 & \dots & 0 \\
\alpha_1 & 0 & \beta_1 & \dots & 0 \\
0 & \beta_1 & 0 & \alpha_2 & \vdots \\
\vdots & \vdots & \alpha_2 & 0 & \beta_2 \\
0 & 0 & \vdots & \beta_{n-1} & 0
\end{bmatrix},
\end{equation}

где \( \alpha_i \) и \( \beta_i \) — элементы исходной бидиагональной матрицы \( B \) на главной и наддиагонали соответственно. 

Таким образом, \( T_{GK} \) представляет собой симметричную тридиагональную матрицу с нулями на главной диагонали. Это особенно удобно, поскольку тридиагональные системы уравнений обладают хорошо изученными и численно устойчивыми методами решения (например, методом прогонки).

\subsection{Решение задачи на основе \( T_{GK} \)}

Предполагая, что сингулярные значения \( B \) известны, можно найти собственные векторы \( T_{GK} \), решая систему уравнений:

\begin{equation}
| T_{GK} - \sigma_i I | = 0,
\end{equation}

где \( \sigma_i \) — сингулярные значения \( B \), а \( I \) — единичная матрица.

Восстановленные собственные векторы \( T_{GK} \) напрямую связаны с сингулярными векторами исходной бидиагональной матрицы \( B \).

Тем не менее, симметричная тридиагональная структура \( T_{GK} \) делает задачу проще и численно стабильнее по сравнению с традиционными методами.

\chapter{Скрученные преобразования}

\newpage
\printbibliography

\end{document}