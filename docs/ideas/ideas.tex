% =================================================================================================
% =================================================================================================
% === IDEA 1 ======================================================================================
% =================================================================================================
% =================================================================================================


\section{Уточнение поворотов Гивенса в алгоритме неявного QR разложения без сдвига}

\subsection{Описание алгоритма}

\subsubsection{Приведение к бидиагональной матрице}

На первом этапе изначальная матрица $A$ приводится к бидиагональной матрице $B$. За счёт этого приведения уменьшается количество ненулевых элементов в матрице и сохраняется заложенная в нее информация о сингулярных значениях. 

% Edit 1 (BUSY, указал ссылку просто на информацию про повороты и преобразование хаусхолдера, другие фразы, которые точно описывают необходимые тезисы найти не могу)
% поискать цитаты
% Теорема Экарта-Янга: Бидиагонализация — этап в SVD, а Хаусхолдер/Гивенс оптимальны для унитарных преобразований.
% Блочные алгоритмы (Demmel et al.) Источник: Demmel, J., et al. "Communication-optimal parallel and sequential QR and LU factorizations" (2008). Оптимизация для кэш-памяти: комбинация Хаусхолдера с блочной обработкой. Ключевой вывод: "Blocked Householder remains the method of choice for dense matrices."
% Рандомизированные методы (Halko et al.) Источник: Halko, N., et al. "Finding structure with randomness" (2011). Для огромных матриц используют рандомизированную бидиагонализацию, но с опорой на те же базовые преобразования.
% Про то, что Гивенс и Хаусхолдер действительно самые крутые. Проверить: Golub & Van Loan, "Matrix Computations" (2013, 4th ed.) Глава 5.4: "Householder Bidiagonalization" Описан стандартный алгоритм приведения матрицы к бидиагональной форме с помощью преобразований Хаусхолдера. Глава 5.1.6: Использование поворотов Гивенса для точечного обнуления элементов. Цитата:  "The most stable and efficient way to compute the bidiagonal form is via Householder transformations." 
Этот этап выполняется за счет алгоритмов, основанных на преобразованиях Хаусхолдера или поворотах Гивенса (подробное описание методов можно найти в главе Orthogonalization and Least Squares~\cite{Golub2013}). 
% В отдельных случаях могут применяться методы Ланцоша и рандомизированные алгоритмы.

\subsubsection{Сведение бидиагональной матрицы к сингулярной}
При поиске сингулярных значений бидиагональной матрицы $B$ применяется алгоритм неявного QR разложения матрицы без сдвига (implicit QR-iteration zero shift) \cite{Demmel1990}, который сохраняет бидиагональную структуру и обеспечивает стабильную итеративную схему.
Для этого последовательно применяются повороты Гивенса, и алгоритм описывается рекуррентным соотношением:

\begin{equation}
B_{(i+1)} = \left( \prod_{p=1}^{n-1} J_{2p} \right) B_{(i)} \left( \prod_{p=1}^{n-1} J_{2p-1} \right) = J_{2n-2} \cdots J_6 J_4 J_2 B_{(i)} J_1 J_3 J_5 \cdots J_{2n-3}, \label{to_bidiag_reccur}
\end{equation}

% Edit 21 (DONE)
%  Между ф-лами (1.1) и (1.2) у B нижний индекс всё время меняется, то l, то t, что сбивает с толку. Пусть будет он тем же самым, что в форму выше, например, B_i.
где $B_i$ - текущий член соотношения, $B_{i+1}$ - следующий член соотношения, \( J_k \) - повороты Гивенса, которые обнуляют внедиагональные элементы матрицы \( B \), приводя её к диагональной \( \Sigma \), при этом:
 \begin{itemize}
     \item Левая часть: индексы $k$ принимают чётные значения
     \begin{equation}
     k = 2p \quad \text{для} \quad p = 1,2,\ldots,n-1,
     \end{equation}
     что даёт последовательность $k = 2,4,6,\ldots,2(n-1)$.
 
     \item Правая часть: индексы $k$ принимают нечётные значения
     \begin{equation}
     k = 2p-1 \quad \text{для} \quad p = 1,2,\ldots,n-1,
     \end{equation}
     что даёт последовательность $k = 1,3,5,\ldots,2(n-1)-1$.
 \end{itemize}

% Edit 2 (DONE)
% Указать параграф/страницу прямо перед цитатой, в тексте. По таким же правилам добавить второй источник Accurate Singular Values of Bidiagonal Matrices. Что то типа "что описано на странице../в параграфе.. [1] или более подробно на странице../в параграфе.. [2]"
% Нашел в lawn03 ссылку на книгу, разбираюсь в ней. Как раз та, что написана ниже. В целом думаю, что Accurate Singular Values of Bidiagonal Matrices также можно указать как 2 источник) Пруф того, что ошибка незначительна B. Parlett, The Symmetric Eigenvalue Problem, Prentice Hall, Englewood Cliffs, New Jersey
\begin{note}
Ошибка, увеличивающаяся с каждым поворотом, остается незначительной и не превышает $p(n) \epsilon ||B_i||$ (более подробно в параграфе The Accuracy of the Computed Singular Vectors \cite{Demmel1990}), где  \( \epsilon \) — машинная точность (\( \epsilon \approx 10^{-16} \) для \texttt{double}, что описано в работе \cite[стр. 94-95]{Golub2013}), \( p(n) \) — умеренно растущая функция от размера матрицы \( n \) (например, \( n \), \( n^2 \), но не \( e^n \)). \( \|B_i\| \) — норма матрицы \( B_i \).
\end{note}

% Edit 3 (PASS)
% найти пруф, добавить цитату. Скорее всего это также в lawn (показалось очевидным)
% Edit 22 (DONE)
% "Через несколько итераций матрица B^(i) приближается к диагональной \hat{\Sigma}, содержащей сингулярные значения..."
% а) i-ое приближение обозначается у Вас то нижним, то верхнем индексом. Наведите полный порядок. (mini done)
% б) Очень желательно это показать выкладками. Математика - наука принципиально доказательная.
Нужно произвести \( n-1 \) поворотов с каждой стороны для бидиагональной матрицы \( B \). Через несколько итераций (\ref{to_bidiag_reccur}) матрица \( B_{(i)} \) приближается к диагональной \( \widehat{\Sigma} \), содержащей сингулярные значения \cite[стр. 11]{Demmel1990}:
\begin{equation}
B = \left( \prod J_L \right) \widehat{\Sigma} \left( \prod J_R \right),
\end{equation}

% Edit 4 (DONE) 
% 3. "На каждом шаге результат преобразования сохраняет ортогональность строк и столбцов матрицы, благодаря свойствам ортогональных поворотов." Это откуда следует? Нужно написать.
где \( \prod J_L \) и \( \prod J_R \) — накопленные произведения левых и правых поворотов Гивенса, из которых при их перемножении получаются сингулярные векторы \( U \) и \( V \). Так как каждая матрица поворота Гивенса $J_i$ является ортогональной, то произведение таких матриц также будет ортогональной матрицей (\ref{ort}). $\widehat{\Sigma}$ — матрица близких к истинным сингулярных значений.
\begin{claim} 
    Пусть $Q_1, Q_2$ -- ортогональные матрицы, тогда: 
    \begin{equation} \label{ort}
        (Q_1Q_2)(Q_1Q_2)^T=Q_1Q_2Q_2^TQ_1^T=Q_1EQ_1^T=Q_1Q_1^T=E
    \end{equation}
\end{claim}



На данном этапе повороты Гивенса применяются итеративно, до того момента как алгоритм не сойдется. Далее описаны теоретические и применяемые на практике критерии оптимизации этого этапа.

% Edit 5 (DONE) (тут все еще не понял, что описывать, если сходимость алгоритма делали по этой теории и  так)
% добавить про реализацию в коде, как там это сейчас работает
% Edit 6 (PASS)
% Также исправить все с активаного залога (рассмотрим) на пассивный (рассматриваются). Поменьше субъективного взгляда - нужно приводить факты (не наш критерий, а просто критерий и тп) 
% Edit 24 (DONE)
% б) Написать про роль этого алгоритма в наших вычисления - для чего он нужен. (mini done)
% в) Не ясно, кем предложены, где применяются  (ссылки на литературу). (mini done)
% г) Не понятно, откуда все эти оценки взялись. (тут имеются в виду оценки для сингулярных значений матрицы B(x), когда мы строим контр пример для условия из LINPACK (1.5), но проблема в том, что в lawn03 указано "it is easy to verify", но что-то для меня это не очевидно и easy to verify, пытаюсь найти как можно такую оценку получить)
\subsubsection{Оптимизации первого этапа}

% Edit 32 (DONE)
% Это и есть первый этап алгоритма. Перенести первое предложение в начало "первого этапа", Переформулировать оставшееся в параграф, описывающий, что все нижеописанные критерии нужны для оптимизации первого этапа
% на данном этапе заменить все упоминания "критерия сходимости" на "критерий оптимизации"
 Поскольку каждая итерация приближает матрицу к диагональной форме, важным аспектом является определение критериев оптимизации, гарантирующих точное выделение сингулярных значений.

% Edit 31 (DONE)
% [4] скинуть мне Accurate singular values of bidiagonal matrices 

Пусть $s_1, s_2,...,s_n$ \--- диагональные элементы, а $e_1, e_2, ..., e_{n-1}$ \--- элементы на побочной диагонали матрицы $B$. 

% Edit 23 (DONE)
% (так в этих равенствах и используют арифметику с плавающей точкой, мол, если s_i настолько мал, что его сумма с соседними числами ничего не добавляет в арифметике с плавающей точкой, то мы обнуляем)
% В настоящих вариантах (1.3) и (1.4) не может быть точных равенств - для плавающей точки они крайне маловероятны, поэтому не практичны.
Критерий оптимизации должен гарантировать, что обнуление $e_i$ сильно не повлияет на точность сингулярных значений. К примеру, код в LINPACK \cite{Dongarra1979} имеет два разных порога, использующих особенности арифметики чисел с плавающей точкой. Элемент обнуляется при наличии большой разницы порядков в арифметическом выражении, что приводит к игнорированию меньшего из них в итоговой сумме.
\begin{align}
\text{Если } (|e_i| + |e_{i-1}| + |s_i| = |e_i| + |e_{i-1}|), \text{ то обнуляем }s_i \label{eq:1:1:1}
\\\text{Если } (|s_i| + |s_{i-1}| + |e_{i-1}| = |s_i| + |s_{i-1}|), \text{ то обнуляем }e_{i-1} \label{eq:1:1:2}
\end{align}

Оба случая не подходят для рассматриваемого алгоритма сведения бидиагональной матрицы к сингулярной. При случае (\ref{eq:1:1:1}) нулевые сингулярные значения будут получаться там, где они были достаточно малы. Второй вариант (\ref{eq:1:1:2})  также является неудовлетворительным, что заметно на примере: $\eta$ -- число настолько маленького значения, что в арифметике с плавающей точкой $1+\eta=1$. Тогда можно рассмотреть матрицу
\begin{center}
$B(x)=\begin{pmatrix}
    \eta^2&1&&\\
    &1&x\\
    &&1&1\\
    &&&\eta^2
\end{pmatrix}$
\end{center}

\begin{lemma} \label{min_sigma_bound}
Пусть даны следующие реккурентные соотношения: \\
\begin{minipage}{0.48\textwidth}
\begin{align*}
\mu_1& = |s_1| \\
\text{for }& j = 1 \text{ to } n-1 \text{ do} \\
&\mu_{j+1} = |s_{j+1}| \cdot \left( \frac{\mu_j}{\mu_j + |e_j|} \right)
\end{align*}
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
\begin{align*}
\lambda_n& = |s_n| \\
\text{for }& j = n-1 \text{ to } 1 \text{ step } -1 \text{ do} \\
&\lambda_j = |s_j| \cdot \left( \frac{\lambda_{j+1}}{\lambda_{j+1} + |e_j|} \right)
\end{align*}
\end{minipage}
\vspace{1em}

Пусть $\underline{\sigma} = \min(\min\limits_j \lambda_j, \ \min\limits_j \mu_j)$, тогда справедлива оценка:
\begin{equation}
\underline{\sigma}\ \leq \sigma_{min}\leq \sqrt{n}\cdot\underline{\sigma}
\end{equation}
\end{lemma}
(Доказательство данной леммы подробно изложено в работе Деммеля и Кахана \cite[стр. 5-6]{Demmel1990})


Для матрицы $B(\eta)$ было вычислено, что $\min\limits_j \lambda_j = \min\limits_j \mu_j = \eta^3$. Следовательно, согласно Лемме \ref{min_sigma_bound}, $\underline{\sigma} = \min(\eta^3, \eta^3) = \eta^3$. Таким образом, получена следующая оценка для наименьшего сингулярного значения:
\begin{align*}
\eta^3 &\leq \sigma_{\min} \leq 2\eta^3 \\
&\sigma_{\min} \approx \eta^3
\end{align*}

При $x=0$ матрица $B(0)$ будет блочно-диагональной, а значит поиск сингулярных значений сводится к поиску сингулярных значений двух отдельных матриц:

\begin{equation*}
B_1=\begin{pmatrix}
\eta^2&1\\
0&1\\
\end{pmatrix},
\quad
B_2=\begin{pmatrix}
1&1\\
0&\eta^2\\
\end{pmatrix}
\end{equation*}

\begin{note}
    Пусть $\sigma_i$ - сингулярные значения матрицы $B$. Тогда $\sigma_i = \sqrt{\lambda_i}$, где $\lambda_i$ - собственные значения матрицы $BB^T$ (или $B^TB$). 
\end{note}

Для $B_1B_1^T$ найдем минимальный корень характеристического многочлена:
\begin{equation*}
\begin{gathered}
\lambda^2-\lambda(\eta^4+2)+\eta^4=0 \\
D = (\eta^4+2)^2-4\eta^4 = 4+\eta^8 \\
\lambda_{min} = \frac{\eta^4+2-\sqrt{4+\eta^8}}{2}
\approx \frac{\eta^4+2-2(1+\frac{\eta^8}{8})}{2} = \frac{\eta^4-\frac{\eta^8}{4}}{2} \approx \frac{\eta^4}{2}
\end{gathered}
\end{equation*}

Следовательно, для матрицы $B(0)$ наименьшее сингулярное значение составляет $\sigma_{min} = \sqrt{\frac{\eta^4}{2}} = \frac{\eta^2}{\sqrt{2}}$. Если критерий обнуления (\ref{eq:1:1:2}) применен к $\eta$ в контексте матрицы $B(\eta)$, возникает существенное расхождение между полученными оценками наименьшего сингулярного значения ($\eta^3$ и $\frac{\eta^2}{\sqrt{2}}$). Это несоответствие может привести к значительной относительной ошибке в вычислениях.


Рассмотрим другие критерии: пусть $\sigma_{min}$ обозначет нижнюю границу для наименьшего сингулярного значения, $tol$ \--- критерий допуска, который зависит от желаемой относительной точности сингулярных значений. Значение $tol$ должно быть меньше 1, но больше машинной точности $\epsilon$. Самый простой допустимый вариант будет установка $e_i$ равными 0, если значение меньше, чем $tol\cdot\sigma_{min}$. При таком методе, числа на побочной диагонали будут обнуляться очень долго. Гораздо лучшие критерии можно получить следующими способами.

Пусть $\lambda_j$ и $\mu_j$ вычисляются с помощью реккурентных соотношений из Леммы \ref{min_sigma_bound}. Тогда будет получена оценка на нижний порог сингулярного значения для соответствующего элемента на побочной диагонали и можно применить следующии критерии оптимизации\cite{Demmel1990}:

\noindent\textit{Критерий оптимизации 1a (обнуление $e_j$)}. Если $|\frac{e_j}{\mu_j}|\leq tol$, то будет обнулен элемент $e_j$.\vspace{1em}
\\\textit{Критерий оптимизации 1b (обнуление $e_j$)}. Если $|\frac{e_j}{\lambda_{j+1}}|\leq tol$, то будет обнулен элемент $e_j$.\vspace{1em}
%$\\\textit{Критерий сходимости 2a}. Если сингулярные векторы не требуеются и\linebreak $e^2_{n-1}\leq0.5\cdot tol\cdot [(\frac{\min\limits_{j<n}\mu_j}{\sqrt{n-1}})^2-|s_n|^2]$, то будет обнулен элемент $e_{n-1}$.\vspace{1em}
%\\\textit{Критерий сходимости 2b}. Если сингулярные векторы не требуеются и\linebreak $e^2_1\leq0.5\cdot tol\cdot [(\frac{\min\limits_{j>1}\lambda_j}{\sqrt{n-1}})^2-|s_1|^2]$, то будет обнулен элемент $e_1$.\vspace{1em}

% Edit 16 (DONE) (Петя сделал сам)
% (Хоть я не раз уже спрашивал, но тут по факту все - это оптимизация сходимости, я не понимаю, как это можно поделить на оптимизацию сходимости и просто критерии сходимости, если мы рассматриваем просто разные методы. Их либо все в один раздел, либо придумать новые критерии разграничения, но я ничего не вижу в том, чтобы оставить именно так, только в конце можно дописать итоговый выбор и мотивацию этого)
% Здесь уже не очень понятно, где какой критерий используется - нужно разделить критерии, которые направлены на ускорение работы алгоритма (за счет обнуления отдельных векторов) и те, что являются критериями сходимости самого алгоритма. Вынести это в 2 отдельных subsubsubsection: 1) сходимость вспомогательного алгоритма, 2) Оптимизации вычисления (над названием можно еще подумать)
% Edit 30 (DONE)
% Добавить пометку, что данные критерии не нужно полностью просчитывать, так как у наших алгоритмах на входе уже есть сингулярные значения
Описанные критерии требуют больше вычислительных затрат, чем критерии, реализованные в библиотеке LINPACK, зато помогают избежать ситуаций, когда обнуление значения приводит к недопустимой относительной ошибке, как это было показано на примере с критерием \eqref{eq:1:1:2}. Также алгоритму на вход подаются истиные сингулярные значения исходной матрицы, что упрощает реализацию критериев.



\subsubsection{Сходимость первого этапа}

В программной реализации алгоритма неявного QR разложения матрицы без сдвига из библиотеки LAPACK (DBDSQR) \cite{LAPACK_DBDSQR} используется критерий сходимости, зависящий от значения параметра \(tol\):
\begin{equation}
\text{tol} = \epsilon \cdot tolmul
\end{equation}
где \(\epsilon\) — машинная точность, а \(tolmul\) — параметр критерия сходимости, задаваемый вручную. Абсолютное значение параметра \(tolmul\) должно быть в пределах от 1 до \(\frac{1}{\epsilon}\), при этом предпочтительные значения находятся в интервале от 10 (для обеспечения быстрой сходимости) до \(\frac{1}{10 \cdot \epsilon}\) (для достижения высокой точности). Значение по умолчанию устанавливается как \(\max(10, \min(100, \epsilon^{-0.125}))\).

При значении \(tol \geq 0\) алгоритм находит сингулярные значения с относительно высокой точностью. Вводится порог обнуления элемента:
\begin{equation}
\text{threshold} = \max(tol \cdot \sigma_{\text{min}}, \text{maxiter} \cdot (n \cdot (n \cdot \text{unfl})))
\end{equation}
где \(\sigma_{\text{min}}\) — нижняя граница минимального сингулярного значения, \(\text{maxiter}\) — максимальное количество итераций (по умолчанию 6), \(n\) — количество вычисляемых сингулярных значений, а \(\text{unfl}\) — минимальное нормализованное положительное число, которое может быть безопасно использовано в арифметике с плавающей точкой.

При \(tol < 0\) используется другой порог обнуления, при котором алгоритм вычисляет сингулярные значения с фиксированной абсолютной точностью:
\begin{equation}
\text{threshold} = \max (|tol| \cdot \sigma_{\text{max}}, \text{maxiter} \cdot (n \cdot (n \cdot \text{unfl})))
\end{equation}
где \(\sigma_{\text{max}}\) — оценка максимального сингулярного значения.

При выполнении алгоритма, если значение \(|e_i| \le threshold\), то оно обнуляется.

\subsubsection{Подстановка истинных сингулярных значений}

Вместо приближенной диагональной матрицы \( \widehat{\Sigma} \) подставим истинные сингулярные значения \( \Sigma \):

\begin{equation}
\tilde{B} = \left( \prod J_L \right) \Sigma \left( \prod J_R \right).
\end{equation}

Теперь, изменив левое выражение, получаем матрицу $\tilde{B}$, отличную от $B$. Для решения задачи остается уточнить повороты, из которых получаются левые и правые сингулярные векторы, чтобы получить точное разложение.

\subsubsection{Уточнение поворотов Гивенса}

Для того, чтобы получить точные левые сингулярные векторы нужно решить следующую оптимизацинную задачу: \(min\| \widehat{B} - B \|\). Этого можно добиться путем изменения углов \( \theta \) поворотов Гивенса \( J_k \):

\begin{equation} \label{b_hat}
\tilde{B} = J(\theta_1) J(\theta_2) \cdots J(\theta_n) \Sigma J(\theta_{n+1}) J(\theta_{n+2}) \cdots J(\theta_{2n}).
\end{equation}
Для этого применяется алгоритм координатного спуска: пусть $\theta = (\theta_1, \theta_2, ..., \theta_{2n})$ — вектор углов поворотов Гивенса. 
Целевая функция минимизации имеет вид:
\begin{equation}
f(\theta) = \frac{1}{2} \| \tilde{B}(\theta) - B \|_F^2
\end{equation}
где: 

\begin{itemize}
\item $\tilde{B}(\theta) = \left( \prod_{k=1}^n J_L^{(k)}(\theta_k) \right) \Sigma \left( \prod_{m=1}^n J_R^{(m)}(\theta_{n+m}) \right)^T$, 
\item $J_L^{(k)}$ — левые повороты Гивенса (действуют на строки), 
\item $J_R^{(m)}$ — правые повороты Гивенса (действуют на столбцы)
\end{itemize}

\begin{note}
\begin{equation}
    \|A\|_F^2 = \langle A, A\rangle = \text{tr}\left(A^TA\right)
\end{equation}
\end{note}
\begin{note}
\begin{equation} \label{j_diff}
    \frac{\partial [J(\theta_1) J(\theta_2) \cdots J(\theta_n)]}{\partial \theta_k} =
    J(\theta_1) J(\theta_2) \cdots  J(\theta_{k-1})\frac{\partial J(\theta_k)}{\partial \theta_k}J(\theta_{k+1}) \cdots J(\theta_n)
\end{equation}
\end{note}

Пусть $A = \tilde{B}(\theta) - B$, тогда частная производная для левого поворота принимает вид:
\begin{equation*}
    \frac{\partial f}{\partial \theta_k} =
    \frac{1}{2}\frac{\partial (\|A\|_F^2)}{\partial \theta_k} =
    \frac{1}{2}\frac{\partial (\langle A, A\rangle)}{\partial \theta_k} =
    \frac{1}{2} \left[ \langle\frac{\partial A}{\partial \theta_k}, A\rangle + \langle A, \frac{\partial A}{\partial \theta_k}\rangle \right]= \langle A, \frac{\partial A}{\partial \theta_k}\rangle = \text{tr}(A^T\cdot\frac{\partial A}{\partial \theta_k})
\end{equation*}

Учитывая \eqref{b_hat} и \eqref{j_diff}:
\begin{equation}
\begin{split}
    \frac{\partial f}{\partial \theta_k} = 
    \text{tr}\left((\tilde{B}(\theta) - B)^T\cdot\frac{\partial [\tilde{B}(\theta) - B]}{\partial \theta_k}\right)=
    \text{tr}\left((\tilde{B}(\theta) - B)^T\cdot\frac{\partial \tilde{B}(\theta)}{\partial \theta_k}\right)
    =\\[6pt] =
    \text{tr}\left((U\Sigma V^T - B)^T\cdot\frac{\partial U}{\partial \theta_k}\Sigma V^T\right) 
    =\\[6pt] =
    \text{tr}\left((U\Sigma V^T - B)^TJ(\theta_1) \cdots  J(\theta_{k-1})\frac{\partial J(\theta_k)}{\partial \theta_k}J(\theta_{k+1}) \cdots J(\theta_n)\Sigma V^T\right)
\end{split}
\end{equation}

\begin{note}
Частная производная для правых поворотов получается аналогичным способом, только под дифференцирование попадает матрица $V^T$:
\begin{equation*}
    \frac{\partial V^T}{\partial \theta_{n+k}} = \left(\frac{\partial V}{\partial \theta_{n+k}}\right)^T = \left(J(\theta_{n+1}) \cdots  J(\theta_{n+k-1})\frac{\partial J(\theta_{n+k})}{\partial \theta_{n+k}}J(\theta_{n+k+1}) \cdots J(\theta_{2n})\right)^T
\end{equation*}
\end{note}

Полный алгоритм для уточнения поворотов Гивенса представлен ниже.

Для $t = 0,1,...,T_{\text{max}}$:
\begin{enumerate}
 \item Обновить левые повороты (строки 1 до $n$):
 \begin{align*}
 \text{Для } k &= 1 \text{ до } n: \\
    \frac{\partial f}{\partial \theta_k} &= 
 \text{tr}\left((U\Sigma V^T - B)^TJ(\theta_1) \cdots  J(\theta_{k-1})\frac{\partial J(\theta_k)}{\partial \theta_k}J(\theta_{k+1}) \cdots J(\theta_n)\Sigma V^T\right) \\
 \theta_k^{(t+1)} &= \theta_k^{(t)} - \eta_L^{(k)} \cdot \frac{\partial f}{\partial \theta_k} \\
 U &= J(\theta_1)J(\theta_2)\cdots J(\theta_n)
 \end{align*}

 \item Обновить правые повороты (столбцы $n+1$ до $2n$):
 \begin{align*}
 \text{Для } m &= 1 \text{ до } n: \\
 \frac{\partial f}{\partial \theta_{n+m}} &= \text{tr}\left((U\Sigma V^T - B)^TU\Sigma \left[J(\theta_{n+1}) \cdots  J(\theta_{n+k-1})\frac{\partial J(\theta_{n+k})}{\partial \theta_{n+k}}J(\theta_{n+k+1}) \cdots J(\theta_{2n})\right]^T\right) \\ \\
 \theta_{n+m}^{(t+1)} &= \theta_{n+m}^{(t)} - \eta_R^{(m)} \cdot \frac{\partial f}{\partial \theta_{n+m}}\\
 V &= J(\theta_{n+1})J(\theta_{n+2})\cdots J(\theta_{2n})
 \end{align*}

 \item Проверить сходимость:
 \begin{equation}
 \max\left( \|\theta_{\text{left}}^{(t+1)} - \theta_{\text{left}}^{(t)}\|_\infty, \|\theta_{\text{right}}^{(t+1)} - \theta_{\text{right}}^{(t)}\|_\infty \right) < \epsilon
 \end{equation}
\end{enumerate}

\textbf{Адаптация шага}:
\begin{equation}
\eta_L^{(k)} = \frac{0.1}{\|\frac{\partial f}{\partial \theta_k}\| + 10^{-8}}, \quad \eta_R^{(m)} = \frac{0.1}{\|\frac{\partial f}{\partial \theta_{n+m}}\| + 10^{-8}}
\end{equation}

Данный эвристический метод является менее вычислительно затратным способом установки скорости обучения, по сравнению с альтернативными методами (Adam, RMSProp), при этом устойчив и позволяет регулировать шаг в обратной зависимости от величины градиента.

% \item \textbf{Структура производных}:
% \begin{itemize}
%     \item Для $J_L^{(k)}$: $\frac{\partial J_L^{(k)}}{\partial \theta_k} = G_L(\theta_k) \otimes I_{\text{rest}}$
%     \item Для $J_R^{(m)}$: $\frac{\partial J_R^{(m)}}{\partial \theta_{N+m}} = I_{\text{rest}} \otimes G_R(\theta_{N+m})$
% \end{itemize}
% где $G_L, G_R$ — генераторы поворотов


% Edit 17 (DONE)
% Описать алгоритм подкруток.
% Второй метод основан на поочередном уточнении углов. На каждой итерации несильно изменяется один параметр $\theta_i$ --- его подбирают так, чтобы уменьшить норму разности матриц $\widehat{B}$ и $B$. Если удается найти угол, уменьшающий норму, его значение сохраняется вместо изначального. В противном случае подбор продолжается. Иначе происходит переход к следующему параметру.
%Поскольку на каждой итерации корректируется только один угол, метод работает неэффективно. (насчет неэффективной работы не проверено, но как будто очевидно)



% =================================================================================================
% =================================================================================================
% === IDEA 2 ======================================================================================
% =================================================================================================
% =================================================================================================



% https://github.com/Kobril/SVD-project/wiki/%D0%A3%D1%82%D0%BE%D1%87%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5-%D0%BF%D0%BE%D0%B2%D0%BE%D1%80%D0%BE%D1%82%D0%BE%D0%B2-%D0%93%D0%B8%D0%B2%D0%B5%D0%BD%D1%81%D0%B0-%D0%B2-implicit-zero%E2%80%90shift-QR-%D0%B8-%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0-QR_zero_proper.h
% Решение задачи оптимизации градиентным спуском не увенчалось большим успехом, так что было принято решение остановится на более простой идеей с небольшими изменениями значений синуса и косинуса в поворотах Гивенса.
\newpage
\section{"Обратный" поворот Якоби}

\subsection{Описание алгоритма}

\subsubsection{Принцип сведения $A$ к $\Sigma$}
% Edit 18 (DONE)
% Вынести определение поворота Якоби/Гивенса в Глоссарий
В отличие от первой идеи, метод Якоби предполагает работу с изначальной матрицей $A\in \mathbb{C}^{m\times n}$ при помощи поворотов Якоби. Есть два варианта работы алгоритма:\textit{ двусторонний метод Якоби, односторонний метод Якоби} ~\cite{Dongarra2018}.
\subsubsubsection{Двусторонний метод Якоби}
Этот метод предполагает домножение изначальной матрицы $A$ размера $n\times n \ $ слева на $J(i,j,\theta)$, справа на $K(i,j,\phi)$ таким образом, чтобы обнулить недиагональные элементы, пока $\left\| A - diag(A) \right\|_F > tol$, где $tol$ - заданный допуск. 
Отсюда следует рекуррентная последовательность, позволяющая получить сингулярные значения $\Sigma$:
\begin{center}
    $A_{(k+1)} = J^T_{(k)}A_{(k)}K_{(k)}, \ A_{(0)}= A, \ A_{(k)}\longrightarrow\Sigma \text{ при } k\to\infty$.
\end{center}

Левые повороты при перемножении дают левый сингулярный вектор, $U=J_{(0)}J_{(1)}...$, аналогично, правые повороты - правый сингулярный вектор $V=K_{(0)}K_{(1)}...$

Чтобы определить матрицы $J(i,j,\theta), K(i,j,\phi)$, требуется рассмотреть уравнение:
\begin{equation}
    \hat{J}_{(k)}^T\hat{A}_{(k)}\hat{K}_{(k)} = \begin{bmatrix}
        c_J&s_J\\
        -s_J&c_J
    \end{bmatrix}^T
    \begin{bmatrix}
        a_{ii}&a_{ij}\\
        a_{ji}&a_{jj}
    \end{bmatrix}
    \begin{bmatrix}
        c_K&s_K\\
        -s_K&c_K
    \end{bmatrix} = \begin{bmatrix}
        d_{ii} &\\
        &d_{jj}
    \end{bmatrix} = \hat{A}_{(k+1)},
\end{equation}
где $d_{ii}, d_{jj} -\text{сингулярные числа } \hat{A}_{(k)}$.

Однако углы для $J, K$ находятся неоднозначно.

% DONE - ищу материалы с развернутым содержанием, добавить еще в глоссарий.
В случае матрицы размера $m \times n, \ m>n$ используется предобработка с использованием QR-разложения ~\cite{10.5555/867597}: 
\begin{center}
    $A = Q_{m\times m}R_{m\times n} = Q\begin{bmatrix}
        R_1\\0
    \end{bmatrix} = \begin{bmatrix}
        Q_1&Q_2
    \end{bmatrix}
    \begin{bmatrix}
        R_1\\0
    \end{bmatrix},
    $
\end{center}
где $R_1 \in \mathbb{R}^{n\times n}, Q_1 \in \mathbb{R}^{m\times n}, Q_2\in \mathbb{R}^{m\times (m-n)}$ ~\cite{Golub2013}. После этого метод Якоби применяется для матрицы $R_1$:
\begin{center}
    $W^TR_1V=\Sigma = \mathrm{diag(\sigma_1,...,\sigma_n)}$.
\end{center}
Определяя 
\begin{center}
    $U = Q\begin{bmatrix}
        W&0\\0&I
    \end{bmatrix},$    
\end{center}
получаем
\begin{center}
     $U^TAV = \mathrm{diag}(\sigma_1,...,\sigma_n)$.
\end{center}


% Edit 7 (DONE) 
% Попробовать ужать часть с односторонним методом. Тут он, наверное, имеет только историческую ценность. И нужно объяснить, почему выбирается именно двусторонний метод. Чем он так лучше в нашем случае? + TODO описать критерии сходимости этого этапа, вдохновляться первой задачей Миши
\subsubsubsection{Односторонний метод Якоби}
В данном методе матрица поворотов применяется к изначальной матрице $A$ только с правой стороны - $AJ$ для ортогонализации столбцов $A$, что неявно является двусторонним методом Якоби для матрицы $A^TA$.
Столбцы матрицы сходятся к $U\Sigma$, где левые сингулярные векторы перемножаются с сингулярными значениями:
\begin{center}
     $A_{(0)}=A,\ A_{(k+1)} = A_{(k)}J_{(k)},\ A_{(k)} \longrightarrow U\Sigma, \text{ при } k\to\infty$.
 \end{center}
 Отсюда неявно следует, что $A^T_{(k)}A_{(k)} \to \Sigma^2$. Аналогично двустороннему методу Якоби $V = J_{(0)}J_{(1)}...$, получаем правый сингулярный вектор. Нахождение левого сингулярного вектора 
 \begin{center}
     $A_{(\infty)} = U\Sigma \Rightarrow U = A_{(\infty)}\Sigma^{-1}.$
 \end{center}

 % Edit 26 (BUSY) - непонятно, как доказать, поднял вопрос на созвоне
 % Все подобные утверждения необходимо подтвердить выкладками. (нужно доказательство)
 При этом, при малых сингулярных значениях $\sigma_i<<1$ выражение $u_{ii} = \frac{a_{ii}}{\sigma_{ii}}$ может быть подсчитано с значительной потери точности, поэтому для исследования методов Якоби будет рассматриваться двусторонний метод.

\subsubsubsection{Критерий сходимости}
% Edit 19 (DONE) (Перечитываю статью о сходимости алгоритмов)
% Добавить указание страницы/главы, где это доказывается для Двустороннего метода Якоби
% Edit 20 (DONE)  замеч.: циклическое поведение -> зацикливание (неточность при переводе)
% Сформулировать что такое циклический порядок/циклическое поведение строк и столбцов и вынести в Глоссарий
Форсайт и Хенрици доказали (стр. 6-7, 10-12) ~\cite{Forsythe1960}, что методы Якоби для спектрального и сингулярного разложения при обходе циклически по строкам или столбцам гарантированно сходятся, если можно выбрать углы поворота так, чтобы они оставались ограниченными значением, строго меньшим $\frac{\pi}{2}$, т.е. $\left|\theta\right| \le b <\frac{\pi}{2}$. При значении $b = \frac{\pi}{2}$ может возникнуть случай, когда $A_{(k+6)}=A_{(k)}$, т.е. алгоритм не сходится (стр. 16-17) ~\cite{Forsythe1960}, вызывая зацикливание. 

% Edit 27 (DONE)
% Да неужели сходимость обеспечивается игнорированием поворотов?! Везде понимаете, что пишете?
% (нужно сделать акцент на том, что пороговые значения позволяют оптимизировать вычисления. Обосновать, как пороговые значения позволяют нам сделать допущение о том, что поворот можно заменить на тривиальный)
Задача о сингулярном разложении может быть рассмотрена как поиск ортогонального базиса столбцов оригинальной матрицы, тогда при рассмотрении ортогонализации пары столбцов $(A^{(k-1)}_p, A^{(k-1)}_q)$ известно (стр. 362-363) ~\cite{deRijk1989}:
\begin{itemize}
    \item Ортогонализация пары столбцов может быть неэффективна в том случае, если их скалярное произведение сравнительно меньше чем среднее скалярное произведение всех пар.
    \item Ортогонализация может нарушить ортогональность других пар столбцов, на задачу которую могло быть потрачено больше вычислительных ресурсов. 
\end{itemize}
Таким образом, игнорирование ортогонализации столбцов, чье скалярное произведение меньше порога, зависящего от каждого обхода,  оптимизирует вычисления, так как в каждом обходе остаются неортогональные столбцы. Это приводит к реализации метода Якоби с использование пороговых значений, для которого известно, что сходимость гарантирована и скорость сходимости ускоряется ~\cite{Dongarra2018}, ~\cite{Forsythe1960}. 

\subsubsection{Обратный ход}
% Edit 8 (DONE) - больше нечего добавить
% Добавить параграф с описанием того, что можно начинать с нуля, а можно и с как то посчитанных векторов
% Тут встает важный вопрос -- мы опять пытаемся "с нуля" находить векторы, или все же как то используем найденные до этого повороты? не забыть спросить Парфа. В этом и заключается главный вопрос. Исходный метод Якоби итеративно уменьшает норму внедиагональных элементов матрицы; возможно, обратные повороты стоит выбирать так, чтобы каждый поворот приближал текущую матрицу к исходной A по норме разницы некоторых элементов? В репе в ветке со второй идеей уже перенесен код, надо будет глянуть.

\subsubsubsection{Выбор начального приближения}
Идея обратного хода предполагает работу с матрицей сингулярных значений $\Sigma$, которая известна заранее, таким образом, чтобы поворотами Якоби слева и справа сводить ее к оригинальной матрице $A \in \mathbb{R}^{m\times n}$. Возможны несколько вариантов работы алгоритма: 
\begin{itemize}
    \item Использовать единичные сингулярные векторы.
    \item Использовать подсчитанные сингулярные векторы.
\end{itemize}
Первый вариант рассматривается, так как единичная матрица является ортогональной с ортонормированными столбцами в ней. 
Идея использовать уже подсчитанные заранее сингулярные векторы позволит поворотами слева и справа уточнить их значения. 

\subsubsubsection{Случай с единичными сингулярными векторами}
Рассмотрим структуру $\Sigma = I_{m\times m}\Sigma_{m\times n}I_{n\times n}$ или

\begin{center}
    $\begin{bmatrix}
        1 & & & \\
         & \ddots & & \\
         & & \ddots & \\
         & & & 1
    \end{bmatrix}_{n \times n}
    \begin{bmatrix}
        \sigma_1 & & \\
         & \ddots & \\
         & & \sigma_n \\
         & & \\
         & & 
    \end{bmatrix}_{n \times m}
    \begin{bmatrix}
        1 & & \\
         & \ddots & \\
         & & 1
    \end{bmatrix}_{m \times m}
    =
    \begin{bmatrix}
        \sigma_1 & & \\
         & \ddots & \\
         & & \sigma_n \\
         & & \\
         & & 
    \end{bmatrix}_{m \times n}$
    
    \vspace{1em} 
    где $\sigma_i$ - сингулярные значения.
\end{center}

Имея в качестве входных параметров матрицу сингулярных значений $\Sigma$ изначальной матрицы $A\in \mathbb{R}^{n\times n}$, применяем повороты Якоби слева и справа таким образом, чтобы результатом преобразования стала исходная матрица $A$. В данной идее поворот Якоби будет формироваться как $J(c) = \begin{bmatrix}
    c&s\\-s&c
\end{bmatrix},\ c\in[0,1]$. При этом перемноженные правые повороты - это правый сингулярный вектор, а перемноженные левые повороты - левый сингулярный вектор. Данный подход представляется в виде итеративного алгоритма:
\begin{equation}
    \tilde{A}_{(k+1)}=J^T_{(k)}\tilde{A}_{(k)}K_{(k)},\ \tilde{A}_{(0)} = \Sigma, \tilde{A}_{(k)} \longrightarrow A, \text{при }k \to \infty.
\end{equation}

Порядок действий:
\begin{enumerate}
    \item Сформировать список элементов матрицы $\tilde{A}_{(k)}$ для последующего обхода, такого что значения $|\tilde{a}_{ij} - a_{ij}|$ располагались в убывающем порядке.
    \item Используя порядок прохождения элементов матрицы, сначала рассмотреть применение левого поворота следущим образом: сформировать функцию \newline$\mathrm{d}(c)= \left\|J^T(i,j,c)\tilde{A}_{k} - A\right\|_F, c \in [0, 1]$, применить минимизацию Брента и получить подходящее значение $c_1$.
    \item Рассмотреть применение правого поворота аналогичным образом: сформировать функцию $\mathrm{d}(c) = \left\| (J^T(i,j,c_1)\tilde{A}_{(k)})K(i,j, c) - A \right\|_F$, применить минимизацию Брента и получить подходящее значение $c_2$.
    \item Сформировать матрицу $\tilde{A}_{(k+1)} =J^T(i,j,c_1)\tilde{A}_{(k)}K(i,j,c_2).$ 
    
% Edit 29 (DONE)
% Раскрыть этап с rot - уже не rot
\end{enumerate}

Итерации продолжаются пока $\left\| \tilde{A}_{k} - A\right\|_F >tol$, $tol$ - заданный допуск. Левый сингулярный вектор $U = J_{(0)}, J_{(1)}, ...$, правый сингулярный вектор $V = (K_{(1)}K_{(2)}...)^T$ .

Для минимизации Брента известно, что сходимость  гарантируется для любой непрерывной функции, у которой известно существование минимума на заданном интервале, причем скорость сходимости может быть как линейной, так и суперлинейной в зависимости от функции ~\cite{Brent_2013}. Например, если вторая производная вблизи точки минимума всегда положительна, то скорость сходимости гарантированно суперлинейная ~\cite{Brent1971}. В случае существовании нескольких минимумов на заданном интервале, минимизация Брента может дойти до локального, но не до глобального минимума. 

\begin{note}
    Существует идея формировать матрицы поворотов Якоби $J(i,j,\theta) = \begin{bmatrix}
        \mathrm{cos}(\theta)&\mathrm{sin}(\theta)\\
        \mathrm{-sin}(\theta)&\mathrm{cos}(\theta)
    \end{bmatrix}$, где $\theta \in [-\frac{\pi}{4}, \frac{\pi}{4}]$, с дальнейшей минимизацией невязки для получения подходящих значений углов. 
\end{note}

% Edit 25 (DONE)
% Посмотреть Edit 4, доказательство
\begin{note}
    Благодаря свойствам ортогональных поворотов \eqref{ort} гарантируется, что итоговые матрицы $U$ и $V$ также будут ортогональными (или унитарными в комплексном случае). 
\end{note}


% Пока пусть будет. Надо спросить у Парфы
\subsubsubsection{Случай с частично посчитанными сингулярными векторами} %DONE
Идея работы алгоритма в данном случае никак не отличается от прошлого пункта, однако меняются начальные значения. Рассмотрим структуру 
\begin{center}
    $\tilde{A}_{m \times n} = \tilde{U}_{m\times m}\Sigma_{m\times n}\tilde{V}^T_{n \times n} = \tilde{A}_{(0)}$,
\end{center}
Аналогично получаем итеративный алгоритм:
\begin{equation}
    \tilde{A}_{(k+1)}= J^T_{(k)}\tilde{A}_{(k)}K_{(k)};
\end{equation}
Все шаги повторяются тем же образом, что в случае с единичными сингулярными векторами.


% =================================================================================================
% =================================================================================================
% === IDEA 3 ======================================================================================
% =================================================================================================
% =================================================================================================



\newpage
\section{''Наивный'' метод}

% https://github.com/Quaret/SVD-project/wiki/%D0%98%D1%81%D0%BF%D0%BE%D0%BB%D1%8C%D0%B7%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5-MR%5E3-%D0%B4%D0%BB%D1%8F-%D1%80%D0%B5%D1%88%D0%B5%D0%BD%D0%B8%D1%8F-BSVD
% https://github.com/MathPerv/SVD-project/wiki/%D0%9D%D0%B0%D0%BF%D1%80%D0%B0%D0%B2%D0%BB%D0%B5%D0%BD%D0%B8%D1%8F-%D0%B8%D1%81%D1%81%D0%BB%D0%B5%D0%B4%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D1%8F-SVD#%D0%B8%D0%B4%D0%B5%D1%8F-3-%D0%BD%D0%B0%D0%B8%D0%B2%D0%BD%D1%8B%D0%B9-%D0%BC%D0%B5%D1%82%D0%BE%D0%B4
% Олежа, прости, эту часть особо не правил, она все еще в том безобразном состоянии, в котором я защищал нирку... Но я верю в тебя!
% Олежа: Нормалин, работаем! :)

% {Олежа:} - это комментарии Олежи

% Edit 9 (TODO)
% Олежа: а мы работаем только при действительно значных матрицах? То есть никакого там эрмитова сопряжения...
% Петя: всегда с эрмитовым. Все определения/формулы/теоремы пытаемся давать/составлять/доказывать для эрмитова случая.
% Олежа: DONE! На самом деле это было просто, ибо эрмитово сопряжение и псевдообращение (по аналогии с обратной (обращением) матрицы) - это обобщения на комплексный случай матриц

\subsection{Описание алгоритма}
\textbf{На вход} принимается \textbf{бидиагональная матрица} \(B\) следующего вида:
\[
        B= diag(\alpha_1,\dots,\alpha_n)+diag_{+1}(\beta_1,\dots,\beta_{n-1}).
\]

\begin{note}
    Если на вход поступает не бидиагональная матрица \(A\), то она может быть сведена к необходимой форме путём известных преобразований.
\end{note}

\begin{note}
    Под \(diag_{+1}\) понимается ''нулевая'' матрица с первой ненулевой наддиагональю.
\end{note}

Также нам известны \textbf{сингулярные значения} из соответствующего разложения исходной матрицы \(B\):
\begin{equation}
        B=U \Sigma V^*,
\end{equation}
где $U,V$ - унитарные матрицы, состоящие из неизвестных нам правых и левых сингулярных векторов $u_{(i,:)},v_{(:,j)}$ соответственно, а $\Sigma$ - диагональная матрица с сингулярными значениями \(\sigma_j\).

\textbf{Наша задача} - вычислить \textbf{левые и правые сингулярные векторы} соответственно.

Существует достаточно понятный и часто используемый в практике приём - взять неизвестную задачу и свести её к известной, для которой имеются методы решения. В нашем случае предлагается свести задачу сингулярного разложения бидиагональной матрицы (в англоязычной литературе именуемой BSVD) к задаче спектрального разложения некоторой трёхдиагональной матрицы (в англоязычной литературе именуемой TSEP).

Есть два подхода, как можно свести одну задачу к другой - получить ''нормальные уравнения'' или использовать ''форму Голуба-Кахана''.

%Олежа: я установил, что нормальные уравнения естественным образом возникают из задачи наименьших квадратов - но я пока источника не нашёл


\subsubsection{Нормальные уравнения}

\subsubsubsection{Построение нормальных уравнений}
На вход поступает бидиагональная матрица $B$ с известными сингулярными значениями $\Sigma$ (или исходная матрица $A$, которая приводится к бидиагональной посредством известных преобразований).

% Edit 31 (TODO)
% "Этот шаг нужен затем, чтобы после приведения BSVD к TSEP (определе­ние этих аббревиатур было дано несколькими абзацами ранее)" Читатель предусматривается совсем склерозным? Это было строк 10 тому назад. 
% Нужно вынести в глоссарий
% Олежа: DONE! (Запись в скобках про уточнение аббревиатур следует удалить(сделал). В этом и была суть правки Парфа).
Наш первый шаг - свести задачу сингулярного разложения бидиагональной матрицы к задаче спектрального разложения трёхдиагональной матрицы. Этот шаг нужен затем, чтобы после приведения BSVD к TSEP воспользоваться уже известными решателями задач спектрального разложения (к примеру, $MR^3$).

Вычислим так называемые ''нормальные уравнения'':

\begin{equation} \label{Norm_eq}
    \begin{split}
        B^*B=V \Sigma (U^*U=I_n) \Sigma V^*=V \Sigma^2 V^*\\
        BB^*=U\Sigma (V^* V=I_n) \Sigma U^*=U \Sigma^2 U^*
    \end{split}
\end{equation}

Заметим, что задача свелась к задаче спектрального разложения трёхдиагональной матрицы, где $\Sigma^2$ - это собственные значения, а правые и левые сингулярные векторы выступают в роли собственных векторов, входящих в соответствующие матрицы (это становится естественным, если вспомнить, что матрицы \(V,U\) - унитарные). 

Распишем одно из нормальных уравнений более подробно, дабы убедиться, что будет получена именно тридиагональная матрица (для наглядности рассмотрим матрицы размером $3\times 3$ с пониманием, что этот пример легко распространяется на случай матриц с размером $n\times n$):
\begin{equation*}
    \begin{split}
        BB^*=\begin{bmatrix}
            \alpha_1 & \beta_1 & 0\\
            0 & \alpha_2 & \beta_2 \\
            0 & 0 & \alpha_3  
        \end{bmatrix}\begin{bmatrix}
            \overline{\alpha_1} & 0 & 0\\
            \overline{\beta_1} & \overline{\alpha_2} & 0 \\
            0 & \overline{\beta_2} & \overline{\alpha_3}  
        \end{bmatrix} =
        \begin{bmatrix}
            \alpha_1^2+\beta^2_1 & \beta_1 \overline{\alpha_2} & 0 \\
            \alpha_2 \overline{\beta_1} & \alpha_2^2+\beta_2^2 & \beta_2 \overline{\alpha_3} \\
            0 & \alpha_3 \overline{\beta_2} & \alpha_3^2
        \end{bmatrix} =\\[6pt]= diag(\alpha_1^2+\beta^2_1, \alpha_2^2+\beta^2_2,\alpha_3^2)+diag_{+1}(\beta_1 \overline{\alpha_2},\beta_2 \overline{\alpha_3})+diag_{-1}(\overline{\beta_1} \alpha_2,\overline{\beta_2} \alpha_3).
    \end{split}
\end{equation*}

Получаются две тридиагональные матрицы с известными нам собственными значениями и неизвестными нам собственными векторами, являющимися искомыми левыми и правыми сингулярными векторами исходной матрицы $B$.

\subsubsubsection{Решения задачи на основе нормальных уравнений}
Мы построили ''нормальные уравнения''. Далее можно отыскать собственные векторы у полученной задачи TSEP, воспользовавшись одной из подходящих процедур библиотеки \textbf{LAPACK}. Полученные собственные векторы с точностью до рассматриваемых ''нормальных уравнений'' (\(B^*B\) или \(BB^*\)) будут искомыми левыми и правыми сингулярными векторами соответственно.

Также в качестве подхода имеет смысл рассмотреть не два нормальных уравнения, а одно (к примеру, $B^*B$). При рассмотрении $B^*B$ находятся левые сингулярные векторы $V$. Правые сингулярные векторы можно найти, вспомнив одну особенность сингулярного разложения:
\begin{equation*}
     B=U \Sigma V^* \Leftrightarrow U\Sigma(V^*V=I)=BV \Rightarrow u_{(:,j)}=u_j= \frac{Bv_{(i,:)}}{\sigma_j}.
\end{equation*}

\subsubsubsection{Проблема нормальных уравнений}

Сведение задачи BSVD к нахождению спектральных разложений \( B B^* \) или \( B^* B \) является численно неустойчивым. Это становится понятным при рассмотрении числа обусловленности, определённого через максимальные и минимальные сингулярные числа:
\[\mu(B) = \frac{\sigma_{max}}{\sigma_{min}}.\]
Заметим, что:
\[ B^*B=(V\Sigma U^*)(U \Sigma V^*)=V (\Sigma)^2 V^*. \]

\begin{note}
    Напомним, что возведение диагональной матрицы в степень эквивалентно возведению всех элементов диагонали в соответствующий показатель. Также напомним, что матрицы \(U\) и \(V\) являются унитарными, то есть \(U^*U=1\) и \(V^*V=1\).
\end{note}

Аналогичное разложение будет иметь и \(BB^*\). Из этого видно:
\[ \mu(B^*B) = \mu(BB^*) = \left( \frac{\sigma_{max}}{\sigma_{min}} \right)^2 =(\mu(B))^2.\]
Заключим, что при умножении матрицы на саму себя число обусловленности возводится в квадрат, что приводит к значительной потере точности. 

Тем не менее, рассмотрение подхода с использованием так называемых ''нормальных уравнений'' имеет смысл для последующего сравнительного анализа.

\subsubsection{Форма Голуба-Кахана}

Существует более устойчивая альтернатива, которая позволяет избежать прямого умножения матрицы на себя ~\cite{mr3_algo4triagonal_sym_eigen_and_bidiagSVD}. Введём в рассмотрение матрицу следующего вида:

\begin{equation}
M = \begin{bmatrix} 0 & A \\ A^* & 0 \end{bmatrix}.
\end{equation}

Эту матрицу далее будем называть циклической. В случае бидиагональной матрицы \( B \in \R{n}{n} \) задача принимает еще более удобную форму:

\begin{equation}
M_B = \begin{bmatrix} 0 & B \\ B^* & 0 \end{bmatrix}.
\end{equation}

%Соотношение между сингулярными числами/векторами и собственными числами/векторами для циклических матриц подробно рассмотрено в разделе 8.6.1 ~\cite{Golub2013}. 

%Олежа: Вот тут, конечно, есть нюанс - в книге Голуба и в немецкой статье рассматриваются чутка разные матрицы (звёздочку надо перекинуть с нижней матрицы на верхнюю в исходной циклической матрице) - в целом, они говорят об одном и том же, но чтобы это понять, с книжкой Голуба надо несколько дополнительных усилий проделать... Не знаю, что с этим нюансом делать. Вообще то, о чём говорится в книге Голуба - это, в сущности, наша форма Джордана-Виландта, приведённая ниже.

% Edit 10 (DONE)
% Олежа: Стоит ли привести эти соотношения?
% Петя: Ждем что скажет Парф.

\subsubsubsection{Приведение к форме Голуба-Кахана}

% Edit 11 (TODO)
% Олежа: тут должна быть ссылка на немецкую диссертацию по MR^3, но без списка литературы я не уверен, что это стоит отдельно прописывать
% Петя: очень даже стоит :) и в библиографию тож надо закинуть, она теперь тоже есть в файле references.bib. Инструкция по добавлению есть в чате
% Олежа: Понял, что эта немецкая диссертация уже упомянута и в тексте нашем, и в референсе 

%Олежа: ~\cite[стр.~121]{mr3_algo4triagonal_sym_eigen_and_bidiagSVD} - форма Д-В.

Рассмотрим \(M_B \in \R{2n}{2n}\). Представим её в \textbf{форме Джордана-Виландта}:

\begin{equation} \label{D-V-eigen}
    M_B= \begin{bmatrix}
        0 & B \\
        B^* & 0
    \end{bmatrix} = J
    \begin{bmatrix}
        -\Sigma & 0 \\
        0 & \Sigma
    \end{bmatrix} J^*,
\end{equation}

где \(J= \frac{1}{\sqrt{2}}\begin{bmatrix}
    U & U \\
    -V & V
\end{bmatrix}\).

В сущности, эта форма задаёт соотношения между сингулярным разложением матрицы \(B\) и спектральным разложением циклической матрицы \(M_B\).

Удостоверимся, что данная форма корректна:

% Edit 12 (BUSY) (Этим занимается Коля - убрать это все и сделать по нормальному. Верю в него, поглядим на результат.)
% \split - это окружение, которое позволяет делать переносы. Тут это необходимо, ибо иначе матрицы уходят за пределы страницы
\begin{equation*}
    \begin{split}
        J \begin{bmatrix} -\Sigma & 0 \\ 0 & \Sigma \end{bmatrix} J^* 
        &= \left( \frac{1}{\sqrt{2}}\right)^2 
            \begin{bmatrix} U & U \\ -V & V \end{bmatrix}
            \begin{bmatrix} -\Sigma & 0 \\ 0 & \Sigma \end{bmatrix}
            \begin{bmatrix} U^* & -V^* \\ U^* & V^* \end{bmatrix}= \\[6pt]
        &= \frac{1}{2} 
            \begin{bmatrix} -U\Sigma & U\Sigma \\ V\Sigma & V\Sigma \end{bmatrix}
            \begin{bmatrix} U^* & -V^* \\ U^* & V^* \end{bmatrix}=\\[6pt] 
        &= \frac{1}{2} 
            \begin{bmatrix} 0 & 2 U\Sigma V^* \\ 2 V\Sigma U^* & 0 \end{bmatrix} = \begin{bmatrix} 0 & B \\ B^* & 0 \end{bmatrix}.
    \end{split}
\end{equation*}

Следующее утверждение необходимо для доказательства, что выражение \eqref{D-V-eigen} можно интерпретировать как спектральное разложение циклической матрицы.

\begin{claim} \label{J-unitary}
    Матрица \(J\) является унитарной.
\end{claim}
\begin{proof}
    \begin{equation*}
        \begin{split}
            J^* J= \frac{1}{2} \begin{bmatrix} U^* &-V^*\\ U^* & V^* \end{bmatrix} \begin{bmatrix} U & U \\ -V & V \end{bmatrix} = \frac{1}{2} \begin{bmatrix}
        U^*U+V^*V & U^*U-V^*V \\ U^*U-V^*V & U^*U+V^*V
    \end{bmatrix} =\\[6pt] = \begin{bmatrix}
        I_n & 0 \\ 0 & I_n
    \end{bmatrix} = I_{2n} \in \mathbb{C}^{2n \times 2n}.
        \end{split}
    \end{equation*}
\end{proof}

Затем, используя бидиагональность \( B \), можно применить матрицу перестановки \( P \) к \( M_B \), чтобы привести её к \textbf{форме Голуба-Кахана} \( T_{GK} \):

\begin{equation}
T_{GK}(B) = P \begin{bmatrix} 0 & B \\ B^* & 0 \end{bmatrix} P^* = diag_{\pm 1}(\alpha_1,\beta_1,\alpha_2,\beta_2,\dots,\alpha_{n-1},\beta_{n-1},\alpha_{n}).
\end{equation}

где \( \alpha_i \) и \( \beta_i \) — элементы исходной бидиагональной матрицы \( B \) на главной и наддиагонали соответственно. 

Опишем принцип действия матрицы перестановки. Пусть дан вектор \(x \in \mathbb{C}^{2n} \). Тогда действие матрицы перестановки производится так

\begin{equation}
    Px=(x_{n+1},x_1, x_{n+2},x_2,\dots,x_{2n},x_n)^*.
\end{equation}
\begin{note}
    Определим также действие транспонированной матрицы перестановки.
    \begin{equation}
        P^*x=(x_2,x_4,\dots,x_{2n},x_1,x_3,\dots,x_{2n-1})^*.
    \end{equation}
\end{note}

\begin{claim} \label{P-unitary}
    Матрица перестановки \(P\) является унитарной.
\end{claim}
\begin{proof}
    Для подтверждения последнего замечания рассмотрим выражение \(P^*(Px)\). Сперва вспомним: 
    \begin{equation}
        Px=(x_{n+1},x_1, x_{n+2},x_2,\dots,x_{2n},x_n)^*.
    \end{equation}
    Заметим, что вторая половина вектора \(x\) \([x_{n+1},x_{n+2},\dots,x_{2n}]\) находится на нечётных позициях, а первая половина вектора  \(x\) \([x_{1},x_{2},\dots,x_{n}]\) - на чётных. 

    В сущности, если рассмотреть действие транспонированной матрицы и переназначить индексы, то можно увидеть, что:
    \begin{equation*}
        \begin{split}
            P^*(Px)=P^*(x_{n+1},x_1, x_{n+2},x_2,\dots,x_{2n},x_n)^*=P^*(x_{1}^\prime,x_2^\prime,\dots,x_{2n-1}^\prime,x_{2n}^\prime)^*=\\[6pt]=(x_2^\prime,x_4^\prime,\dots,x_{2n}^\prime,x_1^\prime,x_3^\prime,\dots,x_{2n-1}^\prime)^*=(x_{1},x_2,\dots,x_{2n-1},x_{2n})^*=I_{2n}x. 
        \end{split}
    \end{equation*}
    То есть \(P^*P=I_{2n}\).
\end{proof}

\begin{note}
    Теоретически матрицу \(P\) можно представить через набор единиц и нулей, расположенных специальным образом в соответствии с умножением строки матрицы на столбец. Однако на практике действие матрицы перестановок легче реализовать в виде непосредственной работы с индексами матрицы или вектора, на которую (матрицу) или на который (вектор) действует матрица перестановок.   
\end{note}

\begin{example}[Форма Голуба-Кахана для матрицы \(2\times2\)]
    В качестве иллюстративного примера реализации формы Голуба-Кахана рассмотрим эту форму на основе матрицы \(S\in \mathbb{C}^{2\times2}\):

\[
    S=\begin{bmatrix} a_1 & b_1 \\ 0 & a_2 \end{bmatrix}.
\]

Составим циклическую матрицу:
\[
    M_S=\begin{bmatrix}
        0 & S \\
        S^* & 0
    \end{bmatrix}=\begin{bmatrix} 0 & 0& a_1 & b_1 \\ 0 & 0 & 0 & a_2 \\ \overline{a_1} & 0 & 0 & 0 \\ \overline{b_1} & \overline{a_2} & 0 & 0  \end{bmatrix}.
\]

Далее будем интерпретировать столбцы матрицы в качестве векторов, на которые будут действовать матрицы перестановки.

\begin{note}
    В этом случае матрица перестановки будет переводить набор индексов \((1,2,3,4)\) в набор \((3,1,4,2)\), а транспонированная матрица перестановки - в набор \((2,4,1,3)\).
\end{note}

Форма Голуба-Кахана для этого примера выходит следующей:
\begin{equation*}
    \begin{split}
        T_{GK}(S)=P\begin{bmatrix} 0 & 0& a_1 & b_1 \\ 0 & 0 & 0 & a_2 \\ \overline{a_1} & 0 & 0 & 0 \\ \overline{b_1} & \overline{a_2} & 0 & 0  \end{bmatrix}P^*
        =
        \begin{bmatrix} \overline{a_1} & 0& 0 & 0 \\ 0 & 0 & a_1 & b_1 \\ \overline{b_1} & \overline{a_2} & 0 & 0 \\ 0 & 0 & 0 & a_2  \end{bmatrix}P^*=\\[6pt]=\left(P\begin{bmatrix} a_1 & 0& b_1 & 0 \\ 0 & 0 &  a_2 & 0 \\ 0 & \overline{a_1} & 0 & 0 \\ 0 & \overline{b_1} & 0 & \overline{a_2}  \end{bmatrix}\right)^*= \begin{bmatrix} 0 & a_1 & 0 & 0 \\ \overline{a_1} & 0 & \overline{b_1} & 0 \\ 0 & b_1 & 0 & a_2 \\ 0 & 0 & \overline{a_2} & 0  \end{bmatrix}.
        \end{split} 
    \end{equation*}
\end{example}

Вновь рассмотрим  форму Голуба-Кахана \(T_{GK}\):
\begin{equation}
T_{GK} (B) = P \begin{bmatrix} 0 & B \\ B^* & 0 \end{bmatrix} P^* .
\end{equation}

Разложим циклическую матрицу по форме Джордана-Виландта:
\begin{equation}
T_{GK} = P J\begin{bmatrix} -\Sigma & 0 \\ 0 & \Sigma \end{bmatrix}J^* P^* = (PJ) \begin{bmatrix} -\Sigma & 0 \\ 0 & \Sigma \end{bmatrix} (PJ)^*=Q\Lambda Q^*.
\end{equation}

Последнее означает, что комбинация форм Голуба-Кахана и Джордана-Виландта задаёт, в сущности, спектральное разложение формы Голуба-Кахана.

% Edit 13 (TODO)
% Олежа:  стоит ли здесь уточнить комплексный случай? Да, я именно про унитарные матрицы.
% Петя: Стоит-стоит.

\begin{claim}
     Матрица \(PJ\) является унитарной.
\end{claim}
\begin{proof}
   Как показано в утверждениях  \ref{J-unitary} и \ref{P-unitary},  матрицы \(P\) и \(J\) - унитарные. Заметим, что:

    \[(PJ)^*PJ=J^* P^* P J=J^* (P^* P=I_{2n}) J=J^*J=I_{2n}.\]
    
\end{proof}


В итоге задача сингулярного разложения свелась к задаче спектрального разложения. 

\begin{note}
    В полученной задаче о поиске спектрального разложения стоит уточнить обозначения:
    \[
        \Lambda = diag(-\sigma_n,\dots,-\sigma_1,\sigma_1,\dots,\sigma_n) \ - \text{спектр,} 
    \]
    \[
        Q=PJ \ - \text{набор собственных векторов.}
    \]
\end{note}

Отдельно рассмотрим вид собственных векторов полученной задачи:
\begin{equation} \label{eq:eigenvector}
    \begin{split}
        Q=PJ \Rightarrow q_{i}=\frac{1}{\sqrt{2}}P(u_1,\dots,u_n,\pm v_1,\dots,\pm v_n)^* \Leftrightarrow \\[6pt]  \Leftrightarrow \sqrt{2}q_i=(\pm v_1,u_1,\pm v_2,u_2,\dots,\pm v_n,u_n)^*, 
    \end{split}
\end{equation}
где \(q_i\) - собственный вектор.


Таким образом, \( T_{GK} \) представляет собой симметричную (с точностью до комплексного сопряжения) тридиагональную матрицу с нулями на главной диагонали. Это особенно удобно, поскольку тридиагональные системы уравнений обладают хорошо изученными и численно устойчивыми методами решения (например, методом прогонки).

\subsubsubsection{Решение задачи на основе $T_{GK}$}


Предполагая, что сингулярные значения бидиагональной матрицы \( B \) известны, можно найти собственные векторы \( T_{GK} \), решая систему уравнений:
% Edit 14 (BUSY) (спросим у Парфа/Дроздова)
% Олежа: Xм, кажется мы тут решаем обычное уравнение на поиск собственных векторов и значений, а-ля: Ax=lx => (A-l*I)x=0. И как будто тут пропустили x в основном тексте, а ведь x - это и есть собственные векторы. Я всё-таки исправил это уравнение. 

\begin{equation}
\label{eq:eigen_eq}
( T_{GK} - \sigma_i I )x = 0,
\end{equation}
где \( \sigma_i \) — сингулярные значения \( B \), \( I \) — единичная матрица, а \(x\) - искомый собственный вектор.

Проблема системы~\eqref{eq:eigen_eq} в том, что матрица \((T_{GK}-\sigma_iI)\) -- вырожденная. В ином случае собственный вектор был бы единственно тождественен нулевому \(x\equiv 0\). Это замечание означает неприменимость стандартного метода прогонки.

Для решения этой системы уравнений воспользуемся \textbf{методом обратных итераций}~\cite[стр. 111]{verzhbitsky2021}. 

Нам известны точные собственные значения \(\pm\sigma_i\) матрицы \(T_{GK}\). Возьмём в рассмотрение некоторое число \(\lambda\), близкое к какому-то собственному значению \(\sigma_j\) (положим \(\lambda=\sigma_j+\varepsilon\), где \(\varepsilon=10^{-3}\)). 

Рассмотрим итерационную систему вида
\begin{equation}
    \label{eq:eigen_it_eq}
    (T_{GK}-\lambda I)z^{(k)}=f^{(k-1)}.  
\end{equation}
Представим векторы \(z^{(k)}\) и \(f^{(k-1)}\) в виде линейной комбинации собственных векторов (то есть разложим их по базису этих векторов)
\begin{equation}
\label{eq: sum_of_eigen}
z^{(k)}=\sum_{i=1}^na^{(k)}_{i}q_{i}, \quad f^{(k-1)}=\sum_{i=1}^nb^{(k-1)}_{i}q_{i},
\end{equation}
где \(q_i\) - набор собственных векторов. Тогда
\[
    \sum_{i=1}^n(\sigma_{i}-\lambda)a^{(k)}_{i}q_{i}=\sum_{i=1}^{n}b^{(k-1)}_iq_i.
\]
Поскольку набор собственных векторов линейно независим, то
\begin{equation}
\label{eq: coef}
    a_i^{(k)}=\frac{b^{(k-1)}_i}{\sigma_i-\lambda}, \quad \forall i \in \{1,2,...,n\}.
\end{equation}
Видно, что при \(i=j\) в~\eqref{eq: coef} получаем наибольший коэффициент \(a_i^{(k)}\) возле вектора \(q_j\)~(см. \eqref{eq: sum_of_eigen}). Благодаря нормировке
\[
    f^{(k)}=\frac{z^{(k)}}{\Arrowvert z^{(k)}\Arrowvert}
\]
происходит контроль за ростом этого коэффициента.

Итерации происходят до тех пор, пока \(\Arrowvert f^{(m)}-f^{(m-1)}\Arrowvert < \varepsilon\), где \(\varepsilon\) - заданная точность. В качестве начального приближения следует выбрать вектор \(z^{(0)}\) такой, что \(\Arrowvert z^{(0)} \Arrowvert=1\)~\cite[стр. 113]{verzhbitsky2021}. Причём, стоит выбрать его таким, чтобы он не получился ортогональным искомому собственному вектору \(q_j\), соответствующему собственному значению \(\sigma_j\).

Вычисление системы~\eqref{eq:eigen_it_eq} стоит провести, учитывая тридиагональный вид (хотя это и не обязательно -- достаточно использовать любой устойчивый метод решения СЛАУ). Специализированным для таких случаев методом является \textbf{метод прогонки}. В данном случае он корректен, потому что матрица вида \((T_{GK}-\lambda I)\) уже не является вырожденной. 

Но стандартный метод прогонки (она также называется \textbf{монотонной}) устойчив при \textbf{диагональном преобладании}~\cite[стр. 53]{verzhbitsky2021}, которое в данной ситуации не обязано соблюдаться.  Это может привести к неадекватному поведению прогоночных коэффициентов (в частности, к их возрастанию). Поэтому будет использован метод \textbf{немонотонной прогонки}. В сущности, это частный случай метода Гаусса с выбором главного элемента.

Пусть дана следующая итерационная система
\begin{align*}
    &(T_{GK}-\lambda I)z^{(k)}=z^{(k-1)} \Leftrightarrow \\
    & \Leftrightarrow \begin{cases}
        -\lambda z^{(k)}_1+\beta_1z^{(k)}_2=z^{(k-1)}_1,\\
        \beta_{1}z_1^{(k)}-\lambda z^{(k)}_2+\alpha_{1}z_3^{(k)}=z^{(k-1)}_2,\\
        \alpha_{1} z^{(k)}_2-\lambda z_3^{(k)}+\beta_2 z^{(k)}_4=z^{(k-1)}_3,\\
        \dots\\
        \alpha_{n-2} z^{(k)}_{n-1}-\lambda z_n^{(k)}=z_n^{(k-1)}.
    \end{cases}
\end{align*}
Она представима в виде
\begin{align*}
\begin{cases}
    c_0y_0-b_0y_1=f_0,\\
    -a_{i}y_{i-1}+c_{i}-b_{i}y_{i+1}=f_{i} \quad i=1 ,2,\dots,N-1,\\
    -a_Ny_{N-1}+c_{N}y_N=f_N,
\end{cases}
\end{align*}
где \(a_i=b_i\) - элементы над- и под- диагонали (знаки у элементов -- повествовательная формальность, не имеющая принципиальных причин), \(c_i=-\lambda\), \(y_i=z^{(k)}_i\), \(f_i=z_i^{(k-1)}\).

Тогда на \(l\)-ом этапе имеется следующая система, полученная монотонным методом прогонки (иными словами, была получена система, в которой последовательно исключили \(y_{l-1}\))
\begin{align}
\begin{cases}
    (c_l-a_l \gamma_l)y_l-b_ly_{l+1}=f_{l}+a_l \rho_l, \quad i=l\\
    \label{eq:iter}
    -a_i y_{i-1}+c_iy_i-b_iy_{j+1}=f_i, \quad l+1 \leq i \leq N-1,\\
    -a_Ny_{N-1}+c_Ny_N=f_N,
\end{cases}
\end{align}
где \(\gamma_l, \rho_l\) -- прогоночные коэффициенты. 
Вид первого уравнения системы~\eqref{eq:iter} был получен стандартным методом прогонки
\begin{align*}
    &-a_ly_{l-1}+c_{l}y_{l}-b_{l}y_{l+1}=f_{l}  \Leftrightarrow \\
    &\Leftrightarrow (y_{l-1}=\gamma_l y_l + \rho_l) \Rightarrow (c_l-a_l\gamma_l)y_l-b_ly_{l+1}=f_{l}+a_{l}\rho_l. 
\end{align*}
Если принять \(C=c_l-a_l \gamma_l, \ F=f_l+a_l\rho_l, \ A=a_l, m_l\leq l, \Phi=f_{l+1}\), то будет установлен  \textbf{инвариант} алгоритма:

\begin{cases}
    C y_{m_l} - b_l y_{l+1}=F, \quad i=l,\\
    -A y_{m_l}+c_{l+1}y_{l+1}-b_{l+1}y_{l+2}=\Phi, \quad i=l+1,\\
    -a_{l+2} y_{l+1} +c_{l+2} y_{l+2} - b_{l+2} y_{l+3} = f_{l+2}, \quad i=l+2,\\
    -a_iy_{i-1}+c_{i}y_i+b_iy_{i+1}=f_i, \quad l+3 \leq i \leq N-1,\\
    -a_N y_{N-1} + c_{N} y_{N}=f_{N}, \quad i=N.
\end{cases}

Рассмотрим детально шаг алгоритма, для простоты считая \(m_l=l\) (на деле индекс \(m_l\) мог быть получен предшествующем ему неоднократным выбором главного элемента и он необязательно равен \(l\)).
\begin{itemize}
\item Если \(|c_l-a_l\gamma_l|\geq |b_l|\), то \textbf{соблюдено} диагональное преобладание, следовательно, продолжается прямой ход монотонной прогонки.
То есть
\begin{equation}
\label{eq: first_it}
    y_l-\gamma_{l+1}y_{l+1}=\rho_{l+1},
\end{equation}
где \(\gamma_{l+1}=\frac{b_l}{c_l-a_l \gamma_l}\), \(\rho_{l+1}=\frac{f_{l}+a_{l}\rho_{l}}{c_l-a_l\gamma_l}\). Таким образом был исключён \(y_l\).

Подставив выведенное соотношение~\eqref{eq: first_it}  в \eqref{eq:iter} при \(i=l+1\), получим
\begin{align}
    -a_{l+1}(\gamma_{l+1}y_{l+1}+\rho_{l+1})+c_{l+1}y_{l+1}-b_{l+1}y_{l+2}=f_{l+1} \Leftrightarrow \\ 
    \Leftrightarrow y_{l+1} (c_{l+1}-a_{l+1}\gamma_{l+1})+y_{l+2}b_{l+1}=f_{l+1}+a_{l+1}\rho_{l+1}.
\end{align}
Заменив \(C=c_{l+1}-a_{l+1}\gamma_{l+1}, \ A=a_{l+2}, \ \Phi=f_{l+2}, \ F=f_{l+1} + a_{l+1} \rho_{l+1}\),  придём к инварианту алгоритма (\(i=l+2\) итерация остаётся неизменной).

\itemЕсли \(|c_l-a_l \gamma_l|<|b_l|\), то диагонально преобладание \textbf{нарушается}. Тогда
\begin{equation}
    \label{eq: select_item}
    y_{l+1} -\gamma_{l+1} y_{l}=\rho_{l+1},
\end{equation}
где \(\gamma_{l+1}=\frac{c_l-a_l\gamma_l}{b_l}\), \(\rho_{l+1}=-\frac{f_{l+1}+a_l\rho_l}{b_l}\). Таким образом исключается \(y_{l+1}\). 

Подставив~\eqref{eq: select_item}  в \eqref{eq:iter} при \(i=l+1\), получим
\begin{align*}
    -a_{l+1}y_l+c_{l+1}(\gamma_{l+1}y_l+\rho_{l+1})-b_{l+1}y_{l+2}=f_{l+1} \Leftrightarrow \\ \Leftrightarrow y_l (c_{l+1} \gamma_{l+1}-a_{l+1})-y_{l+2}b_{l+1}=f_{l+1}-c_{l+1}\rho_{l+1}.
\end{align*}
Рассмотрев аналогично выражение \eqref{eq:iter} при \(i=l+2\), придём к следующему равенству
\begin{align*}
    -a_{l+2}(\gamma_{l+1}y_l+\rho_{l+1})+c_{l+2}y_{l+2}-b_{l+2}y_{l+3}=f_{l+2} \Leftrightarrow \\
    \Leftrightarrow  y_{l}(-a_{l+2}\gamma_{l+1})+c_{l+2}y_{l+2}-b_{l+2}y_{l+3}=f_{l+2}+\rho_{l+1}a_{l+2}.
\end{align*}
Заменив \(A=-a_{l+2}\gamma_{l+1}, \ \Phi=f_{l+2}+\rho_{l+1}a_{l+2}, \ F=f_{l+1}-c_{l+1}\rho_{l+1}, \ C = c_{l+1} \gamma_{l+1} - a_{l+1},\) придём к инварианту алгоритма.

Таким образом, описан шаг процесса исключения с выбором главного элемента.
\end{itemize}

Авторы классического учебника по методам решения подобных систем отмечают~\cite[стр. 96]{samarskiy1978}:

<<Для предлагаемого алгоритма порядок вычисления неизвестных может иметь немонотонный характер. Это требует хранения информации о том, какое неизвестное вычисляется через какое, уже найденное на предыдущих шагах неизвестное, при помощи прогоночных коэффициентов \(\gamma_{l+1}\) и \(\rho_{l+1}\).  Эту информацию можно хранить в виде двух целочисленных множеств индексов \(\theta\) и \(\kappa\): \(\theta=\{\theta_i, 1\leq i \leq N\}\), \(\kappa=\{\kappa_i,1\leq i\leq N\}\), так что неизвестные находятся по формулам \(y_m=\gamma_{i+1}y_{n}+\rho_{i+1}, \ m=\theta_{i+1}, \ n=\kappa_{i+1}, \ i= N-1,N-2,\dots,0.\) Множества \(\theta\)  и \(\kappa\) строятся на прямом ходе метода.>>

Тем самым авторами упомянутого ранее учебника с приведённой выше цитатой изложен пример, как по полученным данным построить решение.

Поскольку известны все собственные значения формы Голуба-Кахана, то, воспользовавшись методом обратных итераций, проводя вычисления методом немонотонной прогонки, можно получить набор собственных векторов. Он напрямую связан с сингулярными векторами исходной бидиагональной матрицы \( B \) (см. выражение \eqref{eq:eigenvector}).

\subsubsubsection{Проблемы решения задачи на основе $T_{GK}$}

Предложенное решение почти гарантировано будет обречено в случае повторяющихся собственных значений.
% Edit 15 (BUSY) (спросим у Парфа/Дроздова)
%Олежа: Отдельно возникает проблема потери ортогональности у собственных векторов. Она решается путём ортогональности полученного набора векторов - но это утверждение я пока что не могу доказать

% Олежа: метод обратных итераций(для нахождения собственных векторов) + метод прогонки(из-за трёхдиагональной структуры) = ?


% =================================================================================================
% =================================================================================================
% =================================================================================================
% =================================================================================================
% =================================================================================================

