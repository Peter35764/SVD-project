\section{Уточнение поворотов Гивенса в implicit zero-shift QR}

\subsection{Описание алгоритма}

\subsubsection{Приведение к бидиагональной матрице}

На первом этапе изначальная матрица $A$ приводится к бидиагональной матрице $B$. Производя это приведение мы уменьшаем количество ненулевых элементов в матрице и сохраняем заложенную в нее информацию.

% Edit 1 
% BUSY (в поиске конкретных фраз про способы)
% поискать цитаты
% Теорема Экарта-Янга: Бидиагонализация — этап в SVD, а Хаусхолдер/Гивенс оптимальны для унитарных преобразований.
% Блочные алгоритмы (Demmel et al.) Источник: Demmel, J., et al. "Communication-optimal parallel and sequential QR and LU factorizations" (2008). Оптимизация для кэш-памяти: комбинация Хаусхолдера с блочной обработкой. Ключевой вывод: "Blocked Householder remains the method of choice for dense matrices."
% Рандомизированные методы (Halko et al.) Источник: Halko, N., et al. "Finding structure with randomness" (2011). Для огромных матриц используют рандомизированную бидиагонализацию, но с опорой на те же базовые преобразования.
% Про то, что Гивенс и Хаусхолдер действительно самые крутые. Проверить: Golub & Van Loan, "Matrix Computations" (2013, 4th ed.) Глава 5.4: "Householder Bidiagonalization" Описан стандартный алгоритм приведения матрицы к бидиагональной форме с помощью преобразований Хаусхолдера. Глава 5.1.6: Использование поворотов Гивенса для точечного обнуления элементов. Цитата:  "The most stable and efficient way to compute the bidiagonal form is via Householder transformations." 
Обычно этот этап выполняется за счет алгоритмов, основанных на преобразованиях Хаусхолдера или поворотах Гивенса. В отдельных случаях могут применяться методы Ланцоша и рандомизированные алгоритмы. 

\subsubsection{Сведение бидиагональной матрицы к сингулярной}

Теперь, имея $B$ можно привести ее к диагональной с помощью последовательных поворотов Гивенса. Этот алгоритм можно описать рекуррентным соотношением~\cite{Demmel1990}:

\begin{equation}
B_{(i+1)} = J_{2n-2} \cdots J_6 J_4 J_2 B_{(i)} J_1 J_3 J_5 \cdots J_{2n-3},
\end{equation}

где \( J_k \) - повороты Гивенса, которые зануляют внедиагональные элементы матрицы \( B \), приводя её к диагональной \( \Sigma \). 

% Edit 2 (DONE)
% найти цитату для уточнения точности машинного эпсилон
При этом ошибка, увеличивающаяся с каждым поворотом, остается незначительной и не превышает $p(n) \epsilon ||B_\ell||$ ~\cite{Demmel1990}, где  \( \epsilon \) — машинная точность (\( \epsilon \approx 10^{-16} \) для \texttt{double}~\cite{Golub2013}), \( p(n) \) — умеренно растущая функция от размера матрицы \( n \) (например, \( n \), \( n^2 \), но не \( e^n \)). \( \|B_t\| \) — норма матрицы \( B_t \).
% BUSY (Нашел в lawn03 ссылку на книгу, разбираюсь в ней. Как раз та, что написана ниже. В целом думаю, что Accurate Singular Values of Bidiagonal Matrices также можно указать как 2 источник) Пруф того, что ошибка незначительна B. Parlett, The Symmetric Eigenvalue Problem, Prentice Hall, Englewood Cliffs, New Jersey

% Edit 3 (DONE) 
% найти пруф, добавить цитату. Скорее всего это также в lawn (показалось очевидным)
Нужно произвести \( n-1 \) поворотов с каждой стороны для бидиагональной матрицы \( B \). Через несколько итераций матрица \( B^{(i)} \) приближается к диагональной \( \widehat{\Sigma} \), содержащей сингулярные значения:
\begin{equation}
B = \left( \prod J_L \right) \widehat{\Sigma} \left( \prod J_R \right),
\end{equation}

% Edit 4 (DONE) 
% объяснить сохранение ортогональности на сингулярных векторах
где \( \prod J_L \) и \( \prod J_R \) — накопленные произведения левых и правых поворотов Гивенса, из которых при их перемножении получаются сингулярные вектора \( U \) и \( V \). Так как матрица поворота Гивенса $J$ является ортогональной, то произведение таких матриц также будет ортогональной матрицей.  $\widehat{\Sigma}$ — матрица близких к истинным сингулярных значений.

В данном случае повороты Гивенса применяются итеративно, до того момента как алгоритм не сойдется. Далее описаны теоретические и применяемые на практике критерии схождения.

% Edit 5 (BUSY) (в коде сходимость взята наугад: i<10 i++)
% добавить про реализацию в коде, как там это сейчас работает, а потом подумаем про компановку
% Edit 6 (DONE)
% Также исправить все с активаного залога (рассмотрим) на пассивный (рассматриваются). Поменьше субъективного взгляда - нужно приводить факты (не наш критерий, а просто критерий и тп) 
\subsubsubsection{\textbf{Схождение вспомогательного алгоритма}}

В разделе рассматриваются критерии сходимости для предложенного алгоритма приведения бидиагольной матрицы $B$ к диагональной $\Sigma$. Пусть $s_1, s_2,...,s_n$ \--- диагональные элементы, а $e_1, e_2, ..., e_{n-1}$ \--- элементы на побочной диагонали матрицы $B$. 

Критерий сходимости должен гарантировать, что обнуление $e_i$ сильно не повлияет на точность сингулярных значений. К примеру, код в LINPACK имеет два разных порога для обнуления элемента:
\begin{align}
\text{Если } (|e_i| + |e_{i-1}| + |s_i| = |e_i| + |e_{i-1}|), \text{ то обнуляем }s_i
\\\text{Если } (|s_i| + |s_{i-1}| + |e_{i-1}| = |s_i| + |s_{i-1}|), \text{ то обнуляем }e_{i-1}
\end{align}

Оба случая не подходят для рассматриваемого алгоритма. При случае $(1.1)$ нулевые сингулярные значения будут получаться там, где их не было до этого. Второй вариант $(1.2)$ также является неудовлетворительным. Это можно заметить на примере: возьмем $\eta$ настолько маленького значения, что в арифметике с плавающей точкой $1+\eta=1$. Тогда можно рассмотреть матрицу
\begin{center}
$B(x)=\begin{pmatrix}
    \eta^2&1&&\\
    &1&x\\
    &&1&1\\
    &&&\eta^2
\end{pmatrix}$
\end{center}

\noindentПри $x=\eta$ самое минимальное сингулярное значение $B(\eta)$ будет примерно равно $\eta^3$. Рассматриваемый критерий обнулит $\eta$, но $B(0)$ имеет минимальное сингулярное значение возле $\frac{\eta^2}{\sqrt{2}}$. Полученные оценки $\eta^3$ и $\frac{\eta^2}{\sqrt{2}}$ слишком разнятся, что приведет к большой относительной ошибке.

Пусть $\sigma$ обозначет нижнюю границу для наименьшего сингулярного значения, $tol$ \--- критерий допуска, который зависит от желаемой относительной точности сингулярных значений. Значение $tol$ должно быть меньше 1, но больше машинной точности $\epsilon$. Самый простой допустимый вариант будет установка $e_i$ равными 0, если значение меньше, чем $tol\cdot\sigma$. При таком методе, числа на побочной диагонали будут зануляться очень долго. Гораздо лучшие критерии можно получить такими способами~\cite{Demmel1990}:
\newpageПусть $\lambda_j$ и $\mu_j$ вычисляются с помощью данных реккурентных соотношений:

\begin{minipage}{0.48\textwidth}
\begin{align*}
\mu_1& = |s_1| \\
\text{for }& j = 1 \text{ to } n-1 \text{ do} \\
&\mu_{j+1} = |s_{j+1}| \cdot \left( \frac{\mu_j}{\mu_j + |e_j|} \right)
\end{align*}
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
\begin{align*}
\lambda_n& = |s_n| \\
\text{for }& j = n-1 \text{ to } 1 \text{ step } -1 \text{ do} \\
&\lambda_j = |s_j| \cdot \left( \frac{\lambda_{j+1}}{\lambda_{j+1} + |e_j|} \right)
\end{align*}
\end{minipage}
\vspace{1em}

\noindent\textit{Критерий сходимости 1a (обнуление $e_j$)}. Если $|\frac{e_j}{\mu_j}|\leq tol$, то будет обнулен элемент $e_j$.\vspace{1em}
\\\textit{Критерий сходимости 1b (обнуление $e_j$)}. Если $|\frac{e_j}{\lambda_{j+1}}|\leq tol$, то будет обнулен элемент $e_j$.\vspace{1em}
%$\\\textit{Критерий сходимости 2a}. Если сингулярные вектора не требуеются и\linebreak $e^2_{n-1}\leq0.5\cdot tol\cdot [(\frac{\min\limits_{j<n}\mu_j}{\sqrt{n-1}})^2-|s_n|^2]$, то будет обнулен элемент $e_{n-1}$.\vspace{1em}
%\\\textit{Критерий сходимости 2b}. Если сингулярные вектора не требуеются и\linebreak $e^2_1\leq0.5\cdot tol\cdot [(\frac{\min\limits_{j>1}\lambda_j}{\sqrt{n-1}})^2-|s_1|^2]$, то будет обнулен элемент $e_1$.\vspace{1em}

Эти критерии требуют больше вычислительных затрат, чем критерии, реализованные в библиотеке LINPACK, зато помогают избежать ситуаций, когда обнуление значения приводит к недопустимой относительной ошибке.

\subsubsection{Подстановка истинных сингулярных значений}

Вместо приближенной диагональной матрицы \( \widehat{\Sigma} \) подставим истинные сингулярные значения \( \Sigma \):

\begin{equation}
\tilde{B} = \left( \prod J_L \right) \Sigma \left( \prod J_R \right).
\end{equation}

Теперь, изменив левое выражение, мы получили матрицу $\tilde{B}$, отличную от $B$. Для решения задачи остается уточнить повороты, из которых получаются левые и правые сингулярные векторы, чтобы получить точное разложение.

\subsubsection{Уточнение поворотов Гивенса}

Для того, чтобы получить точные левые сингулярные векторы нужно решить следующую оптимизацинную задачу:
\( 
min\| \widehat{B} - B \| 
\)

Этого можно добиться путем изменения углов \( \theta \) поворотов Гивенса \( J_k \):

\begin{equation}
\tilde{B} = J(\theta_1) J(\theta_2) \cdots J(\theta_N) \Sigma J(\theta_{N+1}) J(\theta_{N+2}) \cdots J(\theta_{2N}).
\end{equation}

Для этого существует несколько идей, таких как градиентный спуск, покоординатный спуск и "подкрутки" каждого отдельного угла.


% https://github.com/Kobril/SVD-project/wiki/%D0%A3%D1%82%D0%BE%D1%87%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5-%D0%BF%D0%BE%D0%B2%D0%BE%D1%80%D0%BE%D1%82%D0%BE%D0%B2-%D0%93%D0%B8%D0%B2%D0%B5%D0%BD%D1%81%D0%B0-%D0%B2-implicit-zero%E2%80%90shift-QR-%D0%B8-%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0-QR_zero_proper.h
% Решение задачи оптимизации градиентным спуском не увенчалось большим успехом, так что было принято решение остановится на более простой идеей с небольшими изменениями значений синуса и косинуса в поворотах Гивенса.
\newpage
\section{"Обратный" поворот Якоби}

\subsection{Принцип сведения $A$ к $\Sigma$}
В отличие от первой идеи, метод Якоби предполагает работу с изначальной матрицей $A\in M_{m\times n}(\mathbb{C})$ при помощи поворотов Якоби (\ref{eq:2:1}), однако это то же самое, что повороты Гивенса. Есть два варианта работы алгоритма:\textit{ двусторонний метод Якоби, односторонний метод Якоби} ~\cite{Dongarra2018}.
\begin{equation} \label{eq:2:1}
    J(i,j,\theta) = 
    \begin{pmatrix}E&&&&\\
        &c&&s\\
        &&E&&\\
        &-s&&c\\
        &&&&E
    \end{pmatrix},\
    c = \cos(\theta), s =\sin(\theta) 
\end{equation}
\paragraph{Двусторонний метод Якоби}
Этот метод предполагает домножение изначальной матрицы $A$ размера $n\times n \ $ слева на $J(i,j,\theta)$, справа на $K(i,j,\phi)$ таким образом, чтобы занулить недиагональные элементы, пока $\left\| A - diag(A) \right\|_F > tol$, $tol$ - заданный допуск. 
Отсюда следует рекуррентная последовательность, позволяющая получить сингулярные значения $\Sigma$
\begin{center}
    $A_{(k+1)} = J^T_{(k)}A_{(k)}K_{(k)}, \ A_{(0)}= A, \ A_{(k)}\longrightarrow\Sigma \text{ при } k\to\infty$
\end{center}

Левые повороты при перемножении дают левый сингулярный вектор, $U=J_{(0)}J_{(1)}...$, аналогично, правые повороты - правый сингулярный вектор $V=K_{(0)}K_{(1)}...$

Чтобы определить матрицы $J(i,j,\theta), K(i,j,\phi)$ требуется рассмотреть уравнение
\begin{equation}
    \hat{J}_{(k)}^T\hat{A}_{(k)}\hat{K}_{(k)} = \begin{pmatrix}
        c_J&s_J\\
        -s_J&c_J
    \end{pmatrix}^T
    \begin{pmatrix}
        a_{ii}&a_{ij}\\
        a_{ji}&a_{jj}
    \end{pmatrix}
    \begin{pmatrix}
        c_K&s_K\\
        -s_K&c_K
    \end{pmatrix} = \begin{pmatrix}
        d_{ii} &\\
        &d_{jj}
    \end{pmatrix} = A_{(k+1)},
\end{equation}
где $d_{ii}, d_{jj} -\text{сингулярные числа } \hat{A}_{(k)}$

Однако углы для $J, K$ находятся неоднозначно.

% Не забыть добавить ссылку к учебнику! + расписать подробнее, потому что нужно учесть все детали разных способов
В случае матрицы размера $m \times n, \ m>n$ поступают несколькими способами.
Первый подход делает матрицу квадратный при помощи
\begin{center}
    $\bar{A} = [A,0] =(\bar{a}_{ij})\in M_{m\times m}(\mathbb{C})$.
\end{center}
%Теперь при решении поиска поворотов Якоби будем иметь
%\begin{equation}
   % \begin{bmatrix}
   %     c_J&s_J\\-s_J&c_J
   % \end{bmatrix}^T
  %  \begin{bmatrix}
  %      \bar{a}_{pp}&\bar{a}_{pq}\\\bar{a}_{qp}&\bar{a}_{qq}
 %   \end{bmatrix}
  %  \begin{bmatrix}
   %     c_K&s_K\\-s_K&c_K
 %   \end{bmatrix}
%\end{equation}
%\paragraph{Односторонний метод Якоби}

% Edit 7 (DONE) 
% Попробовать ужать часть с односторонним методом. Тут он, наверное, имеет только историческую ценность. И нужно объяснить, почему выбирается именно двусторонний метод. Чем он так лучше в нашем случае? + TODO описать критерии схождения этого этапа, вдохновляться первой задачей Миши
\paragraph{Односторонний метод Якоби}
Идея метода заключается в том, чтобы использовать матрицу поворота Якоби к изначальной матрице $A$ только с правой стороны - $AJ$ для ортогонализации столбцов $A$, что неявно является двусторонним методом Якоби для матрицы $A^TA$.
Столбцы матрицы сходятся к $U\Sigma$, где левые сингулярные вектора перемножаются с сингулярными значениями.
\begin{center}
     $A_{(0)}=A,\ A_{(k+1)} = A_{(k)}J_{(k)},\ A_{(k)} \longrightarrow U\Sigma, \text{ при } k\to\infty$
 \end{center}
 Отсюда неявно следует, что $A^T_{(k)}A_{(k)} \to \Sigma^2$. Аналогично двустороннему методу Якоби $V = J_{(0)}J_{(1)}...$, получаем правый сингулярный вектор. Нахождение правого сингулярного вектора 
 \begin{center}
     $A_{(\infty)} = U\Sigma \Rightarrow U = A_{(\infty)}\Sigma^{-1}$
 \end{center}
 Заметим, что при малых сингулярных значений $\sigma_i<<1$ выражение $u_{ii} = \frac{a_{ii}}{\sigma_{ii}}$ может быть подсчитано с значительной потери точности, поэтому для исследования методов Якоби будет рассматриваться двусторонний метод.
% Для определения матрицы поворота требуется рассмотреть уравнение.
% \begin{equation}
%     \hat{J}^T_{(k)}\begin{pmatrix}
%         b_{ii}&b_{ij}\\
%         b_{ji}&b_{jj}
%     \end{pmatrix}\hat{J}_{(k)} = \begin{pmatrix}
%         d_{ii} &\\
%         & d_{jj}
%     \end{pmatrix},
% \end{equation}
% где $b_{ij} = a_i^Ta_j, a_i-\text{i-ый столбец }A_{(k)},\ d_{ii}, d_{jj} -\text{собственные числа } A^TA$.

\paragraph{Критерий сходимости}
Для двустороннего метода Якоби доказано ~\cite{Forsythe1960}, что при циклическом порядке (строк или столбцов) cходимость гарантируется, если можно выбрать углы поворота так, чтобы они оставались ограниченными значением, строго меньшим $\frac{\pi}{2}$. Однако этот предел может иногда не достигаться точно, что теоретически может приводить к циклическому поведению или замедленной сходимости. На практике же применяются различные модификации, например, использование пороговых значений для пропуска незначительных поворотов, что ускоряет процесс и обеспечивает сходимость. Несмотря на то, что строгих доказательств для некоторых параллельных или более сложных порядков обработки может не существовать, обширный численный опыт показывает, что двусторонний метод Якоби надёжно сходится ~\cite{Dongarra2018}.

\subsection{Обратный ход}
% Edit 8 (DONE) - опустил момент, что на входе либо A, \Sigma, либо A,\Sigma, U,V.
% Тут встает важный вопрос -- мы опять пытаемся "с нуля" находить вектора, или все же как то используем найденные до этого повороты? не забыть спросить Парфа. В этом и заключается главный вопрос. Исходный метод Якоби итеративно уменьшает норму внедиагональных элементов матрицы; возможно, обратные повороты стоит выбирать так, чтобы каждый поворот приближал текущую матрицу к исходной A по норме разницы некоторых элементов? В репе в ветке со второй идеей уже перенесен код, надо будет глянуть.
\paragraph{Описание алгоритма}
Рассмотрим структуру
\begin{center}
    \begin{bmatrix}
        1&&&&\\
        &.&&&\\
        &&.&&\\
        &&&.&\\
        &&&&1
    \end{bmatrix} 
    \begin{bmatrix}
        $\sigma_1$&&\\
        &.&\\
        &&$\sigma_n$\\
        \\\\
    \end{bmatrix}
    \begin{bmatrix}
        1&&\\
        &.&\\
        &&1
    \end{bmatrix}=
     \begin{bmatrix}
        $\sigma_1$&&\\
        &.&\\
        &&$\sigma_n$\\
        \\\\
    \end{bmatrix}, $\sigma_i$ - сингулярные значения.
\end{center}
Или в компактной записи: $\Sigma = I_{m\times m}\Sigma_{m\times n}I_{n\times n}$.
Идея обратного метода заключается в том, что имея матрицу сингулярных значений $\Sigma$ для матрицы $A\in M_{m\times n}(\mathbb{C})$, применять повороты Якоби справа и слева таким образом, чтобы результатом преобразования стала исходная матрица $A$. При этом перемноженные правые повороты - это правый сингулярный вектор, а перемноженные левые повороты - левый сингулярный вектор. Отсюда следует итеративный алгоритм
\begin{equation}
    C_{(i)}=...B_4B_2C_{(0)}B_1B_3...;\ C_{(0)} = \Sigma
\end{equation}

Каждый шаг алгоритма направлен на уменьшение $\left\|  A-C_{(i)}\right\|_F$ следующим образом: формируем матрицу 
\begin{center}
    $A-C_{(i)} = N$
\end{center}
Находим максимальный элемент $\displaystyle\max_{i,j}\left\{ \left| N_{ij} \right| \right\} = g$. Используя значения $i,j,g$, применяем функцию $rot$ для получения $cos(\theta),sin(\theta)$ с последующим формированием матрицы поворота Якоби $B_{(i)}$. Итерации продолжаются до тех пор, пока $\left\|  A-C_{(i)}\right\|_F >tol$, $tol$ - заданный допуск. Правый сингулярный вектор $U = B_{(2n)}...B_{(4)}B_{(2)}$, левый сингулярный вектор $V^T = B_{(1)}B_{(3)}...B_{(2n-1)}$.

\begin{note}
    Размерности матриц $A, C_{(i)}$ совпадают, поэтому выражение для $N$ корректно.
\end{note}
\begin{note}
    На каждом шаге результат преобразования сохраняет ортогональность строк и столбцов матрицы, благодаря свойствам ортогональных поворотов. Это гарантирует, что итоговые матрицы $U$ и $V$ также будут ортогональными (или унитарными в комплексном случае).
\end{note}
%Ортогональные повороты \( J_i \) применяются слева к текущей матрице, изменяя её строки, а \( K_i \) применяются справа, изменяя её столбцы. Каждый шаг направлен на приближение текущей матрицы к исходной матрице \( A \).





\newpage
\section{"Наивный" метод}

% https://github.com/Quaret/SVD-project/wiki/%D0%98%D1%81%D0%BF%D0%BE%D0%BB%D1%8C%D0%B7%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5-MR%5E3-%D0%B4%D0%BB%D1%8F-%D1%80%D0%B5%D1%88%D0%B5%D0%BD%D0%B8%D1%8F-BSVD
% https://github.com/MathPerv/SVD-project/wiki/%D0%9D%D0%B0%D0%BF%D1%80%D0%B0%D0%B2%D0%BB%D0%B5%D0%BD%D0%B8%D1%8F-%D0%B8%D1%81%D1%81%D0%BB%D0%B5%D0%B4%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D1%8F-SVD#%D0%B8%D0%B4%D0%B5%D1%8F-3-%D0%BD%D0%B0%D0%B8%D0%B2%D0%BD%D1%8B%D0%B9-%D0%BC%D0%B5%D1%82%D0%BE%D0%B4
% Олежа, прости, эту часть особо не правил, она все еще в том безобразном состоянии, в котором я защищал нирку... Но я верю в тебя!
% Олежа: Нормалин, работаем! :)

% {Олежа:} - это комментарии Олежи

% Edit 9 (TODO)
% Олежа: а мы работаем только при действительно значных матрицах? То есть никакого там эрмитова сопряжения...
% Петя: всегда с эрмитовым. Все определения/формулы/теоремы пытаемся давать/составлять/доказывать для эрмитова случая.

Сведение задачи SVD к нахождению собственных разложений \( A A^T \) или \( A^T A \) является численно неустойчивым. Это становится понятным при рассмотрении числа обусловленности, определённого через максимальные и минимальные сингулярные числа:
\[\mu(A) = \frac{\sigma_{max}}{\sigma_{min}}.\]
Заметим, что:
\[ A^TA=(V \Sigma U^T)(U \Sigma V^T)=V (\Sigma)^2V^T. \]

\begin{note}
    Напомним, что возведение диагональной матрицы в степень эквивалентно возведению всех элементов диагонали в соответствующий показатель. Также напомним, что матрицы \(U\) и \(V\) являются ортогональными, то есть \(U^TU=1\) и \(V^TV=1\).
\end{note}

Аналогичное разложение будет иметь и \(AA^T\). Из этого видно:
\[ \mu(A^TA) = \left( \frac{\sigma_{max}}{\sigma_{min}} \right)^2 = (\mu(A))^2.\]
Заключим, что при умножении матрицы на саму себя число обусловленности возводится в квадрат, что приводит к значительной потере точности.

Существует более устойчивая альтернатива, которая позволяет избежать прямого умножения матрицы на себя ~\cite{mr3_algo4triagonal_sym_eigen_and_bidiagSVD}. Рассмотрим составление новой матрицы следующего вида:

\begin{equation}
M = \begin{bmatrix} 0 & A^T \\ A & 0 \end{bmatrix}.
\end{equation}

Эту матрицу далее будем называть циклической. В случае бидиагональной матрицы \( B \in \R{n}{n} \) задача принимает еще более удобную форму:

\begin{equation}
M_B = \begin{bmatrix} 0 & B \\ B^T & 0 \end{bmatrix}.
\end{equation}

Соотношение между сингулярными числами/векторами и собственными числами/векторами для циклических матриц подробно рассмотрено в разделе 8.6.1 ~\cite{Golub2013}. 

% Edit 10 (DONE)
% Олежа: Стоит ли привести эти соотношения?
% Петя: Ждем что скажет Парф.
\subsection{Приведение к форме Голуба-Кахана}

% Edit 11 (TODO)
% Олежа: тут должна быть ссылка на немецкую диссертацию по MR^3, но без списка литературы я не уверен, что это стоит отдельно прописывать
% Петя: очень даже стоит :) и в библиографию тож надо закинуть, она теперь тоже есть в файле references.bib. Инструкция по добавлению есть в чате

Даны ранее упомянутая циклическая матрица \(M_B\) и бидиагональная матрица \(B \in \R{n}{n}\) следующего вида:

\begin{equation}
    B= diag(\alpha_1,\dots,\alpha_n)+diag_{+1}(\beta_1,\dots,\beta_{n-1}).
\end{equation}

\begin{note}
    под \(diag_{+1}\) понимается нулевая матрица с первой ненулевой наддиагональю.
\end{note}

\begin{note}
    Введём также обозначение сингулярного разложения бидиагональной матрицы \(B\):
    \[
    B=U\Sigma V^T,
    \]
    где \(U,\Sigma,V \in \R{n}{n}\) формируют соответствующие сингулярные триплеты.
\end{note}
Сперва представим циклическую матрицу в \textbf{форме Джордана-Виландта}:

\begin{equation}
    M_B= \begin{bmatrix}
        0 & B \\
        B^T & 0
    \end{bmatrix} = J
    \begin{bmatrix}
        -\Sigma & 0 \\
        0 & \Sigma
    \end{bmatrix} J^T,
\end{equation}

где \(J= \frac{1}{\sqrt{2}}\begin{bmatrix}
    U & U \\
    -V & V
\end{bmatrix}\).

Удостоверимся, что данная форма корректна:

% Edit 12 (BUSY) (Этим занимается Коля - убрать это все и сделать по нормальному. Верю в него, поглядим на результат.)
% \split - это окружение, которое позволяет делать переносы. Тут это необходимо, ибо иначе матрицы уходят за пределы страницы
\begin{equation*}
    \begin{split}
        J \begin{bmatrix} -\Sigma & 0 \\ 0 & \Sigma \end{bmatrix} J^T 
        &= \left( \frac{1}{\sqrt{2}}\right)^2 
            \begin{bmatrix} U & U \\ -V & V \end{bmatrix}
            \begin{bmatrix} -\Sigma & 0 \\ 0 & \Sigma \end{bmatrix}
            \begin{bmatrix} U^T & -V^T \\ U^T & V^T \end{bmatrix}= \\[6pt]
        &= \frac{1}{2} 
            \begin{bmatrix} -U\Sigma & U\Sigma \\ V\Sigma & V\Sigma \end{bmatrix}
            \begin{bmatrix} U^T & -V^T \\ U^T & V^T \end{bmatrix}=\\[6pt] 
        &= \frac{1}{2} 
            \begin{bmatrix} 0 & 2 U\Sigma V^T \\ 2 V\Sigma U^T & 0 \end{bmatrix} = \begin{bmatrix} 0 & B \\ B^T & 0 \end{bmatrix}.
    \end{split}
\end{equation*}

\begin{claim}
    Матрица \(J\) является ортогональной.
\end{claim}
\begin{proof}
    \begin{equation*}
        \begin{split}
            J^T J= \frac{1}{2} \begin{bmatrix} U^T &-V^T\\ U^T & V^T \end{bmatrix} \begin{bmatrix} U & U \\ -V & V \end{bmatrix} = \frac{1}{2} \begin{bmatrix}
        U^TU+V^TV & U^TU-V^TV \\ U^TU-V^TV & U^TU+V^TV
    \end{bmatrix} =\\[6pt] = \begin{bmatrix}
        I_n & 0 \\ 0 & I_n
    \end{bmatrix} = I_{2n} \in \R{2n}{2n}.
        \end{split}
    \end{equation*}
\end{proof}

Затем, используя бидиагональность \( B \), можно применить матрицу перестановки \( P \) к \( M_B \), чтобы привести её к \textbf{форме Голуба-Кахана} \( T_{GK} \):

\begin{equation}
T_{GK} = P \begin{bmatrix} 0 & B \\ B^T & 0 \end{bmatrix} P^T = diag_{\pm 1}(\alpha_1,\beta_1,\alpha_2,\beta_2,\dots,\alpha_{n-1},\beta_{n-1},\alpha_{n}).
\end{equation}

где \( \alpha_i \) и \( \beta_i \) — элементы исходной бидиагональной матрицы \( B \) на главной и наддиагонали соответственно. 

Опишем принцип действия матрицы перестановки. Пусть дан вектор \(x \in \mathbb{R}^{2n} \). Тогда действие матрицы перестановки производится так

\begin{equation}
    Px=(x_{n+1},x_1, x_{n+2},x_2,\dots,x_{2n},x_n)^T.
\end{equation}
\begin{note}
    Определим также действие транспонированной матрицы перестановки.
    \begin{equation}
        P^Tx=(x_2,x_4,\dots,x_{2n},x_1,x_3,\dots,x_{2n-1})^T.
    \end{equation}
\end{note}

\begin{claim}
    Матрица перестановки \(P\) является ортогональной.
\end{claim}
\begin{proof}
    Для подтверждения последнего замечания рассмотрим выражение \(P^T(Px)\). Сперва вспомним: 
    \begin{equation}
        Px=(x_{n+1},x_1, x_{n+2},x_2,\dots,x_{2n},x_n)^T.
    \end{equation}
    Заметим, что вторая половина вектора \(x\) \([x_{n+1},x_{n+2},\dots,x_{2n}]\) находится на нечётных позициях, а первая половина вектора  \(x\) \([x_{1},x_{2},\dots,x_{n}]\) - на чётных. 

    В сущности, если рассмотреть действие транспонированной матрицы и переназначить индексы, то можно увидеть, что:
    \begin{equation*}
        \begin{split}
            P^T(Px)=P^T(x_{n+1},x_1, x_{n+2},x_2,\dots,x_{2n},x_n)^T=P^T(x_{1}^\prime,x_2^\prime,\dots,x_{2n-1}^\prime,x_{2n}^\prime)^T=\\[6pt]=(x_2^\prime,x_4^\prime,\dots,x_{2n}^\prime,x_1^\prime,x_3^\prime,\dots,x_{2n-1}^\prime)^T=(x_{1},x_2,\dots,x_{2n-1},x_{2n})^T=I_{2n}x. 
        \end{split}
    \end{equation*}
    То есть \(P^TP=I_{2n}\).
\end{proof}

\begin{note}
    Теоретически матрицу \(P\) можно представить через набор единиц и нулей, расположенных специальным образом в соответствии с умножением строки матрицы на столбец. Однако на практике действие матрицы перестановок легче реализовать в виде непосредственной работы с индексами матрицы или вектора, на которые действует матрица перестановок.   
\end{note}

\begin{example}[Форма Голуба-Кахана для матрицы \(2\times2\)]
    В качестве иллюстративного примера реализации формы Голуба-Кахана рассмотрим эту форму на основе матрицы \(S\in \R{2}{2}\):

\[
    S=\begin{bmatrix} a_1 & b_1 \\ 0 & a_2 \end{bmatrix}.
\]

Составим циклическую матрицу:
\[
    M_S=\begin{bmatrix} 0 & 0& a_1 & b_1 \\ 0 & 0 & 0 & a_2 \\ a_1 & 0 & 0 & 0 \\ b_1 & a_2 & 0 & 0  \end{bmatrix}.
\]

Далее будем интерпретировать столбцы матрицы в качестве векторов, на которые будут действовать матрицы перестановки.

\begin{note}
    В этом случае матрица перестановки будет переводить набор индексов \((1,2,3,4)\) в набор \((3,1,4,2)\), а транспонированная матрица перестановки - в набор \((2,4,1,3)\).
\end{note}

Форма Голуба-Кахана для этого примера выходит следующей:
\begin{equation*}
    \begin{split}
        T_{GK}(S)=P\begin{bmatrix} 0 & 0& a_1 & b_1 \\ 0 & 0 & 0 & a_2 \\ a_1 & 0 & 0 & 0 \\ b_1 & a_2 & 0 & 0  \end{bmatrix}P^T
        =
        \begin{bmatrix} a_1 & 0& 0 & 0 \\ 0 & 0 & a_1 & b_1 \\ b_1 & a_2 & 0 & 0 \\ 0 & 0 & 0 & a_2  \end{bmatrix}P^T=\\[6pt]=\left(P\begin{bmatrix} a_1 & 0& b_1 & 0 \\ 0 & 0 & a_2 & 0 \\ 0 & a_1 & 0 & 0 \\ 0 & b_1 & 0 & a_2  \end{bmatrix}\right)^T= \begin{bmatrix} 0 & a_1 & 0 & 0 \\ a_1 & 0 & b_1 & 0 \\ 0 & b_1 & 0 & a_2 \\ 0 & 0 & a_2 & 0  \end{bmatrix}.
        \end{split} 
    \end{equation*}
\end{example}

Рассмотрим  форму Голуба-Кахана \(T_{GK}\):

\begin{equation}
T_{GK} (B) = P \begin{bmatrix} 0 & B \\ B^T & 0 \end{bmatrix} P^T .
\end{equation}

Разложим циклическую матрицу по форме Джордана-Виландта:
\begin{equation}
T_{GK} = P J\begin{bmatrix} -\Sigma & 0 \\ 0 & \Sigma \end{bmatrix}J^T P^T = (PJ) \begin{bmatrix} -\Sigma & 0 \\ 0 & \Sigma \end{bmatrix} (PJ)^T=Q\Lambda Q^T.
\end{equation}
Последнее означает, что "союз" форм Голуба-Кахана и Джордана-Виландта - это, в сущности, спектральное разложение формы Голуба-Кахана.

% Edit 13 (TODO)
% Олежа:  стоит ли здесь уточнить комплексный случай? Да, я именно про унитарные матрицы.
% Петя: Стоит-стоит.
\begin{note}
    Пусть дана квадратная симметричная матрица \(A \in \R{2n}{2n}\). 
    Тогда спектральное разложение может быть представимо в следующем виде:
    \[
     A=Q\Lambda Q^T,
    \]
    где \(\Lambda\) - диагональная матрица с собственными значениями \(\lambda_i\), \(Q\) - ортогональная матрица, столбцы которой являются собственными векторами \(q_{(:,j)}=q_j\). 
\end{note}

\begin{note}
    Матрица \(PJ\) является ортогональной, поскольку матрицы \(P\) и \(J\) - ортогональные. То есть 
    \[(PJ)^TPJ=J^TP^TPJ=I_{2n}.\]
\end{note}

В итоге задача сингулярного разложения свелась к задаче спектрального разложения. 

\begin{note}
    В полученной задаче о поиске спектрального разложения стоит уточнить обозначения:
    \[
    \Lambda = \begin{bmatrix} -\Sigma & 0 \\ 0 & \Sigma \end{bmatrix}, \ Q= PJ,
    \]
    где \(J= \frac{1}{\sqrt{2}}\begin{bmatrix}
    U & U \\
    -V & V
\end{bmatrix}\).
\end{note}

Отдельно рассмотрим вид собственных векторов полученной задачи, для краткости опустив возможный знак минус у \(v_i\):
\begin{equation} \label{eq:eigenvector}
    \begin{split}
        Q=PJ \Rightarrow q_{i}=\frac{1}{\sqrt{2}}P(u_1,\dots,u_n,v_1,\dots,v_n)^T \Leftrightarrow \\[6pt]  \Leftrightarrow \sqrt{2}q_i=(v_1,u_1,v_2,u_2,\dots,v_n,u_n)^T, 
    \end{split}
\end{equation}
где \(q_i\) - собственный вектор.

Таким образом, \( T_{GK} \) представляет собой симметричную тридиагональную матрицу с нулями на главной диагонали. Это особенно удобно, поскольку тридиагональные системы уравнений обладают хорошо изученными и численно устойчивыми методами решения (например, методом прогонки).



\subsection{Решение задачи на основе \( T_{GK} \)}

Предполагая, что сингулярные значения \( B \) известны, можно найти собственные векторы \( T_{GK} \), решая систему уравнений:
% Edit 14 (BUSY) (спросим у Парфа/Дроздова)
% Олежа: Xм, кажется мы тут решаем обычное уравнение на поиск собственных векторов и значений, а-ля: Ax=lx => (A-l*I)x=0. И как будто тут пропустили x в основном тексте, а ведь x - это и есть собственные векторы. Я всё-таки исправил это уравнение. 

\begin{equation}
( T_{GK} - \sigma_i I )x = 0,
\end{equation}

где \( \sigma_i \) — сингулярные значения \( B \), \( I \) — единичная матрица, а \(x\) - искомый собственный вектор.

Восстановленные собственные векторы \( T_{GK} \) напрямую связаны с сингулярными векторами исходной бидиагональной матрицы \( B \) (см. выражение \eqref{eq:eigenvector}).

Тем не менее, симметричная тридиагональная структура \( T_{GK} \) делает задачу проще и численно стабильнее по сравнению с традиционными методами.

% Edit 15 (BUSY) (спросим у Парфа/Дроздова)
%Олежа: Отдельно возникает проблема потери ортогональности у собственных векторов. Она решается путём ортогональности полученного набора векторов - но это утверждение я пока что не могу доказать
