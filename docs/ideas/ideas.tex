% =================================================================================================
% =================================================================================================
% === IDEA 1 ======================================================================================
% =================================================================================================
% =================================================================================================


\section{Уточнение поворотов Гивенса в implicit zero-shift QR}

\subsection{Описание алгоритма}

\subsubsection{Приведение к бидиагональной матрице}

На первом этапе изначальная матрица $A$ приводится к бидиагональной матрице $B$. Производя это приведение мы уменьшаем количество ненулевых элементов в матрице и сохраняем заложенную в нее информацию.

% Edit 1 (BUSY, указал ссылку просто на информацию про повороты и преобразование хаусхолдера, другие фразы, которые точно описывают необходимые тезисы найти не могу)
% поискать цитаты
% Теорема Экарта-Янга: Бидиагонализация — этап в SVD, а Хаусхолдер/Гивенс оптимальны для унитарных преобразований.
% Блочные алгоритмы (Demmel et al.) Источник: Demmel, J., et al. "Communication-optimal parallel and sequential QR and LU factorizations" (2008). Оптимизация для кэш-памяти: комбинация Хаусхолдера с блочной обработкой. Ключевой вывод: "Blocked Householder remains the method of choice for dense matrices."
% Рандомизированные методы (Halko et al.) Источник: Halko, N., et al. "Finding structure with randomness" (2011). Для огромных матриц используют рандомизированную бидиагонализацию, но с опорой на те же базовые преобразования.
% Про то, что Гивенс и Хаусхолдер действительно самые крутые. Проверить: Golub & Van Loan, "Matrix Computations" (2013, 4th ed.) Глава 5.4: "Householder Bidiagonalization" Описан стандартный алгоритм приведения матрицы к бидиагональной форме с помощью преобразований Хаусхолдера. Глава 5.1.6: Использование поворотов Гивенса для точечного обнуления элементов. Цитата:  "The most stable and efficient way to compute the bidiagonal form is via Householder transformations." 
Обычно этот этап выполняется за счет алгоритмов, основанных на преобразованиях Хаусхолдера или поворотах Гивенса (подробное описание методов можно найти в главе Orthogonalization and Least Squares~\cite{Golub2013}). В отдельных случаях могут применяться методы Ланцоша и рандомизированные алгоритмы.

\subsubsection{Сведение бидиагональной матрицы к сингулярной}

Можно привести $B$ к диагональному виду с помощью последовательных поворотов Гивенса. Этот алгоритм описывается рекуррентным соотношением~\cite{Demmel1990}:

\begin{equation}
B_{(i+1)} = \left( \prod_{p=1}^{n-1} J_{2p} \right) B_{(i)} \left( \prod_{p=1}^{n-1} J_{2p-1} \right) = J_{2n-2} \cdots J_6 J_4 J_2 B_{(i)} J_1 J_3 J_5 \cdots J_{2n-3},
\end{equation}

% Edit 21 (DONE)
%  Между ф-лами (1.1) и (1.2) у B нижний индекс всё время меняется, то l, то t, что сбивает с толку. Пусть будет он тем же самым, что в форму выше, например, B_i.
где \( J_k \) - повороты Гивенса, которые зануляют внедиагональные элементы матрицы \( B \), приводя её к диагональной \( \Sigma \), при этом:
\begin{itemize}
    \item Левая часть: индексы $k$ принимают чётные значения
    \begin{equation}
    k = 2p \quad \text{для} \quad p = 1,2,\ldots,n-1,
    \end{equation}
    что даёт последовательность $k = 2,4,6,\ldots,2(n-1)$.

    \item Правая часть: индексы $k$ принимают нечётные значения
    \begin{equation}
    k = 2p-1 \quad \text{для} \quad p = 1,2,\ldots,n-1,
    \end{equation}
    что даёт последовательность $k = 1,3,5,\ldots,2(n-1)-1$.
\end{itemize}

% Edit 2 (DONE)
% Указать параграф/страницу прямо перед цитатой, в тексте. По таким же правилам добавить второй источник Accurate Singular Values of Bidiagonal Matrices. Что то типа "что описано на странице../в параграфе.. [1] или более подробно на странице../в параграфе.. [2]"
% Нашел в lawn03 ссылку на книгу, разбираюсь в ней. Как раз та, что написана ниже. В целом думаю, что Accurate Singular Values of Bidiagonal Matrices также можно указать как 2 источник) Пруф того, что ошибка незначительна B. Parlett, The Symmetric Eigenvalue Problem, Prentice Hall, Englewood Cliffs, New Jersey
При этом ошибка, увеличивающаяся с каждым поворотом, остается незначительной и не превышает $p(n) \epsilon ||B_i||$ (более подробно в параграфе The Accuracy of the Computed Singular Vectors~\cite{Demmel1990}), где  \( \epsilon \) — машинная точность (\( \epsilon \approx 10^{-16} \) для \texttt{double}, что описано на страницах 94-45~\cite{Golub2013}), \( p(n) \) — умеренно растущая функция от размера матрицы \( n \) (например, \( n \), \( n^2 \), но не \( e^n \)). \( \|B_i\| \) — норма матрицы \( B_i \).

% Edit 3 (PASS)
% найти пруф, добавить цитату. Скорее всего это также в lawn (показалось очевидным)
% Edit 22 (TODO)
% "Через несколько итераций матрица B^(i) приближается к диагональной \hat{\Sigma}, содержащей сингулярные значения..."
% а) i-ое приближение обозначается у Вас то нижним, то верхнем индексом. Наведите полный порядок. (mini done)
% б) Очень желательно это показать выкладками. Математика - наука принципиально доказательная.
Нужно произвести \( n-1 \) поворотов с каждой стороны для бидиагональной матрицы \( B \). Через несколько итераций матрица \( B_{(i)} \) приближается к диагональной \( \widehat{\Sigma} \), содержащей сингулярные значения:
\begin{equation}
B = \left( \prod J_L \right) \widehat{\Sigma} \left( \prod J_R \right),
\end{equation}

% Edit 4 (DONE) 
% 3. "На каждом шаге результат преобразования сохраняет ортогональность строк и столбцов матрицы, благодаря свойствам ортогональных поворотов." Это откуда следует? Нужно написать.
где \( \prod J_L \) и \( \prod J_R \) — накопленные произведения левых и правых поворотов Гивенса, из которых при их перемножении получаются сингулярные векторы \( U \) и \( V \). Так как каждая матрица поворота Гивенса $J_i$ является ортогональной, то произведение таких матриц также будет ортогональной матрицей (\ref{ort}). $\widehat{\Sigma}$ — матрица близких к истинным сингулярных значений.
\begin{note}
    Пусть $Q_1, Q_2$ -- ортогональные матрицы, тогда: 
    \begin{equation} \label{ort}
        (Q_1Q_2)(Q_1Q_2)^T=Q_1Q_2Q_2^TQ_1^T=Q_1EQ_1^T=Q_1Q_1^T=E
    \end{equation}
\end{note}



В данном случае повороты Гивенса применяются итеративно, до того момента как алгоритм не сойдется. Далее описаны теоретические и применяемые на практике критерии сходимости.

% Edit 5 (DONE????) (код был написан в итоге под теорию из LAPACK)
% добавить про реализацию в коде, как там это сейчас работает, а потом подумаем про компановку
% Edit 6 (PASS)
% Также исправить все с активаного залога (рассмотрим) на пассивный (рассматриваются). Поменьше субъективного взгляда - нужно приводить факты (не наш критерий, а просто критерий и тп) 
% Edit 24 (TODO)
% б) Написать про роль этого алгоритма в наших вычисления - для чего он нужен.
% в) Не ясно, кем предложены, где применяются  (ссылки на литературу).
% г) Не понятно, откуда все эти оценки взялись.
\subsubsubsection{Cходимость вспомогательного алгоритма}

В разделе рассматриваются критерии сходимости для предложенного алгоритма implicit zero-shift QR приведения бидиагольной матрицы $B$ к диагональной $\Sigma$.

Пусть $s_1, s_2,...,s_n$ \--- диагональные элементы, а $e_1, e_2, ..., e_{n-1}$ \--- элементы на побочной диагонали матрицы $B$. 


% Edit 23 (BUSY)
% (так в этих равенствах и используют арифметику с плавающей точкой, мол, если s_i настолько мал, что его сумма с соседними числами ничего не добавляет в арифметике с плавающей точкой, то мы обнуляем)
% В настоящих вариантах (1.3) и (1.4) не может быть точных равенств - для плавающей точки они крайне маловероятны, поэтому не практичны.
Критерий сходимости должен гарантировать, что обнуление $e_i$ сильно не повлияет на точность сингулярных значений. К примеру, код в LINPACK имеет два разных порога для обнуления элемента:
\begin{align}
\text{Если } (|e_i| + |e_{i-1}| + |s_i| = |e_i| + |e_{i-1}|), \text{ то обнуляем }s_i \label{eq:1:1:1}
\\\text{Если } (|s_i| + |s_{i-1}| + |e_{i-1}| = |s_i| + |s_{i-1}|), \text{ то обнуляем }e_{i-1} \label{eq:1:1:2}
\end{align}

Оба случая не подходят для рассматриваемого алгоритма. При случае (\ref{eq:1:1:1}) нулевые сингулярные значения будут получаться там, где их не было до этого. Второй вариант (\ref{eq:1:1:2})  также является неудовлетворительным. Это можно заметить на примере: возьмем $\eta$ настолько маленького значения, что в арифметике с плавающей точкой $1+\eta=1$. Тогда можно рассмотреть матрицу
\begin{center}
$B(x)=\begin{pmatrix}
    \eta^2&1&&\\
    &1&x\\
    &&1&1\\
    &&&\eta^2
\end{pmatrix}$
\end{center}

\noindentПри $x=\eta$ самое минимальное сингулярное значение $B(\eta)$ будет примерно равно $\eta^3$. Рассматриваемый критерий обнулит $\eta$, но $B(0)$ имеет минимальное сингулярное значение возле $\frac{\eta^2}{\sqrt{2}}$. Полученные оценки $\eta^3$ и $\frac{\eta^2}{\sqrt{2}}$ слишком разнятся, что приведет к большой относительной ошибке.

Пусть $\sigma_{min}$ обозначет нижнюю границу для наименьшего сингулярного значения, $tol$ \--- критерий допуска, который зависит от желаемой относительной точности сингулярных значений. Значение $tol$ должно быть меньше 1, но больше машинной точности $\epsilon$. Самый простой допустимый вариант будет установка $e_i$ равными 0, если значение меньше, чем $tol\cdot\sigma_{min}$. При таком методе, числа на побочной диагонали будут зануляться очень долго. Гораздо лучшие критерии можно получить следующими способами.

Пусть $\lambda_j$ и $\mu_j$ вычисляются с помощью данных реккурентных соотношений~\cite{Demmel1990}, которые дадут нам оценку на нижний порог сингулярного значения для соответствующего элемента на побочной диагонали:

\begin{minipage}{0.48\textwidth}
\begin{align*}
\mu_1& = |s_1| \\
\text{for }& j = 1 \text{ to } n-1 \text{ do} \\
&\mu_{j+1} = |s_{j+1}| \cdot \left( \frac{\mu_j}{\mu_j + |e_j|} \right)
\end{align*}
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
\begin{align*}
\lambda_n& = |s_n| \\
\text{for }& j = n-1 \text{ to } 1 \text{ step } -1 \text{ do} \\
&\lambda_j = |s_j| \cdot \left( \frac{\lambda_{j+1}}{\lambda_{j+1} + |e_j|} \right)
\end{align*}
\end{minipage}
\vspace{1em}

\noindent\textit{Критерий сходимости 1a (обнуление $e_j$)}. Если $|\frac{e_j}{\mu_j}|\leq tol$, то будет обнулен элемент $e_j$.\vspace{1em}
\\\textit{Критерий сходимости 1b (обнуление $e_j$)}. Если $|\frac{e_j}{\lambda_{j+1}}|\leq tol$, то будет обнулен элемент $e_j$.\vspace{1em}
%$\\\textit{Критерий сходимости 2a}. Если сингулярные векторы не требуеются и\linebreak $e^2_{n-1}\leq0.5\cdot tol\cdot [(\frac{\min\limits_{j<n}\mu_j}{\sqrt{n-1}})^2-|s_n|^2]$, то будет обнулен элемент $e_{n-1}$.\vspace{1em}
%\\\textit{Критерий сходимости 2b}. Если сингулярные векторы не требуеются и\linebreak $e^2_1\leq0.5\cdot tol\cdot [(\frac{\min\limits_{j>1}\lambda_j}{\sqrt{n-1}})^2-|s_1|^2]$, то будет обнулен элемент $e_1$.\vspace{1em}

% Edit 16 (TODO)
% Здесь уже не очень понятно, где какой критерий используется - нужно разделить критерии, которые направлены на ускорение работы алгоритма (за счет обнуления отдельных векторов) и те, что являются критериями сходимости самого алгоритма. Вынести это в 2 отдельных subsubsubsection: 1) сходимость вспомогательного алгоритма, 2) Оптимизации вычисления (над названием можно еще подумать)
Эти критерии требуют больше вычислительных затрат, чем критерии, реализованные в библиотеке LINPACK, зато помогают избежать ситуаций, когда обнуление значения приводит к недопустимой относительной ошибке.

В библиотеке LAPACK ~\cite{Anderson1999} реализованы различные критерии сходимости, зависящие от значения параметра \(tol\):
\begin{equation}
\text{tol} = \epsilon \cdot tolmul
\end{equation}
где \(\epsilon\) — машинная точность, а \(tolmul\) — параметр критерия сходимости, задаваемый вручную. Абсолютное значение параметра \(tolmul\) должно быть в пределах от 1 до \(\frac{1}{\epsilon}\), при этом предпочтительные значения находятся в интервале от 10 (для обеспечения быстрой сходимости) до \(\frac{1}{10 \cdot \epsilon}\) (для достижения высокой точности). Значение по умолчанию устанавливается как \(\max(10, \min(100, \epsilon^{-0.125}))\).

При значении \(tol \geq 0\) алгоритм находит сингулярные значения с относительно высокой точностью. Вводится порог зануления элемента:
\begin{equation}
\text{threshold} = \max(tol \cdot \sigma_{\text{min}}, \text{maxiter} \cdot (n \cdot (n \cdot \text{unfl})))
\end{equation}
где \(\sigma_{\text{min}}\) — нижняя граница минимального сингулярного значения, \(\text{maxiter}\) — максимальное количество итераций (по умолчанию 6), \(n\) — количество вычисляемых сингулярных значений, а \(\text{unfl}\) — минимальное нормализованное положительное число, которое может быть безопасно использовано в арифметике с плавающей точкой.

При \(tol < 0\) используется другой порог зануления, при котором алгоритм вычисляет сингулярные значения с фиксированной абсолютной точностью:
\begin{equation}
\text{threshold} = \max (|tol| \cdot \sigma_{\text{max}}, \text{maxiter} \cdot (n \cdot (n \cdot \text{unfl})))
\end{equation}
где \(\sigma_{\text{max}}\) — оценка максимального сингулярного значения.

При выполнении алгоритма, если значение \(|e_i| \le threshold\), то оно зануляется.

\subsubsection{Подстановка истинных сингулярных значений}

Вместо приближенной диагональной матрицы \( \widehat{\Sigma} \) подставим истинные сингулярные значения \( \Sigma \):

\begin{equation}
\tilde{B} = \left( \prod J_L \right) \Sigma \left( \prod J_R \right).
\end{equation}

Теперь, изменив левое выражение, мы получили матрицу $\tilde{B}$, отличную от $B$. Для решения задачи остается уточнить повороты, из которых получаются левые и правые сингулярные векторы, чтобы получить точное разложение.

\subsubsection{Уточнение поворотов Гивенса}

Для того, чтобы получить точные левые сингулярные векторы нужно решить следующую оптимизацинную задачу:
\( 
min\| \widehat{B} - B \| 
\)

Этого можно добиться путем изменения углов \( \theta \) поворотов Гивенса \( J_k \):

\begin{equation}
\tilde{B} = J(\theta_1) J(\theta_2) \cdots J(\theta_N) \Sigma J(\theta_{N+1}) J(\theta_{N+2}) \cdots J(\theta_{2N}).
\end{equation}

Для этого существует несколько идей, таких как градиентный спуск, покоординатный спуск и "подкрутки" каждого отдельного угла.
% Edit 17 (TODO)
% Описать алгоритм подкруток.


\subsection{Программная реализация}

TODO



% =================================================================================================
% =================================================================================================
% === IDEA 2 ======================================================================================
% =================================================================================================
% =================================================================================================



% https://github.com/Kobril/SVD-project/wiki/%D0%A3%D1%82%D0%BE%D1%87%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5-%D0%BF%D0%BE%D0%B2%D0%BE%D1%80%D0%BE%D1%82%D0%BE%D0%B2-%D0%93%D0%B8%D0%B2%D0%B5%D0%BD%D1%81%D0%B0-%D0%B2-implicit-zero%E2%80%90shift-QR-%D0%B8-%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0-QR_zero_proper.h
% Решение задачи оптимизации градиентным спуском не увенчалось большим успехом, так что было принято решение остановится на более простой идеей с небольшими изменениями значений синуса и косинуса в поворотах Гивенса.
\newpage
\section{"Обратный" поворот Якоби}

\subsection{Описание алгоритма}

\subsubsection{Принцип сведения $A$ к $\Sigma$}
% Edit 18 (DONE)
% Вынести определение поворота Якоби/Гивенса в Глоссарий
В отличие от первой идеи, метод Якоби предполагает работу с изначальной матрицей $A\in \mathbb{C}^{m\times n}$ при помощи поворотов Якоби (\ref{eq:2:1}), что является другим названием поворотов Гивенса. Есть два варианта работы алгоритма:\textit{ двусторонний метод Якоби, односторонний метод Якоби} ~\cite{Dongarra2018}.
\subsubsubsection{Двусторонний метод Якоби}
Этот метод предполагает домножение изначальной матрицы $A$ размера $n\times n \ $ слева на $J(i,j,\theta)$, справа на $K(i,j,\phi)$ таким образом, чтобы занулить недиагональные элементы, пока $\left\| A - diag(A) \right\|_F > tol$, $tol$ - заданный допуск. 
Отсюда следует рекуррентная последовательность, позволяющая получить сингулярные значения $\Sigma$:
\begin{center}
    $A_{(k+1)} = J^T_{(k)}A_{(k)}K_{(k)}, \ A_{(0)}= A, \ A_{(k)}\longrightarrow\Sigma \text{ при } k\to\infty$.
\end{center}

Левые повороты при перемножении дают левый сингулярный вектор, $U=J_{(0)}J_{(1)}...$, аналогично, правые повороты - правый сингулярный вектор $V=K_{(0)}K_{(1)}...$

Чтобы определить матрицы $J(i,j,\theta), K(i,j,\phi)$ требуется рассмотреть уравнение:
\begin{equation}
    \hat{J}_{(k)}^T\hat{A}_{(k)}\hat{K}_{(k)} = \begin{pmatrix}
        c_J&s_J\\
        -s_J&c_J
    \end{pmatrix}^T
    \begin{pmatrix}
        a_{ii}&a_{ij}\\
        a_{ji}&a_{jj}
    \end{pmatrix}
    \begin{pmatrix}
        c_K&s_K\\
        -s_K&c_K
    \end{pmatrix} = \begin{pmatrix}
        d_{ii} &\\
        &d_{jj}
    \end{pmatrix} = A_{(k+1)},
\end{equation}
где $d_{ii}, d_{jj} -\text{сингулярные числа } \hat{A}_{(k)}$.

Однако углы для $J, K$ находятся неоднозначно.

% BUSY - ищу материалы с развернутым содержанием, добавить еще в глоссарий.
В случае матрицы размера $m \times n, \ m>n$ используется предобработка с использованием QR-разложения ~\cite{10.5555/867597}: 
\begin{center}
    $A = Q_{m\times m}R_{m\times n} = Q\begin{bmatrix}
        R_1\\0
    \end{bmatrix} = \begin{bmatrix}
        Q_1&Q_2
    \end{bmatrix}
    \begin{bmatrix}
        R_1\\0
    \end{bmatrix},
    $
\end{center}
где $R_1 \in \mathbb{R}^{n\times n}, Q_1 \in \mathbb{R}^{m\times n}, Q_2\in \mathbb{R}^{m\times (m-n)}$ ~\cite{Golub2013}. После этого метод Якоби применяется для матрицы $R_1$:
\begin{center}
    $W^TR_1V=\Sigma = \mathrm{diag(\sigma_1,...,\sigma_n)}$.
\end{center}
Определяя 
\begin{center}
    $U = Q\begin{bmatrix}
        W&0\\0&I
    \end{bmatrix},$    
\end{center}
мы получаем
\begin{center}
     $U^TAV = \mathrm{diag}(\sigma_1,...,\sigma_n)$.
\end{center}


% Edit 7 (DONE) 
% Попробовать ужать часть с односторонним методом. Тут он, наверное, имеет только историческую ценность. И нужно объяснить, почему выбирается именно двусторонний метод. Чем он так лучше в нашем случае? + TODO описать критерии сходимости этого этапа, вдохновляться первой задачей Миши
\subsubsubsection{Односторонний метод Якоби}
Идея метода заключается в том, чтобы использовать матрицу поворота Якоби к изначальной матрице $A$ только с правой стороны - $AJ$ для ортогонализации столбцов $A$, что неявно является двусторонним методом Якоби для матрицы $A^TA$.
Столбцы матрицы сходятся к $U\Sigma$, где левые сингулярные векторы перемножаются с сингулярными значениями:
\begin{center}
     $A_{(0)}=A,\ A_{(k+1)} = A_{(k)}J_{(k)},\ A_{(k)} \longrightarrow U\Sigma, \text{ при } k\to\infty$.
 \end{center}
 Отсюда неявно следует, что $A^T_{(k)}A_{(k)} \to \Sigma^2$. Аналогично двустороннему методу Якоби $V = J_{(0)}J_{(1)}...$, получаем правый сингулярный вектор. Нахождение правого сингулярного вектора 
 \begin{center}
     $A_{(\infty)} = U\Sigma \Rightarrow U = A_{(\infty)}\Sigma^{-1}.$
 \end{center}

 % Edit 26 (TODO)
 % Все подобные утверждения необходимо подтвердить выкладками. (нужно доказательство)
 Заметим, что при малых сингулярных значений $\sigma_i<<1$ выражение $u_{ii} = \frac{a_{ii}}{\sigma_{ii}}$ может быть подсчитано с значительной потери точности, поэтому для исследования методов Якоби будет рассматриваться двусторонний метод.

\paragraph{Критерий сходимости}
% Edit 19 (DONE) (Перечитываю статью о сходимости алгоритмов)
% Добавить указание страницы/главы, где это доказывается для Двустороннего метода Якоби
% Edit 20 (DONE)  замеч.: циклическое поведение -> зацикливание (неточность при переводе)
% Сформулировать что такое циклический порядок/циклическое поведение строк и столбцов и вынести в Глоссарий
Форсайт и Хенрици доказали (стр. 6-7, 10-12) ~\cite{Forsythe1960}, что методы Якоби для спектрального и сингулярного разложения при обходе циклически по строкам или столбцам гарантированно сходятся, если можно выбрать углы поворота так, чтобы они оставались ограниченными значением, строго меньшим $\frac{\pi}{2}$, т.е. $\left|\theta\right| \le b <\frac{\pi}{2}$. При значении $b = \frac{\pi}{2}$ может возникнуть ситуация, когда $A_{(k+6)}=A_{(k)}$, т.е. алгоритм не сходится (стр. 16-17) ~\cite{Forsythe1960}, вызывая зацикливание. 

% Edit 27 (DONE)
% Да неужели сходимость обеспечивается игнорированием поворотов?! Везде понимаете, что пишете?
% (нужно сделать акцент на том, что пороговые значения позволяют оптимизировать вычисления. Обосновать, как пороговые значения позволяют нам сделать допущение о том, что поворот можно заменить на тривиальный)
Задача о сингулярном разложении может быть рассмотрена как поиск ортогонального базиса столбцов оригинальной матрицы, тогда при рассмотрении ортогонализации пары столбцов $(A^{(k-1)}_p, A^{(k-1)}_q)$ известно ~\cite{deRijk1989}:
\begin{itemize}
    \item Ортогонализация пары столбцов может быть неэффективна в том случае, если их скалярное произведение сравнительно меньше чем среднее скалярное произведение всех пар.
    \item Ортогонализация может нарушить ортогональность других пар столбцов, на задачу которую могло быть потрачено больше вычислительных ресурсов. 
\end{itemize}
Таким образом, игнорирование ортогонализации столбцов, чье скалярное произведение меньше порога, зависящего от каждого обхода,  оптимизирует вычисления, так как в каждом обходе остаются неортогональные столбцы. Это приводит к реализации метода Якоби с использование пороговых значений, для которого известно, что сходимость гарантирована и скорость сходимости ускоряется ~\cite{Dongarra2018}, ~\cite{Forsythe1960}. \subsection{Обратный ход}
% Edit 8 (BUSY) - изучаю статьи
% Добавить параграф с описанием того, что можно начинать с нуля, а можно и с как то посчитанных векторов
% Тут встает важный вопрос -- мы опять пытаемся "с нуля" находить векторы, или все же как то используем найденные до этого повороты? не забыть спросить Парфа. В этом и заключается главный вопрос. Исходный метод Якоби итеративно уменьшает норму внедиагональных элементов матрицы; возможно, обратные повороты стоит выбирать так, чтобы каждый поворот приближал текущую матрицу к исходной A по норме разницы некоторых элементов? В репе в ветке со второй идеей уже перенесен код, надо будет глянуть.

\subsubsection{Выбор начального приближения}
Идея обратного хода предполагает работу с матрицей сингулярных значений $\Sigma$, которая известна заранее, таким образом, чтобы поворотами Якоби справа и слева сводить ее к оригинальной матрице $A \in \mathbb{C}^{m\times n}$. Отсюда следует несколько идей работы алгоритма: 
\begin{itemize}
    \item Использовать единичные сингулярные векторы,
    \item Использовать подсчитанные сингулярные векторы (начальное приближение).
\end{itemize}
Первый вариант рассматривается потому, что единичная матрица является ортогональной с ортонормированными столбцами в ней. 
Идея использовать уже подсчитанные заранее сингулярные векторы позволит поворотами справа и слева уточнить их значения. 

\subsubsection{Случай с единичными сингулярными векторами}
Рассмотрим структуру

\begin{center}
    $\begin{bmatrix}
        1 & & & \\
         & \ddots & & \\
         & & \ddots & \\
         & & & 1
    \end{bmatrix}_{n \times n}
    \begin{bmatrix}
        \sigma_1 & & \\
         & \ddots & \\
         & & \sigma_n \\
         & & \\
         & & 
    \end{bmatrix}_{n \times m}
    \begin{bmatrix}
        1 & & \\
         & \ddots & \\
         & & 1
    \end{bmatrix}_{m \times m}
    =
    \begin{bmatrix}
        \sigma_1 & & \\
         & \ddots & \\
         & & \sigma_n \\
         & & \\
         & & 
    \end{bmatrix}_{m \times n}$
    
    \vspace{1em} 
    где $\sigma_i$ - сингулярные значения.
\end{center}

Или в компактной записи: $\Sigma = I_{m\times m}\Sigma_{m\times n}I_{n\times n}$.
Идея обратного метода заключается в том, что имея матрицу сингулярных значений $\Sigma$ для матрицы $A\in \mathbb{C}^{m\times n}$, применять повороты Якоби справа и слева таким образом, чтобы результатом преобразования стала исходная матрица $A$. При этом перемноженные правые повороты - это правый сингулярный вектор, а перемноженные левые повороты - левый сингулярный вектор. Отсюда следует итеративный алгоритм
\begin{equation}
    C_{(i)}=...B_4B_2C_{(0)}B_1B_3...;\ C_{(0)} = \Sigma
\end{equation}

Каждый шаг алгоритма направлен на уменьшение $\left\|  A-C_{(i)}\right\|_F$ следующим образом:
\begin{enumerate}
    \item Формирование матрицы $A-C_{(k-1)} = N$
    \item Нахождение максимального элемента $\displaystyle\max_{i,j}\left\{ \left| N_{ij} \right| \right\} = g$. 
% Edit 29 (DONE)
% Раскрыть этап с rot
    \item Применение функции $makeGivens$ из библиотеки \textbf{EIGEN} для матрицы
    \begin{center}
        $\begin{bmatrix}
            n_{ii} & n_{ij}\\
            n_{ji} & n_{jj}
        \end{bmatrix}$, где $n_{ij} = g$,
    \end{center}
    таким образом, чтобы занулить элемент $g$ и сформировать матрицу поворота Якоби $B_{(k)}$.
    \item Формирование матрицы $C_{(k)} = 
    \begin{cases}
    C_{(k - 1)}B_{(k)}, & \text{если } i \equiv 1 \ (mod 2),\\
    B_{(k)}C_{(k - 1)}, & \text{если } i \equiv 0 \ (mod\ 2).
    \end{cases}$
\end{enumerate}
Итерации продолжаются до тех пор, пока $\left\|  A-C_{(i)}\right\|_F >tol$, $tol$ - заданный допуск. Правый сингулярный вектор $U = B_{(2n)}...B_{(4)}B_{(2)}$, левый сингулярный вектор $V^T = B_{(1)}B_{(3)}...B_{(2n-1)}$.

\begin{note}
    Размерности матриц $A, C_{(k)}$ совпадают, поэтому выражение для $N$ корректно.
\end{note}

% Edit 25 (DONE)
% Посмотреть Edit 4, доказательство
\begin{note}
    На каждом шаге результат преобразования сохраняет ортогональность строк и столбцов матрицы, благодаря свойствам ортогональных поворотов \eqref{ort}. Это гарантирует, что итоговые матрицы $U$ и $V$ также будут ортогональными (или унитарными в комплексном случае). 
\end{note}
%Ортогональные повороты \( J_i \) применяются слева к текущей матрице, изменяя её строки, а \( K_i \) применяются справа, изменяя её столбцы. Каждый шаг направлен на приближение текущей матрицы к исходной матрице \( A \).

% Пока пусть будет. Надо спросить у Парфы
\subsubsection{Случай с частично посчитанными сингулярными векторами} %DONE
Идея работы алгоритма в данном случае никак не отличается от прошлого пункта, однако меняются начальные значения. Рассмотрим структуру 
\begin{center}
    $\tilde{A}_{m \times n} = \tilde{U}_{m\times m}\Sigma_{m\times n}\tilde{V}^T_{n \times n} = C_{(0)}$,
\end{center}
Аналогично, получаем итеративный алгоритм:
\begin{equation}
    C_{(k)}=...B_4B_2C_{(0)}B_1B_3...;\ C_{(0)} = \tilde{A}_{m \times n}
\end{equation}
Все шаги повторяются аналогичным образом подобно случаю с единичными сингулярными векторами.

\subsection{Программная реализация}

TODO



% =================================================================================================
% =================================================================================================
% === IDEA 3 ======================================================================================
% =================================================================================================
% =================================================================================================



\newpage
\section{''Наивный'' метод}

% https://github.com/Quaret/SVD-project/wiki/%D0%98%D1%81%D0%BF%D0%BE%D0%BB%D1%8C%D0%B7%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5-MR%5E3-%D0%B4%D0%BB%D1%8F-%D1%80%D0%B5%D1%88%D0%B5%D0%BD%D0%B8%D1%8F-BSVD
% https://github.com/MathPerv/SVD-project/wiki/%D0%9D%D0%B0%D0%BF%D1%80%D0%B0%D0%B2%D0%BB%D0%B5%D0%BD%D0%B8%D1%8F-%D0%B8%D1%81%D1%81%D0%BB%D0%B5%D0%B4%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D1%8F-SVD#%D0%B8%D0%B4%D0%B5%D1%8F-3-%D0%BD%D0%B0%D0%B8%D0%B2%D0%BD%D1%8B%D0%B9-%D0%BC%D0%B5%D1%82%D0%BE%D0%B4
% Олежа, прости, эту часть особо не правил, она все еще в том безобразном состоянии, в котором я защищал нирку... Но я верю в тебя!
% Олежа: Нормалин, работаем! :)

% {Олежа:} - это комментарии Олежи

% Edit 9 (TODO)
% Олежа: а мы работаем только при действительно значных матрицах? То есть никакого там эрмитова сопряжения...
% Петя: всегда с эрмитовым. Все определения/формулы/теоремы пытаемся давать/составлять/доказывать для эрмитова случая.
% Олежа: DONE! На самом деле это было просто, ибо эрмитово сопряжение и псевдообращение (по аналогии с обратной (обращением) матрицы) - это обобщения на комплексный случай матриц

\subsection{Описание алгоритма}
\textbf{На вход} принимается \textbf{бидиагональная матрица} \(B\) следующего вида:
\[
        B= diag(\alpha_1,\dots,\alpha_n)+diag_{+1}(\beta_1,\dots,\beta_{n-1}).
\]

\begin{note}
    Если на вход поступает не бидиагональная матрица \(A\), то она может быть сведена к необходимой форме путём известных преобразований.
\end{note}

\begin{note}
    Под \(diag_{+1}\) понимается ''нулевая'' матрица с первой ненулевой наддиагональю.
\end{note}

Также нам известны \textbf{сингулярные значения} из соответствующего разложения исходной матрицы \(B\):
\begin{equation}
        B=U \Sigma V^*,
\end{equation}
где $U,V$ - унитарные матрицы, состоящие из неизвестных нам правых и левых сингулярных векторов $u_{(i,:)},v_{(:,j)}$ соответственно, а $\Sigma$ - диагональная матрица с сингулярными значениями \(\sigma_j\).

\textbf{Наша задача} - вычислить \textbf{левые и правые сингулярные векторы} соответственно.

Существует достаточно понятный и часто используемый в человеческой практике приём - взять неизвестную задачу и свести её к известной, для которой имеются методы решения. В нашем случае предлагается свести задачу сингулярного разложения бидиагональной матрицы (в англоязычной литературе именуемой BSVD) к задаче спектрального разложения некоторой тридиагональной матрицы (в англоязычной литературе именуемой TSEP).

Есть два подхода, как можно свести одну задачу к другой - получить ''нормальные уравнения'' или использовать ''форму Голуба-Кахана''.

%Олежа: я установил, что нормальные уравнения естественным образом возникают из задачи наименьших квадратов - но я пока источника не нашёл


\subsubsection{Нормальные уравнения}
\subsubsubsection{Построение нормальных уравнений}
На вход поступает бидиагональная матрица $B$ с известными сингулярными значениями $\Sigma$ (или исходная матрица $A$, которую мы переводим в бидиагональную посредством известных преобразований).

% Edit 31 (TODO)
% "Этот шаг нужен затем, чтобы после приведения BSVD к TSEP (определе­ние этих аббревиатур было дано несколькими абзацами ранее)" Читатель предусматривается совсем склерозным? Это было строк 10 тому назад. 
% Нужно вынести в глоссарий
% Олежа: DONE! (Запись в скобках про уточнение аббревиатур следует удалить(сделал). В этом и была суть правки Парфа).
Наш первый шаг - свести задачу сингулярного разложения бидиагональной матрицы к задаче спектрального разложения тридиагональной матрицы. Этот шаг нужен затем, чтобы после приведения BSVD к TSEP воспользоваться уже известными решателями задач спектрального разложения (к примеру, $MR^3$).

Вычислим так называемые ''нормальные уравнения'':

\begin{equation} \label{Norm_eq}
    \begin{split}
        B^*B=V \Sigma (U^*U=I_n) \Sigma V^*=V \Sigma^2 V^*\\
        BB^*=U\Sigma (V^* V=I_n) \Sigma U^*=U \Sigma^2 U^*
    \end{split}
\end{equation}

Заметим, что мы получили задачу спектрального разложения тридиагональной матрицы, где $\Sigma^2$ - это собственные значения, а правые и левые сингулярные векторы выступают в роли собственных векторов, входящих в соответствующие матрицы (это становится естественным, если вспомнить, что матрицы \(V,U\) - унитарные). 

Распишем одно из нормальных уравнений более подробно, дабы убедиться, что будет получена именно тридиагональная матрица (для наглядности рассмотрим матрицы размером $3\times 3$ с пониманием, что этот пример легко распространяется на случай матриц с размером $n\times n$):
\begin{equation*}
    \begin{split}
        BB^*=\begin{bmatrix}
            \alpha_1 & \beta_1 & 0\\
            0 & \alpha_2 & \beta_2 \\
            0 & 0 & \alpha_3  
        \end{bmatrix}\begin{bmatrix}
            \overline{\alpha_1} & 0 & 0\\
            \overline{\beta_1} & \overline{\alpha_2} & 0 \\
            0 & \overline{\beta_2} & \overline{\alpha_3}  
        \end{bmatrix} =
        \begin{bmatrix}
            \alpha_1^2+\beta^2_1 & \beta_1 \overline{\alpha_2} & 0 \\
            \alpha_2 \overline{\beta_1} & \alpha_2^2+\beta_2^2 & \beta_2 \overline{\alpha_3} \\
            0 & \alpha_3 \overline{\beta_2} & \alpha_3^2
        \end{bmatrix} =\\[6pt]= diag(\alpha_1^2+\beta^2_1, \alpha_2^2+\beta^2_2,\alpha_3^2)+diag_{+1}(\beta_1 \overline{\alpha_2},\beta_2 \overline{\alpha_3})+diag_{-1}(\overline{\beta_1} \alpha_2,\overline{\beta_2} \alpha_3).
    \end{split}
\end{equation*}

В итоге мы получили две тридиагональные матрицы с известными нам собственными значениями и неизвестными нам собственными векторами, являющимися искомыми левыми и правыми сингулярными векторами исходной матрицы $B$.

\subsubsubsection{Решения задачи на основе нормальных уравнений}
Мы построили ''нормальные уравнения''. Далее мы можем отыскать собственные векторы у полученной задачи TSEP, воспользовавшись одной из подходящих процедур библиотеки \textbf{LAPACK}. Полученные собственные векторы с точностью до рассматриваемых ''нормальных уравнений'' (\(B^*B\) или \(BB^*\)) будут искомыми левыми и правыми сингулярными векторами соответственно.

Также в качестве подхода имеет смысл рассмотреть не два нормальных уравнения, а одно (к примеру, $B^*B$). При рассмотрении $B^*B$ мы найдём левые сингулярные векторы $V$. Правые сингулярные векторы можно найти, вспомнив одну особенность сингулярного разложения:
\begin{equation*}
     B=U \Sigma V^* \Leftrightarrow U\Sigma(V^*V=I)=BV \Rightarrow u_{(:,j)}=u_j= \frac{Bv_{(i,:)}}{\sigma_j}.
\end{equation*}

\subsubsubsection{Проблема нормальных уравнений}

Сведение задачи BSVD к нахождению спектральных разложений \( B B^* \) или \( B^* B \) является численно неустойчивым. Это становится понятным при рассмотрении числа обусловленности, определённого через максимальные и минимальные сингулярные числа:
\[\mu(B) = \frac{\sigma_{max}}{\sigma_{min}}.\]
Заметим, что:
\[ B^*B=(V\Sigma U^*)(U \Sigma V^*)=V (\Sigma)^2 V^*. \]

\begin{note}
    Напомним, что возведение диагональной матрицы в степень эквивалентно возведению всех элементов диагонали в соответствующий показатель. Также напомним, что матрицы \(U\) и \(V\) являются унитарными, то есть \(U^*U=1\) и \(V^*V=1\).
\end{note}

Аналогичное разложение будет иметь и \(BB^*\). Из этого видно:
\[ \mu(B^*B) = \mu(BB^*) = \left( \frac{\sigma_{max}}{\sigma_{min}} \right)^2 =(\mu(B))^2.\]
Заключим, что при умножении матрицы на саму себя число обусловленности возводится в квадрат, что приводит к значительной потере точности. 

Тем не менее, рассмотрение подхода с использованием так называемых ''нормальных уравнений'' имеет смысл для последующего сравнительного анализа.

\subsubsection{Форма Голуба-Кахана}

Существует более устойчивая альтернатива, которая позволяет избежать прямого умножения матрицы на себя ~\cite{mr3_algo4triagonal_sym_eigen_and_bidiagSVD}. Введём в рассмотрение матрицу следующего вида:

\begin{equation}
M = \begin{bmatrix} 0 & A \\ A^* & 0 \end{bmatrix}.
\end{equation}

Эту матрицу далее будем называть циклической. В случае бидиагональной матрицы \( B \in \R{n}{n} \) задача принимает еще более удобную форму:

\begin{equation}
M_B = \begin{bmatrix} 0 & B \\ B^* & 0 \end{bmatrix}.
\end{equation}

%Соотношение между сингулярными числами/векторами и собственными числами/векторами для циклических матриц подробно рассмотрено в разделе 8.6.1 ~\cite{Golub2013}. 

%Олежа: Вот тут, конечно, есть нюанс - в книге Голуба и в немецкой статье рассматриваются чутка разные матрицы (звёздочку надо перекинуть с нижней матрицы на верхнюю в исходной циклической матрице) - в целом, они говорят об одном и том же, но чтобы это понять, с книжкой Голуба надо несколько дополнительных усилий проделать... Не знаю, что с этим нюансом делать. Вообще то, о чём говорится в книге Голуба - это, в сущности, наша форма Джордана-Виландта, приведённая ниже.

% Edit 10 (DONE)
% Олежа: Стоит ли привести эти соотношения?
% Петя: Ждем что скажет Парф.

\subsubsubsection{Приведение к форме Голуба-Кахана}

% Edit 11 (TODO)
% Олежа: тут должна быть ссылка на немецкую диссертацию по MR^3, но без списка литературы я не уверен, что это стоит отдельно прописывать
% Петя: очень даже стоит :) и в библиографию тож надо закинуть, она теперь тоже есть в файле references.bib. Инструкция по добавлению есть в чате
% Олежа: Понял, что эта немецкая диссертация уже упомянута и в тексте нашем, и в референсе 

%Олежа: ~\cite[стр.~121]{mr3_algo4triagonal_sym_eigen_and_bidiagSVD} - форма Д-В.

Рассмотрим \(M_B \in \R{2n}{2n}\). Представим её в \textbf{форме Джордана-Виландта}:

\begin{equation} \label{D-V-eigen}
    M_B= \begin{bmatrix}
        0 & B \\
        B^* & 0
    \end{bmatrix} = J
    \begin{bmatrix}
        -\Sigma & 0 \\
        0 & \Sigma
    \end{bmatrix} J^*,
\end{equation}

где \(J= \frac{1}{\sqrt{2}}\begin{bmatrix}
    U & U \\
    -V & V
\end{bmatrix}\).

В сущности, эта форма задаёт соотношения между сингулярным разложением матрицы \(B\) и спектральным разложением циклической матрицы \(M_B\).

Удостоверимся, что данная форма корректна:

% Edit 12 (BUSY) (Этим занимается Коля - убрать это все и сделать по нормальному. Верю в него, поглядим на результат.)
% \split - это окружение, которое позволяет делать переносы. Тут это необходимо, ибо иначе матрицы уходят за пределы страницы
\begin{equation*}
    \begin{split}
        J \begin{bmatrix} -\Sigma & 0 \\ 0 & \Sigma \end{bmatrix} J^* 
        &= \left( \frac{1}{\sqrt{2}}\right)^2 
            \begin{bmatrix} U & U \\ -V & V \end{bmatrix}
            \begin{bmatrix} -\Sigma & 0 \\ 0 & \Sigma \end{bmatrix}
            \begin{bmatrix} U^* & -V^* \\ U^* & V^* \end{bmatrix}= \\[6pt]
        &= \frac{1}{2} 
            \begin{bmatrix} -U\Sigma & U\Sigma \\ V\Sigma & V\Sigma \end{bmatrix}
            \begin{bmatrix} U^* & -V^* \\ U^* & V^* \end{bmatrix}=\\[6pt] 
        &= \frac{1}{2} 
            \begin{bmatrix} 0 & 2 U\Sigma V^* \\ 2 V\Sigma U^* & 0 \end{bmatrix} = \begin{bmatrix} 0 & B \\ B^* & 0 \end{bmatrix}.
    \end{split}
\end{equation*}

Следующее утверждение необходимо для доказательства, что выражение \eqref{D-V-eigen} можно интерпретировать как спектральное разложение циклической матрицы.

\begin{claim} \label{J-unitary}
    Матрица \(J\) является унитарной.
\end{claim}
\begin{proof}
    \begin{equation*}
        \begin{split}
            J^* J= \frac{1}{2} \begin{bmatrix} U^* &-V^*\\ U^* & V^* \end{bmatrix} \begin{bmatrix} U & U \\ -V & V \end{bmatrix} = \frac{1}{2} \begin{bmatrix}
        U^*U+V^*V & U^*U-V^*V \\ U^*U-V^*V & U^*U+V^*V
    \end{bmatrix} =\\[6pt] = \begin{bmatrix}
        I_n & 0 \\ 0 & I_n
    \end{bmatrix} = I_{2n} \in \mathbb{C}^{2n \times 2n}.
        \end{split}
    \end{equation*}
\end{proof}

Затем, используя бидиагональность \( B \), можно применить матрицу перестановки \( P \) к \( M_B \), чтобы привести её к \textbf{форме Голуба-Кахана} \( T_{GK} \):

\begin{equation}
T_{GK}(B) = P \begin{bmatrix} 0 & B \\ B^* & 0 \end{bmatrix} P^* = diag_{\pm 1}(\alpha_1,\beta_1,\alpha_2,\beta_2,\dots,\alpha_{n-1},\beta_{n-1},\alpha_{n}).
\end{equation}

где \( \alpha_i \) и \( \beta_i \) — элементы исходной бидиагональной матрицы \( B \) на главной и наддиагонали соответственно. 

Опишем принцип действия матрицы перестановки. Пусть дан вектор \(x \in \mathbb{C}^{2n} \). Тогда действие матрицы перестановки производится так

\begin{equation}
    Px=(x_{n+1},x_1, x_{n+2},x_2,\dots,x_{2n},x_n)^*.
\end{equation}
\begin{note}
    Определим также действие транспонированной матрицы перестановки.
    \begin{equation}
        P^*x=(x_2,x_4,\dots,x_{2n},x_1,x_3,\dots,x_{2n-1})^*.
    \end{equation}
\end{note}

\begin{claim} \label{P-unitary}
    Матрица перестановки \(P\) является унитарной.
\end{claim}
\begin{proof}
    Для подтверждения последнего замечания рассмотрим выражение \(P^*(Px)\). Сперва вспомним: 
    \begin{equation}
        Px=(x_{n+1},x_1, x_{n+2},x_2,\dots,x_{2n},x_n)^*.
    \end{equation}
    Заметим, что вторая половина вектора \(x\) \([x_{n+1},x_{n+2},\dots,x_{2n}]\) находится на нечётных позициях, а первая половина вектора  \(x\) \([x_{1},x_{2},\dots,x_{n}]\) - на чётных. 

    В сущности, если рассмотреть действие транспонированной матрицы и переназначить индексы, то можно увидеть, что:
    \begin{equation*}
        \begin{split}
            P^*(Px)=P^*(x_{n+1},x_1, x_{n+2},x_2,\dots,x_{2n},x_n)^*=P^*(x_{1}^\prime,x_2^\prime,\dots,x_{2n-1}^\prime,x_{2n}^\prime)^*=\\[6pt]=(x_2^\prime,x_4^\prime,\dots,x_{2n}^\prime,x_1^\prime,x_3^\prime,\dots,x_{2n-1}^\prime)^*=(x_{1},x_2,\dots,x_{2n-1},x_{2n})^*=I_{2n}x. 
        \end{split}
    \end{equation*}
    То есть \(P^*P=I_{2n}\).
\end{proof}

\begin{note}
    Теоретически матрицу \(P\) можно представить через набор единиц и нулей, расположенных специальным образом в соответствии с умножением строки матрицы на столбец. Однако на практике действие матрицы перестановок легче реализовать в виде непосредственной работы с индексами матрицы или вектора, на которую (матрицу) или на который (вектор) действует матрица перестановок.   
\end{note}

\begin{example}[Форма Голуба-Кахана для матрицы \(2\times2\)]
    В качестве иллюстративного примера реализации формы Голуба-Кахана рассмотрим эту форму на основе матрицы \(S\in \mathbb{C}^{2\times2}\):

\[
    S=\begin{bmatrix} a_1 & b_1 \\ 0 & a_2 \end{bmatrix}.
\]

Составим циклическую матрицу:
\[
    M_S=\begin{bmatrix}
        0 & S \\
        S^* & 0
    \end{bmatrix}=\begin{bmatrix} 0 & 0& a_1 & b_1 \\ 0 & 0 & 0 & a_2 \\ \overline{a_1} & 0 & 0 & 0 \\ \overline{b_1} & \overline{a_2} & 0 & 0  \end{bmatrix}.
\]

Далее будем интерпретировать столбцы матрицы в качестве векторов, на которые будут действовать матрицы перестановки.

\begin{note}
    В этом случае матрица перестановки будет переводить набор индексов \((1,2,3,4)\) в набор \((3,1,4,2)\), а транспонированная матрица перестановки - в набор \((2,4,1,3)\).
\end{note}

Форма Голуба-Кахана для этого примера выходит следующей:
\begin{equation*}
    \begin{split}
        T_{GK}(S)=P\begin{bmatrix} 0 & 0& a_1 & b_1 \\ 0 & 0 & 0 & a_2 \\ \overline{a_1} & 0 & 0 & 0 \\ \overline{b_1} & \overline{a_2} & 0 & 0  \end{bmatrix}P^*
        =
        \begin{bmatrix} \overline{a_1} & 0& 0 & 0 \\ 0 & 0 & a_1 & b_1 \\ \overline{b_1} & \overline{a_2} & 0 & 0 \\ 0 & 0 & 0 & a_2  \end{bmatrix}P^*=\\[6pt]=\left(P\begin{bmatrix} a_1 & 0& b_1 & 0 \\ 0 & 0 &  a_2 & 0 \\ 0 & \overline{a_1} & 0 & 0 \\ 0 & \overline{b_1} & 0 & \overline{a_2}  \end{bmatrix}\right)^*= \begin{bmatrix} 0 & a_1 & 0 & 0 \\ \overline{a_1} & 0 & \overline{b_1} & 0 \\ 0 & b_1 & 0 & a_2 \\ 0 & 0 & \overline{a_2} & 0  \end{bmatrix}.
        \end{split} 
    \end{equation*}
\end{example}

Вновь рассмотрим  форму Голуба-Кахана \(T_{GK}\):
\begin{equation}
T_{GK} (B) = P \begin{bmatrix} 0 & B \\ B^* & 0 \end{bmatrix} P^* .
\end{equation}

Разложим циклическую матрицу по форме Джордана-Виландта:
\begin{equation}
T_{GK} = P J\begin{bmatrix} -\Sigma & 0 \\ 0 & \Sigma \end{bmatrix}J^* P^* = (PJ) \begin{bmatrix} -\Sigma & 0 \\ 0 & \Sigma \end{bmatrix} (PJ)^*=Q\Lambda Q^*.
\end{equation}

Последнее означает, что комбинация форм Голуба-Кахана и Джордана-Виландта задаёт, в сущности, спектральное разложение формы Голуба-Кахана.

% Edit 13 (TODO)
% Олежа:  стоит ли здесь уточнить комплексный случай? Да, я именно про унитарные матрицы.
% Петя: Стоит-стоит.

\begin{claim}
     Матрица \(PJ\) является унитарной.
\end{claim}
\begin{proof}
   Как показано в утверждениях  \ref{J-unitary} и \ref{P-unitary},  матрицы \(P\) и \(J\) - унитарные. Заметим, что:

    \[(PJ)^*PJ=J^* P^* P J=J^* (P^* P=I_{2n}) J=J^*J=I_{2n}.\]
    
\end{proof}


В итоге задача сингулярного разложения свелась к задаче спектрального разложения. 

\begin{note}
    В полученной задаче о поиске спектрального разложения стоит уточнить обозначения:
    \[
        \Lambda = diag(-\sigma_n,\dots,-\sigma_1,\sigma_1,\dots,\sigma_n) \ - \text{спектр,} 
    \]
    \[
        Q=PJ \ - \text{набор собственных векторов.}
    \]
\end{note}

Отдельно рассмотрим вид собственных векторов полученной задачи:
\begin{equation} \label{eq:eigenvector}
    \begin{split}
        Q=PJ \Rightarrow q_{i}=\frac{1}{\sqrt{2}}P(u_1,\dots,u_n,\pm v_1,\dots,\pm v_n)^* \Leftrightarrow \\[6pt]  \Leftrightarrow \sqrt{2}q_i=(\pm v_1,u_1,\pm v_2,u_2,\dots,\pm v_n,u_n)^*, 
    \end{split}
\end{equation}
где \(q_i\) - собственный вектор.


Таким образом, \( T_{GK} \) представляет собой симметричную (с точностью до комплексного сопряжения) тридиагональную матрицу с нулями на главной диагонали. Это особенно удобно, поскольку тридиагональные системы уравнений обладают хорошо изученными и численно устойчивыми методами решения (например, методом прогонки).

\subsubsubsection{Решение задачи на основе $T_{GK}$}

Предполагая, что сингулярные значения бидиагональной матрицы \( B \) известны, можно найти собственные векторы \( T_{GK} \), решая систему уравнений:
% Edit 14 (BUSY) (спросим у Парфа/Дроздова)
% Олежа: Xм, кажется мы тут решаем обычное уравнение на поиск собственных векторов и значений, а-ля: Ax=lx => (A-l*I)x=0. И как будто тут пропустили x в основном тексте, а ведь x - это и есть собственные векторы. Я всё-таки исправил это уравнение. 

\begin{equation}
( T_{GK} - \sigma_i I )x = 0,
\end{equation}

где \( \sigma_i \) — сингулярные значения \( B \), \( I \) — единичная матрица, а \(x\) - искомый собственный вектор.

Восстановленные собственные векторы \( T_{GK} \) напрямую связаны с сингулярными векторами исходной бидиагональной матрицы \( B \) (см. выражение \eqref{eq:eigenvector}).

Тем не менее, симметричная тридиагональная структура \( T_{GK} \) делает задачу проще и численно стабильнее по сравнению с традиционными методами.

% Edit 15 (BUSY) (спросим у Парфа/Дроздова)
%Олежа: Отдельно возникает проблема потери ортогональности у собственных векторов. Она решается путём ортогональности полученного набора векторов - но это утверждение я пока что не могу доказать

% Олежа: метод обратных итераций(для нахождения собственных векторов) + метод прогонки(из-за тридиагональной структуры) = ?

\subsection{Программная реализация}

TODO



% =================================================================================================
% =================================================================================================
% =================================================================================================
% =================================================================================================
% =================================================================================================

