## Введение ##
Метод Ланцоша [1950] стал одним из самых успешных методов для приближённого вычисления нескольких собственных значений вещественной симметричной (или комплексной эрмитовой) матрицы. Изначально метод не привлёк особого внимания, так как его рассматривали как метод для приведения матрицы к тридиагональной форме, что лучше выполнялось методами Гивенса и Хаусхолдера. Чтобы конкурировать по точности, метод Ланцоша должен был быть дополнен явной ортогонализацией вычисленных векторов, которые при точной арифметике автоматически были бы ортогональными. Спустя много лет возобновление интереса к методу Ланцоша вызвала работа Пейджа, которая привела к серии важных вкладов со стороны многих авторов, что способствовало лучшему пониманию метода и расширению его применимости. Некоторые из этих вкладов описаны в этом отчёте (раздел 2), чтобы мотивировать рассмотрение конкретных вариантов, реализованных в slepc (раздел 3).

Метод Ланцоша связан с методом Арнольди в том смысле, что Ланцош можно рассматривать как частный случай метода Арнольди, когда матрица симметрична. По этой причине у обоих методов есть общие аспекты. Для подробного описания метода Арнольди см. технический отчёт slepc STR-4, “Arnoldi Methods in slepc”.

Этот отчёт не включает материалы о несимметричной версии метода Ланцоша. Собственные значения, основанные на этом методе, могут быть добавлены в будущие версии slepc.

## Описание алгоритма ##
Этот раздел предоставляет обзор метода Ланцоша и некоторых его вариаций, включая методы для предотвращения потери ортогональности.
Метод Ланцоша может быть выведен с разных точек зрения. Один из таких подходов заключается в приведении симметричной матрицы <i>A</i> порядка <i>n × n</i> к тридиагональной форме с помощью трёхчленной рекуррентной формулы. Задав начальный вектор <i>v<sub>1</sub></i> с единичной нормой и положив <i>β<sub>1</sub> = 0</i>, применяют следующую рекуррентную формулу:

<p align="center">β<sub>j+1</sub>v<sub>j+1</sub> = Av<sub>j</sub> − α<sub>j</sub> v<sub>j</sub> − β<sub>j</sub> v<sub>j−1</sub>(1),</p>

где <i>α<sub>j</sub> = v<sub>j</sub><sup>*</sup> A v<sub>j</sub></i> и <i>β<sub>j+1</sub> = v<sub>j+1</sub><sup>*</sup> A v<sub>j</sub></i>, что порождает ортонормированный набор векторов Ланцоша <i>v<sub>j</sub></i> и тридиагональную матрицу, определённую как

<table border="1" cellpadding="5" cellspacing="0">
  <tr>
    <td>α<sub>1</sub></td>
    <td>β<sub>2</sub></td>
    <td></td>
    <td></td>
    <td></td>
  </tr>
  <tr>
    <td>β<sub>2</sub></td>
    <td>α<sub>2</sub></td>
    <td>β<sub>3</sub></td>
    <td></td>
    <td></td>
  </tr>
  <tr>
    <td></td>
    <td>β<sub>3</sub></td>
    <td>α<sub>3</sub></td>
    <td>⋱</td>
    <td></td>
  </tr>
  <tr>
    <td></td>
    <td></td>
    <td>⋱</td>
    <td>⋱</td>
    <td>β<sub>n</sub></td>
  </tr>
  <tr>
    <td></td>
    <td></td>
    <td></td>
    <td>β<sub>n</sub></td>
    <td>α<sub>n</sub></td>
  </tr>
</table>


Можно показать, что вектор <i>v<sub>n+1</sub></i> равен нулю, и выполняется следующее соотношение:
<p align="center">A V − V T = 0 (3),</p>

где <i>V = [v<sub>1</sub>, v<sub>2</sub>, . . . , v<sub>n</sub>]</i>. То есть рекурсия Ланцоша вычисляет тридиагональную матрицу, которая ортогонально подобна <i>A</i>.

При описании алгоритма вычисления рекурсии Ланцоша можно учитывать два замечания. Первое заключается в том, что <i>β<sub>j+1</sub></i> можно вычислить как <i>‖Av<sub>j</sub> − α<sub>j</sub> v<sub>j</sub> − β<sub>j</sub> v<sub>j−1</sub>‖<sub>2</sub></i>, поскольку <i>v<sub>j+1</sub></i> имеет единичную норму. Численный анализ, проведенный Пейджем [1972], показывает, что этот альтернативный подход улучшает численную устойчивость при реализации рекурсии в арифметике с конечной точностью. 

Второе замечание заключается в том, что <i>α<sub>j</sub></i> можно вычислить как <i>v<sub>j</sub><sup>*</sup>(Av<sub>j</sub> − β<sub>j</sub> v<sub>j−1</sub>)</i>, поскольку <i>v<sub>j</sub></i> и <i>v<sub>j−1</sub></i> ортогональны по конструкции. Этот альтернативный подход также поддерживается Пейджем [1980]. Учитывая эти замечания, основной алгоритм Ланцоша можно записать как в Алгоритме 1.

### Алгоритм 1 (Базовый алгоритм Ланцоша — вид рекурсии)

1. Выберите вектор единичной нормы <i>v<sub>1</sub></i>.
2. Установите <i>β<sub>1</sub> = 0</i>.
3. Для <i>j = 1, 2, . . .</i>:
   - <i>u<sub>j+1</sub> = Av<sub>j</sub> − β<sub>j</sub> v<sub>j−1</sub></i>
   - <i>α<sub>j</sub> = v<sub>j</sub><sup>*</sup> u<sub>j+1</sub></i>
   - <i>u<sub>j+1</sub> = u<sub>j+1</sub> − α<sub>j</sub> v<sub>j</sub></i>
   - <i>β<sub>j+1</sub> = ‖u<sub>j+1</sub>‖<sub>2</sub></i>
   - Если <i>β<sub>j+1</sub> = 0</i>, остановитесь.
   - <i>v<sub>j+1</sub> = u<sub>j+1</sub>/β<sub>j+1</sub></i>.
4. Конец.

Конечно, алгоритм Ланцоша наиболее полезен, когда не вычисляются все <i>n</i> векторы, что невозможно в контексте очень больших матриц. Если выполняется только <i>m</i> шагов Ланцоша, то вместо уравнения (3) имеет место следующее соотношение:

<p align="center">A V<sub>m</sub> − V<sub>m</sub>T<sub>m</sub> = β<sub>m+1</sub> v<sub>m+1</sub> e<sub>m</sub><sup>*</sup>,</p> (4)

где <i>T<sub>m</sub></i> — это ведущая подматрица размера <i>m × m</i> матрицы <i>T</i>, а <i>V<sub>m</sub> = [v<sub>1</sub>, v<sub>2</sub>, . . . , v<sub>m</sub>]</i>. Уравнение (4) описывает остаток <i>m</i>-шаговой факторизации Арнольди. То есть процесс Ланцоша также можно рассматривать как вычисление ортогонального проекции матрицы <i>A</i> на криловское подпространство <i>K<sub>m</sub>(A, v<sub>1</sub>)</i>. С этой точки зрения метод Ланцоша эквивалентен методу Арнольди, см. Алгоритм 2.

### Алгоритм 2 (Базовый алгоритм Ланцоша — вид проекции)

**Входные данные:** Матрица <i>A</i>, число шагов <i>m</i> и начальный вектор <i>v<sub>1</sub></i> нормы 1.  
**Выходные данные:** (<i>V<sub>m</sub>, T<sub>m</sub>, v<sub>m+1</sub>, β<sub>m+1</sub></i>) так, что 

<p align="center">A V<sub>m</sub> − V<sub>m</sub>T<sub>m</sub> = β<sub>m+1</sub> v<sub>m+1</sub> e<sub>m</sub><sup>*</sup></p>

Для <i>j = 1, 2, . . . , m</i>:
- <i>u<sub>j+1</sub> = A v<sub>j</sub></i>
- Ортогонализируйте <i>u<sub>j+1</sub></i> относительно <i>V<sub>j</sub></i> (вычисляя <i>α<sub>j</sub></i>)
- <i>β<sub>j+1</sub> = ‖u<sub>j+1</sub>‖<sub>2</sub></i>
- Если <i>β<sub>j+1</sub> = 0</i>, остановитесь
- <i>v<sub>j+1</sub> = u<sub>j+1</sub>/β<sub>j+1</sub></i>

Конец.

В Алгоритме 2 вторая строка в цикле выполняет процесс Грама-Шмидта для ортогонализации вектора <i>u<sub>j+1</sub></i> относительно столбцов <i>V<sub>j</sub></i>, то есть векторов <i>v<sub>1</sub>, v<sub>2</sub>, . . . , v<sub>j</sub></i> (см. технический отчет SLEPc STR-1, «Рутины ортогонализации в SLEPc» для подробностей о Граме-Шмидте). В этой операции вычисляются <i>j</i> коэффициентов Фурье. В точной арифметике первые <i>j−2</i> коэффициента равны нулю, и, следовательно, соответствующие операции не нужно выполнять (ортогональность относительно первых <i>j − 2</i> векторов является автоматической). Другие два коэффициента — это <i>β<sub>j</sub></i> и <i>α<sub>j</sub></i>. Обратите внимание, что, согласно Пейджу, <i>β<sub>j</sub></i>, вычисленный в этой операции, следует отбросить, и вместо него использовать значение <i>‖u<sub>j</sub>‖<sub>2</sub></i>, вычисленное на предыдущей итерации. С точки зрения ортогонализации, Алгоритм 1 выполняет модифицированный шаг Грама-Шмидта только с векторами <i>v<sub>j−1</sub></i> и <i>v<sub>j</sub></i>, в то время как вычисление <i>α<sub>j</sub></i> как <i>v<sub>j</sub><sup>*</sup>Av<sub>j</sub></i> соответствовало бы классическому Граму-Шмидту.

В дальнейшем мы сосредоточим наше обсуждение на Алгоритме 2, поскольку ортогонализация будет ключевым аспектом устойчивых вариантов Ланцоша, которые справляются с потерей ортогональности.

Как и в случае с методом Арнольди, поскольку <i>V<sub>m</sub><sup>*</sup>v<sub>m+1</sub> = 0</i> по конструкции, то, предварительно умножив уравнение (4) на <i>V<sub>m</sub><sup>*</sup></i>, получаем:

<p align="center">V<sub>m</sub><sup>*</sup>A V<sub>m</sub> = T<sub>m</sub>(5),</p>
То есть матрица <i>T<sub>m</sub></i> представляет собой ортогональную проекцию матрицы <i>A</i> на криловское подпространство, и этот факт позволяет нам вычислять приближения Рейли-Ритца для собственных пар матрицы <i>A</i>. Пусть <i>(λ<sub>i</sub>, y<sub>i</sub>)</i> — собственная пара матрицы <i>T<sub>m</sub></i>, тогда значение Ритца <i>λ<sub>i</sub></i> и вектор Ритца <i>x<sub>i</sub> = V<sub>m</sub>y<sub>i</sub></i> могут быть приняты в качестве приближений для собственных пар матрицы <i>A</i>. Обычно лишь небольшая часть из <i>m</i> приближений является качественной. Это можно оценить с помощью нормы остатка для пары Ритца, которую довольно легко вычислить:

<p align="center">‖A x<sub>i</sub> − λ<sub>i</sub>x<sub>i</sub}‖<sub>2</sub> = ‖A V<sub>m</sub>y<sub>i</sub> − λ<sub>i</sub>V<sub>m</sub>y<sub>i</sub>‖<sub>2</sub> = ‖(AV<sub>m</sub> − V<sub>m</sub>T<sub>m</sub>)y<sub>i</sub>‖<sub>2</sub> = β<sub>m+1</sub>|e<sub>m</sub><sup>*</sup>y<sub>i</sub>|. (6)</p>

Единственное отличие от метода Арнольди заключается в том, что <i>T<sub>m</sub></i> является симметричной тридегональной матрицей, и, следовательно, существует больше возможных методов для вычисления ее собственных пар.

Алгоритм Ланцоша в конечной арифметике сталкивается с проблемами, связанными с потерей ортогональности вектора Ланцоша, что приводит к появлению спurious eigenvalues (ложных собственных значений) и множественных копий значений Ритца. Чтобы решить эти проблемы и поддерживать эффективность алгоритма, вводится концепция рестарта. 
К тому же полная ортогонализация требует хранения всех векторов Ланцоша в памяти и увеличивает вычислительные затраты с увеличением числа шагов. Рестарт ограничивает количество хранимых векторов и, таким образом, снижает затраты на память и вычисления.

Далее расмотрим вариант алгоритма с перезапуском.

## Алгоритм 3 (Ланцош с дефляцией)

**Вход**: Матрица <code>A</code>, количество шагов <code>m</code>, матрицы <code>V<sub>k</sub></code>, <code>T<sub>k</sub></code> при <code>k &lt; m</code>, и начальный вектор <code>v<sub>k+1</sub></code> с нормой 1  
**Выход**: <code>(V<sub>m</sub>, T<sub>m</sub>, v<sub>m+1</sub>, &beta;<sub>m+1</sub>)</code> так, что <code>AV<sub>m</sub> - V<sub>m</sub>T<sub>m</sub> = &beta;<sub>m+1</sub> v<sub>m+1</sub> e<sup>*</sup><sub>m</sub></code>

Для <code>j = k + 1, &hellip;, m</code>
1. <code>u<sub>j+1</sub> = Av<sub>j</sub></code>
2. Ортогонализовать <code>u<sub>j+1</sub></code> относительно <code>V<sub>j</sub></code> (получая <code>&alpha;<sub>j</sub></code>)
3. <code>&beta;<sub>j+1</sub> = &#124;u<sub>j+1</sub>&#124;<sub>2</sub></code>
4. Если <code>&beta;<sub>j+1</sub> = 0</code>, остановиться
5. <code>v<sub>j+1</sub> = u<sub>j+1</sub> / &beta;<sub>j+1</sub></code>
   
*Примечание*: Алгоритм 3 вычисляет только последние <code>m - k</code> столбцов матриц <code>V<sub>m</sub></code> и <code>T<sub>m</sub></code>, активную часть факторизации. Начальный вектор в данном случае — это <code>v<sub>k+1</sub></code>. Операции в цикле схожи с Алгоритмом 2, однако ортогонализация обязательно включает заблокированные векторы Ланцоша (дефляция).

## Алгоритм 4 (Ланцош с явным перезапуском)

**Вход**: Матрица <code>A</code>, начальный вектор <code>v<sub>1</sub></code>, и размер подпространства <code>m</code>  
**Выход**: Частичное собственное разложение <code>AV<sub>k</sub> = V<sub>k</sub>&Theta;<sub>k</sub></code>, где <code>&Theta;<sub>k</sub> = diag(&theta;<sub>1</sub>, &hellip;, &theta;<sub>k</sub>)</code>

1. Нормализовать <code>v<sub>1</sub></code>
2. Инициализировать <code>V<sub>m</sub> = [v<sub>1</sub>]</code>, <code>k = 0</code>

*Цикл перезапуска*
1. Выполнить <code>m - k</code> шагов Ланцоша с дефляцией (Алгоритм 3)
2. Вычислить собственные пары матрицы <code>T<sub>m</sub></code>: <code>T<sub>m</sub> y<sub>i</sub> = y<sub>i</sub> &theta;<sub>i</sub></code>
3. Оценить нормы остаточных членов, <code>&tau;<sub>i</sub> = &beta;<sub>m+1</sub> |e<sup>*</sup><sub>m</sub> y<sub>i</sub>|</code>
4. Зафиксировать сходимые собственные пары, обновить значение <code>k</code>
5. <code>V<sub>m</sub> = V<sub>m</sub> Y</code>

*Примечание*: В Алгоритме 4 ведущая подматрица <code>T</code>, соответствующая зафиксированным векторам, является диагональной, поэтому некоторые операции можно пропустить для этой части.
Потеря ортогональности в контексте перезапуска метода Ланцоша

В методе Ланцоша с перезапуском также необходимо учитывать потерю ортогональности. В случае использования простой схемы явного перезапуска (Алгоритм 4) можно безопасно применять любые из методов, описанных в разделе 2, так как полная ортогональность векторов Ланцоша не требуется для корректного выполнения перезапуска. Однако в случае локальной ортогонализации следует учитывать следующие моменты:

- Поскольку значение <code>m</code> (максимально допустимая размерность подпространства) обычно значительно меньше, чем <code>n</code> (размерность матрицы), эвристические подходы, предложенные Каллумом и Уиллоуби [1985], не могут быть применены. Поэтому следует использовать другой метод для отбраковки ложных собственных значений, а также для удаления избыточных дубликатов. Один из возможных подходов заключается в явном вычислении нормы остатка для каждой сходимой собственной пары, а затем, среди правильных значений, принятии только первой копии (это подробнее объясняется в разделе 3).

- Вектор для перезапуска должен быть явно ортогонализован относительно зафиксированных векторов.
## Имплементация ##
Имплементация алгоритма Ланцоша доступна в SLEPc, начиная с версии 2.3.0. Эта реализация основана на Алгоритме 4 и включает все различные методы для обработки потери ортогональности.

Алгоритм 5 (Ланцош с явным перезапуском и различными методами ортогонализации)

**Вход**: Матрица <code>A</code>, начальный вектор <code>v<sub>1</sub></code>, и размер подпространства <code>m</code>  
**Выход**: Частичное собственное разложение <code>AV<sub>k</sub> = V<sub>k</sub>&Theta;<sub>k</sub></code>, где <code>&Theta;<sub>k</sub> = diag(&theta;<sub>1</sub>, &hellip;, &theta;<sub>k</sub>)</code>

1. Нормализовать <code>v<sub>1</sub></code>
2. Инициализировать <code>V<sub>m</sub> = [v<sub>1</sub>]</code>, <code>k = 0</code>

*Цикл перезапуска*
1. Для <code>j = k + 1, &hellip;, m</code>:
   - <code>u<sub>j+1</sub> = A v<sub>j</sub></code>
   - (*) Ортогонализовать <code>u<sub>j+1</sub></code> относительно <code>[V<sub>k</sub>, v<sub>j−1</sub>, v<sub>j</sub>]</code> (получая <code>&alpha;<sub>j</sub></code>)
   - (*) Определить множество <code>S</code> векторов Ланцоша
   - (*) Ортогонализовать <code>u<sub>j+1</sub></code> относительно <code>S</code>
   - <code>&beta;<sub>j+1</sub> = |u<sub>j+1</sub>|<sub>2</sub></code> (если <code>&beta;<sub>j+1</sub> = 0</code>, остановить)
   - <code>v<sub>j+1</sub> = u<sub>j+1</sub> / &beta;<sub>j+1</sub></code>
2. Вычислить собственные пары матрицы <code>T<sub>m</sub></code>: <code>T<sub>m</sub> y<sub>i</sub> = y<sub>i</sub> &theta;<sub>i</sub></code>
3. Оценить нормы остаточных членов: <code>&tau;<sub>i</sub> = &beta;<sub>m+1</sub> |e<sup>*</sup><sub>m</sub> y<sub>i</sub>|</code>
4. (*) Проверить на наличие ложных собственных значений
5. Зафиксировать сходимые собственные пары, обновить значение <code>k</code>
6. <code>V<sub>m</sub> = V<sub>m</sub> Y</code>

Определение множества <code>S</code> в зависимости от стратегии ортогонализации

- В случае локальной ортогонализации <code>S = &empty;</code>, поэтому вторую ортогонализацию не выполняют.
- Полная ортогонализация эквивалентна <code>S = [v<sub>k+1</sub>, v<sub>k+2</sub>, &hellip;, v<sub>j−2</sub>]</code>. В контексте параллельного исполнения обе ортогонализации, показанные в Алгоритме 5, выполняются как одна операция.
- При периодической и частичной ортогонализации рекуррентное вычисление <code>&omega;<sub>j</sub></code> производится на каждом шаге Ланцоша. Заметим, что вычислительные затраты при этом незначительны.
- При селективной ортогонализации реализация SLEPc явно вычисляет собственные значения и собственные векторы тридиагональной матрицы <code>T<sub>k</sub></code>, вместе с оценками ассоциированных норм остаточных членов, на каждом шаге Ланцоша. Отметим, что для умеренно больших <code>k</code> это может быть вычислительно затратным.

Проверка на наличие ложных собственных значений

Эта проверка в конце Алгоритма 5 необходима только для локальной ортогонализации. Стратегия заключается в следующем: собственные значения, которые повторяются в текущей тридиагональной матрице <code>T<sub>m</sub></code> после перезапуска, просто отбрасываются, исходя из предположения, что, если они действительно повторяются, то они появятся в следующем перезапуске. Для остальных собственных значений явно вычисляется истинная норма остатка <code>&#124;A x<sub>i</sub> − &lambda;<sub>i</sub>x<sub>i</sub>&#124;<sub>2</sub></code>, чтобы гарантировать, что принимаются только те пары, у которых норма остатка лежит в пределах допустимой погрешности.

Для детального сравнения различных стратегий ортогонализации, реализованных в SLEPc, как с точки зрения производительности, так и числовой устойчивости, рекомендуется обратиться к работе Hernandez и др. [2007].

## Список литературы ##
Lanczos and the Riemannian SVD in information retrieval applications-Fierro-Jiang-2005.pdf
<p>[Lanczos Methods in SLEPc](https://slepc.upv.es/documentation/reports/str5.pdf)</p>
