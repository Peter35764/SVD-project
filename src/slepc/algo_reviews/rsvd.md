## Введение

*Случайная матрица* — это матричная случайная величина, то есть матрица, в которой некоторые или все её элементы отбираются случайным образом из распределения вероятностей ([Reference 2](https://en.wikipedia.org/wiki/Random_matrix)).

*Матрица Гаусса* — это распределение вероятностей, которое является обобщением многомерного нормального распределения на матричные случайные величины ([Reference 3](https://ru.wikipedia.org/wiki/Матричное_нормальное_распределение)).

> **Note 1:** Теория случайных матриц понадобится нам вследствие такого замечательного факта, что при сужении размерности исходной матрицы вычисление усечённой SVD почти не страдает. (см. пункты 2.1.3, 2.2.1, 2.2.5, [Reference 1](https://arxiv.org/pdf/0909.4061))

> **NOTE 2:** (TODO) — вообще я всё перевожу эту статью, и, кажется, самое вкусное там — это оценка точности операций.

> **Note 3:** (TODO) — У случайной матрицы здесь имеется крайне полезное применение: с помощью неё мы можем уменьшить размерность и сделать более разреженной матрицу $A$, в результате чего упрощаются ряд операций над матрицами.

## Описание алгоритма

Сам алгоритм нацелен на работу с большими матрицами и предполагает решение усечённой СВД. При относительно малых матрицах, поскольку алгоритм итерационный, возможно решение полной СВД.

Для начала опишем общую концепцию матричной аппроксимации:

### Стадия A:

Вычислить приблизительный базис для некоторого диапазона входной матрицы $A$. Другими словами, нам требуется матрица $Q$, для которой выполняется:

$$ Q \text{ имеет ортонормированные столбцы и } A \approx Q Q^* A. \ \ \ (1.1) $$

Мы бы хотели, чтобы базисная матрица $Q$ содержала как можно меньше столбцов, но ещё нам важнее было бы иметь точную аппроксимацию входной матрицы.

> **Note:** Этот этап может быть эффективно вычислен с помощью методов случайной выборки (наш пациент).

### Стадия B:

Имея матрицу $Q$, удовлетворяющую (1.1), мы используем $Q$ для вычисления стандартной факторизации (QR, SVD и т. д.) матрицы $A$.

> **Note:** Этот этап может быть завершён зарекомендовавшими себя детерминированными алгоритмами.

Для SVD мы хотим вычислить матрицы $U$ и $V$ с ортонормированными столбцами и неотрицательной диагональной матрицей $\Sigma$, такой что:

$$ A \approx U \Sigma V^* $$

Эта цель достигается после трёх простых шагов:

1. Формируем матрицу $B = Q^*A$, которая даёт низкоранговую факторизацию $A \approx Q B$;
2. Вычисляем СВД малой матрицы $B = \tilde{U} \Sigma V^*$;
4. Устанавливаем $U = Q \tilde{U}$;

Вот псевдокод, на котором основывается алгоритм rsvd, реализуемый в slepc:

### Randomized SVD

**Дано:**

- $A \in \mathbb{R}^{m \times n}$;
- Число $k$ сингулярных векторов;
- Показатель ($q$ $q = 1$ или $q = 2$).

> **Note:** Эта процедура вычисляет приближенную 2k-ранговую факторизацию $U \Sigma V^*$, где $U$ и $V$ ортонормированы, а $\Sigma$ неотрицательна и диагональна.

#### Stage A:

- Сгенерировать тестовую матрицу $\Omega$ размера $n \times 2k$, с элементами, распределёнными по Гауссу;
- Сформировать $Y = ( AA^* )^q A \Omega$ путём поочерёдного умножения $A$ и $A^*$;
- Построить матрицу $Q$, столбцы которой образуют ортонормированный базис для диапазона (действия) $Y$.

#### Stage B:

- Сформировать $B = Q^* A$;
- Вычислить СВД малой матрицы: $B = \tilde{U} \Sigma V^*$;
- Установить $U = Q \tilde{U}$.

> **Note:** Вычисление $Y$ на шаге 2 подвержено ошибкам округления. Когда требуется высокая точность, мы должны включить шаг ортонормализации между каждым применением $A$ и $A^*$.

> **TODO:** Корректно сослаться на доказательную базу алгоритма.

## Имплементация

Забить, мяу.

## References

1. [Статья, посвящённая рандомизированному алгоритму](https://arxiv.org/pdf/0909.4061)
2. [Random matrix - Wikipedia](https://en.wikipedia.org/wiki/Random_matrix) — Статья, посвящённая случайным матрицам.
3. [Матричное нормальное распределение — Википедия](https://ru.wikipedia.org/wiki/Матричное_нормальное_распределение) — Статья, посвящённая матричному гауссову распределению.
