## ВВЕДЕНИЕ

Данный обзор предназначен для ознакомления с RSVD. Ознакомление подразумевает, что обзор даст представление, какая теоретическая база лежит в основе RSVD и что такое RSVD.  
В обзоре не будут представлены строгие математические выкладки из тех соображений, что углубление в них требует специальной подготовки, - поэтому основные рассуждения будут представлены крайне не строго. В обмен на эту вольность будет расширен список литературы.  
Обзор содержит следующую структуру: сначала представлена сама имплементация алгоритма – затем будут представлены ответы на основные и очевидные вопросы.  
В обзоре не будет разобрана реализация RSVD из исследуемой библиотеки SLEPc. Это связано с тем, что абстрактное представление RSVD совпадает с программной реализацией в SLEPc с точностью до тонкостей парадигмы программирования, - таких как представление решаемых задач как отдельных классов/структур со своими полями.  
В данном обзоре также не будет глубоко рассмотрен метод степеней, поскольку его теоретическое рассмотрение будет примерно таким же, как и исходного RSVD.

## ИМПЛЕМЕНТАЦИЯ АЛГОРИТМА

Для начала подчеркнём, что алгоритм RSVD предназначен для задач больших размерностей.  
К примеру, есть какая-нибудь матрица $A \in \mathbb{R}^{10^6 \times 10^6}$. Вычислять полное сингулярное выражение дорого. Но нам интересно понять, какие наиболее выраженные геометрические преобразования в себе несёт исходная матрица – то есть мы хотим знать информацию о максимальных сингулярных значениях нашей матрицы. В этом случае очень удобен алгоритм RSVD.  
Вот псевдокод, на коем основывается RSVD:

### Randomized SVD

**Дано:**
- $A \in \mathbb{R}^{m \times n}$;
- $k$ - число сингулярных векторов;
- $p$ - параметр пересэмплирования (обычно принимают равным 5-10).

**Примечание:** эта процедура вычисляет приближенную $k$-ранговую факторизацию $U \Sigma V^*$, где $U$ и $V$ ортонормированы, а $\Sigma$ неотрицательна и диагональна.

### Stage A: Усечение исходной матрицы и построение ортонормированного базиса

- Сгенерировать $\Omega$ тестовой матрицы по Гауссу размером $n \times (k + p)$; \\\формируем случайное подпространство
- Сформировать $Y = A \Omega$; \\\ проекция $A$ на случайное подпространство
- Построить матрицу $Q$, столбцы которой образуют ортонормированный базис для диапазона (действия) $Y$. \\\ как правило, этот шаг реализуется посредством QR-разложения

### Stage B: Построение малой матрицы и вычисление разложения

- Сформировать $B = Q^* A$; // проекция $A$ на базис $Q$
- Вычислить СВД малой матрицы: $B = \tilde{U} \Sigma V^*$; // можно использовать стандартные методы разложения
- Установить $U = Q \tilde{U}$; // Восстановление приближённых левых сингулярных векторов для $A$

Теперь рассмотрим естественно возникающие вопросы:

### Почему этот алгоритм работает корректно?

В сущности, чтобы ответить на этот вопрос, надо продемонстрировать теоретическую основу того, что усечение задачи SVD произошло корректно, ибо сам этап с подсчётом сингулярного разложения ясен при наличии соответствующей подготовки.  
В псевдокоде явно показано, что мы проецируем матрицу $A$ на случайное подпространство и получаем $Y$.  
При этом всём мы наблюдаем занимательный факт – «доминирующие» геометрические преобразования остаются почти без изменения! То есть мы с помощью случайной матрицы производим усечение матрицы $A$ по доминирующим сингулярным значениям. Теоретическая основа этого факта – теорема Дворецкого-Мильмана, лемма Джонсона-Линденштраусса, Теорема Эккарта-Янга и феномен концентрации меры.  

Разберём роль «каждого участника» этой теоретической основы:
- **Теорема Эккарта-Янга** – благодаря ней мы можем с уверенностью исследовать проекцию $A$ на ортонормированное подпространство $Q$, зная, что ошибка такой проекции по отношению к исходной матрице будет минимальна. Ошибка в нашей задаче – это $c \sigma_{k+1}$.
- **Теорема Дворецкого-Мильмана и лемма Джонсона-Линденштраусса** – они обосновывают тот факт, что проекция $A$ на $\Omega$ сохраняет основные геометрические свойства (в том числе и значения максимальных сингуляров, которые характеризуют «мощь» геометрического преобразования, носителем коего является матрица $A$).
- **Феномен концентрации меры** – этот факт объясняет, почему максимальные сингуляры не искажаются. Так же этот факт даёт пространство для многочисленных оценок ошибки и вероятности ошибки.  
Из этих теоретических идей и сформировалась основа корректности работы первого этапа RSVD.

### Зачем нам нужен ортонормированный базис $Q$?

Этот базис нужен за тем, что неизвестно, обладают ли столбцы проекции $Y = A \Omega$ линейной независимостью. А если они обладают линейной зависимостью, то уже велика вероятность роста ошибок, не работает ряд оценок, связанных с теоремой Эккарта-Янга – следовательно, мы уже не можем утверждать, что наша аппроксимация – оптимальна.

### Зачем нужен параметр пересэмплирования $p$?

Это хороший вопрос! Основная проблема, которая имеется при усечении исходной задачи – как деликатно усечь задачу так, чтобы корректно посчитать нужные нам $k$-максимальных сингулярных значений?  
В этом нам помогает параметр пересэмплирования $p$ – он нужен за тем, чтобы уменьшить вероятность ошибки, захватив «немного большее подпространство, нежели необходимое нам». Этот параметр обеспечивает нам численную устойчивость в ряде случаев и повышает вероятность обхода ошибок. Как правило, в качестве такого параметра берут $p = 5, \ldots, 10$.  
А если отбросить этот параметр, то вероятность ошибки может быть сколь угодно большой ($P(\text{ошибка}) \leq 1$).  
Также имеются оценки ошибки, связанные с этим параметром – на их основе можно подбирать ранг $k$ и параметр пересэмплирования $p$.

Ещё есть такая интересная оценка вероятности:\
$$P(\|A - Q Q^* A\| \leq \left[1 + 11 \sqrt{1 + 11 \sqrt{k + p} \sqrt{\min(m, n)}}\right] \sigma_{k+1}) \geq 1 - 6p^{-p}$$

То есть, иными словами, вероятность, что ошибка выйдет за пределы этой оценки:
$$P(\|A - Q Q^* A\| \geq \left[1 + 11 \sqrt{1 + 11 \sqrt{k + p} \sqrt{\min(m, n)}}\right] \sigma_{k+1}) \leq 6p^{-p}$$

Из этого всего мы получаем, что вероятность получить сильную ошибку – экспоненциальна. Становится понятным, что выбор большого параметра пересэмплирования не будет сильно влиять на вероятность ошибки – и поэтому можно сконцентрироваться на $p = 5, \ldots, 10$.

### Почему бы тогда не выбрать максимально большой параметр пересэмплирования?

Тогда пропадает смысл в использовании RSVD – пользователь всё равно ищет сингуляры у большой задачи. Этой мыслью подразумевается, что вычислительные затраты становятся большими.

### Какие слабые места у алгоритма?

Алгоритм RSVD плохо справляется с задачами, где спектр убывает медленно. Из-за медленно убывающего спектра растёт вероятность ошибки (так как сама ошибка зависит от спектра). Как решить эту проблему? Использовать метод степеней – с ним увеличивается число операций – но возрастает точность.  
Также тонким местом является выбор параметра-ранга $k$. Если мы знаем какую-то априорную информацию о росте (спаде) спектра, то выбор соответствующего $k$ не представляется трудным. В ином случае необходимо либо использовать методы, дающие какую-то нужную информацию о спектре, либо увеличивать параметр пересэмплирования, либо пересчитывать $k$ в соответствии с оценками ошибок. Также для случая неизвестного ранга $k$ представляется возможным использовать метод степеней.

### Какие сильные стороны у алгоритма?

RSVD очень хорошо отвечает трём требованиям – устойчивость, малость затрат и точность. 

### Как работает метод степеней?

Для иллюстрации приведён псевдокод:

### Randomized SVD

**Дано:**
- $A \in \mathbb{R}^{m \times n}$;
- число $k$ сингулярных векторов;
- показатель $q$ ($q = 1$ или $q = 2$);

**Примечание:** эта процедура вычисляет приближенную $2k$-ранговую факторизацию $U \Sigma V^*$, где $U$ и $V$ ортонормированы, а $\Sigma$ неотрицательна и диагональна.

### Stage A:
- Сгенерировать $\Omega$ тестовой матрицы по Гауссу размером $n \times 2k$;
- Сформировать $Y = (AA^*)^q A \Omega$ путём поочерёдного умножения $A$ и $A^*$;
- Построить матрицу $Q$, столбцы которой образуют ортонормированный базис для диапазона (действия) $Y$.

### Stage B:
- Сформировать $B = Q^* A$;
- Вычислить СВД малой матрицы: $B = \tilde{U} \Sigma V^*$;
- Установить $U = Q \tilde{U}$;

**Примечание:** Вычисление $Y$ на шаге 2 подвержено ошибкам округления. Когда требуется высокая точность, мы должны включить шаг ортонормализации между каждым применением $A$ и $A^*$.

**Примечание 2:** В сущности, что делает метод степеней – он увеличивает сингуляры возведением в степень. Доминирование максимальных сингуляров становится более очевидным, то есть спектр начинает убывать быстрее.

## REFERENCES

- [Статья, посвящённая рандомизированному алгоритму и на которой основана реализация RSVD в SLEPc](https://arxiv.org/pdf/0909.4061)
- Ledoux, M. The Concentration of Measure Phenomenon. American Mathematical Society, 2001.
- Vershynin, R. High-Dimensional Probability: An Introduction with Applications in Data Science. Cambridge University Press, 2020.
- [Статья, посвящённая лемме Джонсона-Линденштраусса](https://cseweb.ucsd.edu/~dasgupta/papers/jl.pdf)
- [Исходный код алгоритма RSVD, реализованного в SLEPc](https://gitlab.com/slepc/slepc/-/blob/main/src/svd/impls/randomized/rsvd.c?ref_type=heads)
- [Random matrix - Wikipedia](https://en.wikipedia.org/wiki/Random_matrix) – Статья в Википедии, посвящённая случайным матрицам.

