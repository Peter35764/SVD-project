## ВВЕДЕНИЕ
Случайные матрицы, Гауссова нормальная матрица

## ОПИСАНИЕ АЛГОРИТМА
Для начала опишем общую концепцию матричной аппроксимации:

**Стадия A**: Вычислить приблизительную базу для диапазона входной матрицы `A`. Другими словами, нам требуется матрица `Q`, для которой:

$$
Q \text{ имеет ортонормированные столбцы и } A \approx QQ^* A \quad \text{(1.1)}
$$

Мы бы хотели, чтобы базовая матрица `Q` содержала как можно меньше столбцов, но еще нам важнее было бы иметь точную аппроксимацию входной матрицы.

>**Note**: этот этап может быть эффективно вычислен с помощью методов случайной выборки (наш пациент).

**Стадия B**: Имея матрицу `Q`, удовлетворяющую (1.1), мы используем `Q` для вычисления стандартной факторизации (QR, SVD и т. д.) матрицы `A`.

>**Note**: этот этап может быть завершён зарекомендовавшими себя детерминированными алгоритмами.

Для SVD мы хотим вычислить матрицы `U` и `V` с ортонормированными столбцами и неотрицательной диагональной матрицей `Σ` такой, что:

$$
A \approx U\Sigma V^*
$$

Эта цель достигается после трех простых шагов:
1. Форма `B = Q^*A`, которая дает низкоранговую факторизацию `A ≈ QB`.
2. Вычислим SVD малой матрицы: `B = UΣV^*`.
3. Установить `U = QU`.

Также статья ставит проблему матрицы фиксированного ранга (см. 1 пункт при подсчёте SVD) и предлагает такое решение проблемы: см. (6 страница в [References[1]](https://arxiv.org/pdf/0909.4061)).

Сам алгоритм Randomized SVD: см. (9ая страница в [References[1]](https://arxiv.org/pdf/0909.4061)).

- дописать алгоритмическую сложность, выигрыш перед традиционными алгоритмами, рассмотрение производительности
- Получение рандомизированной матрицы – это тоже весёлое поле, которому нужно бы отдельное место отвести.

## ИМПЛЕМЕНТАЦИЯ
Забить, мяу

## REFERENCES
1. [Статья, посвящённая рандомизированному алгоритму](https://arxiv.org/pdf/0909.4061).
